2025-06-27 12:46:21,029 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'smollm:135m': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=8, device='cpu', modes={'deepseek-r1:1.5b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results/energy'), result_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results'), data_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-27 12:46:21,030 - energy_eval - INFO - Using device: cpu
2025-06-27 12:46:21,030 - energy_eval - INFO - Loading dataset from /home/s72kw/Documents/LMPowerConsuption/data/boolq_mini_128.jsonl
2025-06-27 12:46:21,052 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-27 12:46:21,052 - energy_eval - INFO - 
============================================================
Running model: smollm:135m
============================================================
2025-06-27 12:46:21,052 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {}
++++++++++++++++++++++++++++++
2025-06-27 12:46:21,171 - energy_eval - INFO - Completed smollm:135m in 0h 0m 0s
2025-06-27 12:46:21,171 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 0m 0s
============================================================
2025-06-27 12:48:30,037 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'smollm:135m': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=8, device='cpu', modes={'smollm:135m': {False, 'q'}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results/energy'), result_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results'), data_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-27 12:48:30,037 - energy_eval - INFO - Using device: cpu
2025-06-27 12:48:30,038 - energy_eval - INFO - Loading dataset from /home/s72kw/Documents/LMPowerConsuption/data/boolq_mini_128.jsonl
2025-06-27 12:48:30,043 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-27 12:48:30,043 - energy_eval - INFO - 
============================================================
Running model: smollm:135m
============================================================
2025-06-27 12:48:30,043 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {False, 'q'}
++++++++++++++++++++++++++++++
2025-06-27 12:48:30,043 - energy_eval - ERROR - Critical error: 'set' object has no attribute 'items'
Traceback (most recent call last):
  File "/home/s72kw/Documents/LMPowerConsuption/src/experiment.py", line 382, in <module>
    run()
  File "/home/s72kw/Documents/LMPowerConsuption/src/experiment.py", line 58, in run
    for mode_tag, include_passage in model_modes.items():
                                     ^^^^^^^^^^^^^^^^^
AttributeError: 'set' object has no attribute 'items'
2025-06-27 12:49:22,504 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'smollm:135m': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=8, device='cpu', modes={'smollm:135m': {False, 'q'}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results/energy'), result_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results'), data_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-27 12:49:22,504 - energy_eval - INFO - Using device: cpu
2025-06-27 12:49:22,504 - energy_eval - INFO - Loading dataset from /home/s72kw/Documents/LMPowerConsuption/data/boolq_mini_128.jsonl
2025-06-27 12:49:22,510 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-27 12:49:22,510 - energy_eval - INFO - 
============================================================
Running model: smollm:135m
============================================================
2025-06-27 12:49:22,510 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {False, 'q'}
++++++++++++++++++++++++++++++
2025-06-27 12:49:22,510 - energy_eval - ERROR - Critical error: 'set' object has no attribute 'items'
Traceback (most recent call last):
  File "/home/s72kw/Documents/LMPowerConsuption/src/experiment.py", line 382, in <module>
    run()
  File "/home/s72kw/Documents/LMPowerConsuption/src/experiment.py", line 58, in run
    for mode_tag, include_passage in model_modes.items():
                                     ^^^^^^^^^^^^^^^^^
AttributeError: 'set' object has no attribute 'items'
2025-06-27 12:51:31,622 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'smollm:135m': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=8, device='cpu', modes={'smollm:135m': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results/energy'), result_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results'), data_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-27 12:51:31,622 - energy_eval - INFO - Using device: cpu
2025-06-27 12:51:31,622 - energy_eval - INFO - Loading dataset from /home/s72kw/Documents/LMPowerConsuption/data/boolq_mini_128.jsonl
2025-06-27 12:51:31,628 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-27 12:51:31,628 - energy_eval - INFO - 
============================================================
Running model: smollm:135m
============================================================
2025-06-27 12:51:31,628 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-06-27 12:53:09,159 - energy_eval - WARNING - Experiment interrupted by user
2025-06-27 12:53:49,271 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'smollm:135m': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=8, device='cpu', modes={'smollm:135m': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results/energy'), result_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results'), data_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-27 12:53:49,272 - energy_eval - INFO - Using device: cpu
2025-06-27 12:53:49,272 - energy_eval - INFO - Loading dataset from /home/s72kw/Documents/LMPowerConsuption/data/boolq_mini_128.jsonl
2025-06-27 12:53:49,277 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-27 12:53:49,277 - energy_eval - INFO - 
============================================================
Running model: smollm:135m
============================================================
2025-06-27 12:53:49,277 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-27 12:54:33,492 - energy_eval - WARNING - Experiment interrupted by user
2025-06-27 12:55:11,275 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'smollm:135m': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=8, device='cpu', modes={'smollm:135m': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results/energy'), result_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results'), data_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-27 12:55:11,275 - energy_eval - INFO - Using device: cpu
2025-06-27 12:55:11,275 - energy_eval - INFO - Loading dataset from /home/s72kw/Documents/LMPowerConsuption/data/boolq_mini_128.jsonl
2025-06-27 12:55:11,280 - energy_eval - ERROR - Dataset loading failed: Expecting value: line 2 column 1 (char 1)
2025-06-27 12:58:42,403 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'smollm:135m': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=8, device='cpu', modes={'smollm:135m': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results/energy'), result_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results'), data_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-27 12:58:42,404 - energy_eval - INFO - Using device: cpu
2025-06-27 12:58:42,404 - energy_eval - INFO - Loading dataset from /home/s72kw/Documents/LMPowerConsuption/data/boolq_mini_128.jsonl
2025-06-27 12:58:42,547 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-27 12:58:42,547 - energy_eval - INFO - 
============================================================
Running model: smollm:135m
============================================================
2025-06-27 12:58:42,547 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-27 12:58:53,576 - energy_eval - ERROR - Error processing sample 0: name 'CONGFIG' is not defined
2025-06-27 12:58:59,299 - energy_eval - WARNING - Experiment interrupted by user
2025-06-27 12:59:17,900 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'smollm:135m': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=8, device='cpu', modes={'smollm:135m': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results/energy'), result_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results'), data_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-27 12:59:17,900 - energy_eval - INFO - Using device: cpu
2025-06-27 12:59:17,900 - energy_eval - INFO - Loading dataset from /home/s72kw/Documents/LMPowerConsuption/data/boolq_mini_128.jsonl
2025-06-27 12:59:17,905 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-27 12:59:17,905 - energy_eval - INFO - 
============================================================
Running model: smollm:135m
============================================================
2025-06-27 12:59:17,905 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-27 13:01:21,759 - energy_eval - INFO - Completed q mode for smollm:135m
2025-06-27 13:04:20,952 - energy_eval - WARNING - Experiment interrupted by user
2025-06-27 13:10:45,364 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'smollm:135m': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=8, device='cpu', modes={'smollm:135m': {'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results/energy'), result_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results'), data_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-27 13:10:45,364 - energy_eval - INFO - Using device: cpu
2025-06-27 13:10:45,364 - energy_eval - INFO - Loading dataset from /home/s72kw/Documents/LMPowerConsuption/data/boolq_mini_128.jsonl
2025-06-27 13:10:45,369 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-27 13:10:45,369 - energy_eval - INFO - 
============================================================
Running model: smollm:135m
============================================================
2025-06-27 13:10:45,369 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-06-27 13:16:47,863 - energy_eval - INFO - Loaded Wikipedia corpus and indexes
2025-06-27 13:24:00,698 - energy_eval - WARNING - Experiment interrupted by user
2025-06-27 13:25:51,871 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results/energy'), result_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results'), data_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-27 13:25:51,872 - energy_eval - INFO - Using device: cpu
2025-06-27 13:25:51,872 - energy_eval - INFO - Loading dataset from /home/s72kw/Documents/LMPowerConsuption/data/boolq_mini_128.jsonl
2025-06-27 13:25:51,916 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-27 13:25:51,916 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-27 13:25:51,916 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-27 15:49:05,510 - energy_eval - INFO - Completed q mode for deepseek-r1:8b
2025-06-27 15:55:02,228 - energy_eval - INFO - Loaded Wikipedia corpus and indexes
2025-06-28 05:20:14,777 - energy_eval - INFO - Completed q+r mode for deepseek-r1:8b
2025-06-28 05:26:23,032 - energy_eval - INFO - Completed deepseek-r1:8b in 16h 0m 31s
2025-06-28 05:26:23,077 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-06-28 05:26:23,077 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 09:59:30,872 - energy_eval - INFO - Completed q mode for deepseek-r1:14b
2025-06-28 10:05:48,801 - energy_eval - INFO - Loaded Wikipedia corpus and indexes
2025-06-28 10:06:03,808 - energy_eval - ERROR - Error processing sample 0: model requires more system memory (10.5 GiB) than is available (9.7 GiB) (status code: 500)
2025-06-28 10:06:14,453 - energy_eval - ERROR - Error processing sample 1: model requires more system memory (10.5 GiB) than is available (9.7 GiB) (status code: 500)
2025-06-28 10:06:25,040 - energy_eval - ERROR - Error processing sample 2: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:06:35,597 - energy_eval - ERROR - Error processing sample 3: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:06:46,246 - energy_eval - ERROR - Error processing sample 4: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:06:56,821 - energy_eval - ERROR - Error processing sample 5: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:07:07,397 - energy_eval - ERROR - Error processing sample 6: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:07:17,976 - energy_eval - ERROR - Error processing sample 7: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:07:28,597 - energy_eval - ERROR - Error processing sample 8: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:07:39,198 - energy_eval - ERROR - Error processing sample 9: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:07:49,790 - energy_eval - ERROR - Error processing sample 10: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:08:00,453 - energy_eval - ERROR - Error processing sample 11: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:08:11,027 - energy_eval - ERROR - Error processing sample 12: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:08:21,625 - energy_eval - ERROR - Error processing sample 13: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:08:32,242 - energy_eval - ERROR - Error processing sample 14: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:08:42,825 - energy_eval - ERROR - Error processing sample 15: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:08:53,453 - energy_eval - ERROR - Error processing sample 16: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:09:04,034 - energy_eval - ERROR - Error processing sample 17: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:09:14,617 - energy_eval - ERROR - Error processing sample 18: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:09:25,246 - energy_eval - ERROR - Error processing sample 19: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:09:35,812 - energy_eval - ERROR - Error processing sample 20: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:09:46,403 - energy_eval - ERROR - Error processing sample 21: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:09:56,987 - energy_eval - ERROR - Error processing sample 22: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:10:07,591 - energy_eval - ERROR - Error processing sample 23: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:10:18,167 - energy_eval - ERROR - Error processing sample 24: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:10:28,791 - energy_eval - ERROR - Error processing sample 25: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:10:39,369 - energy_eval - ERROR - Error processing sample 26: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:10:49,985 - energy_eval - ERROR - Error processing sample 27: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:11:00,617 - energy_eval - ERROR - Error processing sample 28: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:11:11,233 - energy_eval - ERROR - Error processing sample 29: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:11:21,811 - energy_eval - ERROR - Error processing sample 30: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:11:32,428 - energy_eval - ERROR - Error processing sample 31: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:11:43,042 - energy_eval - ERROR - Error processing sample 32: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:11:53,656 - energy_eval - ERROR - Error processing sample 33: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:12:04,235 - energy_eval - ERROR - Error processing sample 34: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:12:15,038 - energy_eval - ERROR - Error processing sample 35: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:12:25,605 - energy_eval - ERROR - Error processing sample 36: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:12:36,194 - energy_eval - ERROR - Error processing sample 37: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:12:46,820 - energy_eval - ERROR - Error processing sample 38: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:12:57,406 - energy_eval - ERROR - Error processing sample 39: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:13:07,998 - energy_eval - ERROR - Error processing sample 40: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:13:18,558 - energy_eval - ERROR - Error processing sample 41: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:13:29,203 - energy_eval - ERROR - Error processing sample 42: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:13:39,812 - energy_eval - ERROR - Error processing sample 43: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:13:50,393 - energy_eval - ERROR - Error processing sample 44: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:14:00,973 - energy_eval - ERROR - Error processing sample 45: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:14:11,577 - energy_eval - ERROR - Error processing sample 46: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:14:22,160 - energy_eval - ERROR - Error processing sample 47: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:14:32,753 - energy_eval - ERROR - Error processing sample 48: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:14:43,335 - energy_eval - ERROR - Error processing sample 49: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:14:53,926 - energy_eval - ERROR - Error processing sample 50: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:15:04,519 - energy_eval - ERROR - Error processing sample 51: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:15:15,125 - energy_eval - ERROR - Error processing sample 52: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:15:25,706 - energy_eval - ERROR - Error processing sample 53: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:15:36,285 - energy_eval - ERROR - Error processing sample 54: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:15:46,878 - energy_eval - ERROR - Error processing sample 55: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:15:57,464 - energy_eval - ERROR - Error processing sample 56: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:16:08,063 - energy_eval - ERROR - Error processing sample 57: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:16:18,660 - energy_eval - ERROR - Error processing sample 58: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:16:29,432 - energy_eval - ERROR - Error processing sample 59: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:16:40,030 - energy_eval - ERROR - Error processing sample 60: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:16:50,649 - energy_eval - ERROR - Error processing sample 61: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:17:01,260 - energy_eval - ERROR - Error processing sample 62: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:17:11,840 - energy_eval - ERROR - Error processing sample 63: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:17:22,428 - energy_eval - ERROR - Error processing sample 64: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:17:33,018 - energy_eval - ERROR - Error processing sample 65: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:17:43,606 - energy_eval - ERROR - Error processing sample 66: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:17:54,188 - energy_eval - ERROR - Error processing sample 67: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:18:04,815 - energy_eval - ERROR - Error processing sample 68: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:18:15,410 - energy_eval - ERROR - Error processing sample 69: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:18:25,995 - energy_eval - ERROR - Error processing sample 70: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:18:36,605 - energy_eval - ERROR - Error processing sample 71: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:18:47,398 - energy_eval - ERROR - Error processing sample 72: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:18:57,978 - energy_eval - ERROR - Error processing sample 73: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:19:08,553 - energy_eval - ERROR - Error processing sample 74: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:19:19,174 - energy_eval - ERROR - Error processing sample 75: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:19:29,752 - energy_eval - ERROR - Error processing sample 76: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:19:40,327 - energy_eval - ERROR - Error processing sample 77: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:19:50,907 - energy_eval - ERROR - Error processing sample 78: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:20:01,488 - energy_eval - ERROR - Error processing sample 79: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:20:12,062 - energy_eval - ERROR - Error processing sample 80: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:20:22,657 - energy_eval - ERROR - Error processing sample 81: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:20:33,272 - energy_eval - ERROR - Error processing sample 82: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:20:44,020 - energy_eval - ERROR - Error processing sample 83: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:20:54,594 - energy_eval - ERROR - Error processing sample 84: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:21:05,215 - energy_eval - ERROR - Error processing sample 85: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:21:15,790 - energy_eval - ERROR - Error processing sample 86: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:21:26,392 - energy_eval - ERROR - Error processing sample 87: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:21:36,970 - energy_eval - ERROR - Error processing sample 88: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:21:47,598 - energy_eval - ERROR - Error processing sample 89: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:21:58,178 - energy_eval - ERROR - Error processing sample 90: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:22:08,763 - energy_eval - ERROR - Error processing sample 91: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:22:19,324 - energy_eval - ERROR - Error processing sample 92: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:22:29,965 - energy_eval - ERROR - Error processing sample 93: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:22:40,546 - energy_eval - ERROR - Error processing sample 94: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:22:51,308 - energy_eval - ERROR - Error processing sample 95: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:23:01,900 - energy_eval - ERROR - Error processing sample 96: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:23:12,516 - energy_eval - ERROR - Error processing sample 97: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:23:23,104 - energy_eval - ERROR - Error processing sample 98: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:23:33,703 - energy_eval - ERROR - Error processing sample 99: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:23:44,298 - energy_eval - ERROR - Error processing sample 100: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:23:54,863 - energy_eval - ERROR - Error processing sample 101: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:24:05,458 - energy_eval - ERROR - Error processing sample 102: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:24:16,087 - energy_eval - ERROR - Error processing sample 103: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:24:26,680 - energy_eval - ERROR - Error processing sample 104: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:24:37,275 - energy_eval - ERROR - Error processing sample 105: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:24:47,853 - energy_eval - ERROR - Error processing sample 106: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:24:58,497 - energy_eval - ERROR - Error processing sample 107: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:25:09,315 - energy_eval - ERROR - Error processing sample 108: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:25:19,870 - energy_eval - ERROR - Error processing sample 109: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:25:30,449 - energy_eval - ERROR - Error processing sample 110: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:25:41,017 - energy_eval - ERROR - Error processing sample 111: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:25:51,592 - energy_eval - ERROR - Error processing sample 112: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:26:02,191 - energy_eval - ERROR - Error processing sample 113: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:26:13,097 - energy_eval - ERROR - Error processing sample 114: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:26:23,802 - energy_eval - ERROR - Error processing sample 115: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:26:34,395 - energy_eval - ERROR - Error processing sample 116: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:26:45,037 - energy_eval - ERROR - Error processing sample 117: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:26:55,610 - energy_eval - ERROR - Error processing sample 118: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:27:06,177 - energy_eval - ERROR - Error processing sample 119: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:27:16,760 - energy_eval - ERROR - Error processing sample 120: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:27:27,340 - energy_eval - ERROR - Error processing sample 121: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:27:37,948 - energy_eval - ERROR - Error processing sample 122: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:27:48,539 - energy_eval - ERROR - Error processing sample 123: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:27:59,122 - energy_eval - ERROR - Error processing sample 124: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:28:09,704 - energy_eval - ERROR - Error processing sample 125: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:28:20,331 - energy_eval - ERROR - Error processing sample 126: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:28:30,905 - energy_eval - ERROR - Error processing sample 127: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:28:30,905 - energy_eval - INFO - Completed q+r mode for deepseek-r1:14b
2025-06-28 10:28:36,626 - energy_eval - INFO - Completed deepseek-r1:14b in 5h 2m 13s
2025-06-28 10:28:36,626 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-06-28 10:28:36,626 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-06-28 14:30:44,857 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-28 14:30:44,857 - energy_eval - INFO - Using device: cpu
2025-06-28 14:30:44,857 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 14:30:44,873 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 14:30:44,873 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:7b
============================================================
2025-06-28 14:30:44,873 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 14:31:55,119 - energy_eval - WARNING - Experiment interrupted by user
2025-06-28 14:32:22,093 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-28 14:32:22,093 - energy_eval - INFO - Using device: cpu
2025-06-28 14:32:22,093 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 14:32:22,093 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 14:32:22,093 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:7b
============================================================
2025-06-28 14:32:22,093 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {}
++++++++++++++++++++++++++++++
2025-06-28 14:32:22,157 - energy_eval - INFO - Completed deepseek-r1:7b in 0h 0m 0s
2025-06-28 14:32:22,157 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-28 14:32:22,157 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 14:32:22,157 - energy_eval - INFO - Completed q mode for deepseek-r1:8b
2025-06-28 14:34:16,264 - energy_eval - WARNING - Experiment interrupted by user
2025-06-28 14:34:40,003 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-28 14:34:40,003 - energy_eval - INFO - Using device: cpu
2025-06-28 14:34:40,003 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 14:34:40,003 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 14:34:40,003 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:7b
============================================================
2025-06-28 14:34:40,003 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {}
++++++++++++++++++++++++++++++
2025-06-28 14:34:40,084 - energy_eval - INFO - Completed deepseek-r1:7b in 0h 0m 0s
2025-06-28 14:34:40,084 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-28 14:34:40,084 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 15:10:35,453 - energy_eval - WARNING - Experiment interrupted by user
2025-06-28 15:12:11,351 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:8b': {'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-28 15:12:11,351 - energy_eval - INFO - Using device: cpu
2025-06-28 15:12:11,351 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 15:12:11,351 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 15:12:11,351 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-28 15:12:11,351 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 15:12:48,170 - energy_eval - WARNING - Experiment interrupted by user
2025-06-28 15:12:58,511 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:8b': {'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-28 15:12:58,511 - energy_eval - INFO - Using device: cpu
2025-06-28 15:12:58,511 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 15:12:58,526 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 15:12:58,526 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-28 15:12:58,526 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 15:16:38,062 - energy_eval - INFO - Loaded Wikipedia corpus and indexes
2025-06-28 15:18:01,652 - energy_eval - WARNING - Experiment interrupted by user
2025-06-28 15:30:09,956 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:8b': {'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-28 15:30:09,957 - energy_eval - INFO - Using device: cpu
2025-06-28 15:30:09,957 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 15:30:09,964 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 15:30:09,964 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-28 15:30:09,965 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 15:34:11,075 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 4, 1)
2025-06-28 16:35:22,735 - energy_eval - WARNING - Experiment interrupted by user
2025-06-28 16:36:30,234 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:8b': {'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-28 16:36:30,235 - energy_eval - INFO - Using device: cpu
2025-06-28 16:36:30,235 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 16:36:30,238 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 16:36:30,238 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-28 16:36:30,239 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 16:40:19,226 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 3, 48)
2025-06-28 17:09:00,142 - energy_eval - WARNING - Experiment interrupted by user
2025-06-28 17:39:19,549 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:8b': {'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-28 17:39:19,549 - energy_eval - INFO - Using device: cpu
2025-06-28 17:39:19,549 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 17:39:19,556 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 17:39:19,556 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-28 17:39:19,557 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 17:43:26,918 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 4, 7)
2025-06-28 20:26:50,772 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:8b': {'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-06-28 20:26:50,773 - energy_eval - INFO - Using device: cpu
2025-06-28 20:26:50,773 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 20:26:50,777 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 20:26:50,777 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-28 20:26:50,777 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 20:30:48,193 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 3, 57)
2025-06-28 20:31:25,887 - energy_eval - WARNING - Experiment interrupted by user
2025-06-28 20:42:14,444 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:8b': {'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-06-28 20:42:14,444 - energy_eval - INFO - Using device: cpu
2025-06-28 20:42:14,444 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 20:42:14,448 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 20:42:14,448 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-28 20:42:14,448 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 20:46:10,485 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 3, 56)
2025-06-28 21:04:11,179 - energy_eval - INFO - Completed q+r mode for deepseek-r1:8b
2025-06-28 21:04:17,828 - energy_eval - INFO - Completed deepseek-r1:8b in 0h 22m 3s
2025-06-28 21:04:17,828 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 22m 3s
============================================================
2025-06-28 21:23:36,983 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-06-28 21:23:36,983 - energy_eval - INFO - Using device: cpu
2025-06-28 21:23:36,983 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 21:23:36,987 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 21:23:36,988 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:7b
============================================================
2025-06-28 21:23:36,988 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 21:32:18,709 - energy_eval - INFO - Completed q mode for deepseek-r1:7b
2025-06-28 21:36:33,118 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 4, 14)
2025-06-28 21:55:34,659 - energy_eval - INFO - Completed q+r mode for deepseek-r1:7b
2025-06-28 21:55:41,389 - energy_eval - INFO - Completed deepseek-r1:7b in 0h 32m 4s
2025-06-28 21:55:41,390 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-28 21:55:41,390 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 21:58:28,799 - energy_eval - WARNING - Experiment interrupted by user
2025-06-28 23:47:01,290 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-06-28 23:47:01,290 - energy_eval - INFO - Using device: cpu
2025-06-28 23:47:01,290 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 23:47:01,294 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 23:47:01,294 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:7b
============================================================
2025-06-28 23:47:01,294 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 23:47:01,296 - energy_eval - INFO - Completed q mode for deepseek-r1:7b
2025-06-28 23:50:41,009 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 3, 39)
2025-06-28 23:50:41,011 - energy_eval - INFO - Completed q+r mode for deepseek-r1:7b
2025-06-28 23:50:45,040 - energy_eval - INFO - Completed deepseek-r1:7b in 0h 3m 43s
2025-06-28 23:50:45,040 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-28 23:50:45,040 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 23:57:07,414 - energy_eval - INFO - Completed q mode for deepseek-r1:8b
2025-06-29 00:00:47,559 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 3, 40)
2025-06-29 00:17:18,729 - energy_eval - INFO - Completed q+r mode for deepseek-r1:8b
2025-06-29 00:17:22,936 - energy_eval - INFO - Completed deepseek-r1:8b in 0h 26m 37s
2025-06-29 00:17:22,936 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-06-29 00:17:22,936 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-29 00:17:22,947 - energy_eval - INFO - Completed q mode for deepseek-r1:14b
2025-06-29 00:21:06,078 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 3, 43)
2025-06-29 00:43:03,629 - energy_eval - INFO - Completed q+r mode for deepseek-r1:14b
2025-06-29 00:43:15,936 - energy_eval - INFO - Completed deepseek-r1:14b in 0h 25m 52s
2025-06-29 00:43:15,936 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-06-29 00:43:15,936 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-06-29 00:50:31,536 - energy_eval - INFO - Completed q mode for deepseek-r1:32b
2025-06-29 00:50:31,623 - energy_eval - INFO - Completed deepseek-r1:32b in 0h 7m 15s
2025-06-29 00:50:31,623 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 3m 30s
============================================================
2025-06-29 09:26:33,327 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-06-29 09:26:33,327 - energy_eval - INFO - Using device: cpu
2025-06-29 09:26:35,059 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-06-29 09:26:35,059 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:7b
============================================================
2025-06-29 09:26:35,059 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-29 12:42:46,197 - energy_eval - INFO - Completed q mode for deepseek-r1:7b
2025-06-29 12:46:37,931 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 3, 51)
2025-06-29 19:42:26,162 - energy_eval - INFO - Completed q+r mode for deepseek-r1:7b
2025-06-29 19:42:38,631 - energy_eval - INFO - Completed deepseek-r1:7b in 10h 16m 3s
2025-06-29 19:42:38,631 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-29 19:42:38,631 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-29 23:52:42,674 - energy_eval - INFO - Completed q mode for deepseek-r1:8b
2025-06-29 23:56:25,633 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 3, 42)
2025-06-30 07:28:13,044 - energy_eval - INFO - Completed q+r mode for deepseek-r1:8b
2025-06-30 07:28:25,725 - energy_eval - INFO - Completed deepseek-r1:8b in 11h 45m 47s
2025-06-30 07:28:25,726 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-06-30 07:28:25,726 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-30 12:04:34,348 - energy_eval - INFO - Completed q mode for deepseek-r1:14b
2025-06-30 12:08:17,705 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 3, 43)
2025-06-30 22:32:38,073 - energy_eval - INFO - Completed q+r mode for deepseek-r1:14b
2025-06-30 22:32:51,072 - energy_eval - INFO - Completed deepseek-r1:14b in 15h 4m 25s
2025-06-30 22:32:51,072 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-06-30 22:32:51,072 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-01 04:33:46,985 - energy_eval - INFO - Completed q mode for deepseek-r1:32b
2025-07-01 04:33:47,436 - energy_eval - INFO - Completed deepseek-r1:32b in 6h 0m 56s
2025-07-01 04:33:47,436 - energy_eval - INFO - 
============================================================
Experiment completed in 43h 7m 14s
============================================================
2025-07-01 10:37:57,215 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-01 10:37:57,215 - energy_eval - INFO - Using device: cpu
2025-07-01 10:37:58,001 - energy_eval - INFO - Loaded dataset with 7405 samples
2025-07-01 10:37:58,001 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:7b
============================================================
2025-07-01 10:37:58,001 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-01 18:20:05,555 - energy_eval - INFO - Completed q mode for deepseek-r1:7b
2025-07-01 18:24:05,392 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 3, 59)
2025-07-02 08:32:50,387 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-02 08:32:50,388 - energy_eval - INFO - Using device: cpu
2025-07-02 08:32:51,112 - energy_eval - INFO - Loaded dataset with 7405 samples
2025-07-02 08:32:51,112 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:7b
============================================================
2025-07-02 08:32:51,112 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-02 08:32:51,125 - energy_eval - INFO - Completed q mode for deepseek-r1:7b in 0h 0m 0s
2025-07-02 08:36:32,445 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0h 3m 41s
2025-07-02 08:48:51,055 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:1b': 'ollama', 'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:272b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'gemma3:1b': {'q': False, 'q+r': True}, 'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-02 08:48:51,056 - energy_eval - INFO - Using device: cpu
2025-07-02 08:48:52,588 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-02 08:48:52,589 - energy_eval - INFO - 
============================================================
Running model: gemma3:1b
============================================================
2025-07-02 08:48:52,589 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-02 13:22:34,900 - energy_eval - INFO - Completed q mode for gemma3:1b in 4h 33m 42s
2025-07-02 13:26:37,786 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0h 4m 2s
2025-07-02 19:53:03,111 - energy_eval - INFO - Completed q+r mode for gemma3:1b in 6h 26m 25s
2025-07-02 19:53:33,124 - energy_eval - INFO - Completed gemma3:1b in 11h 4m 40s
2025-07-02 19:53:33,124 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-02 19:53:33,124 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-03 01:09:48,209 - energy_eval - INFO - Completed q mode for gemma3:4b in 5h 16m 15s
2025-07-03 01:13:32,556 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0h 3m 44s
2025-07-03 09:08:38,572 - energy_eval - INFO - Completed q+r mode for gemma3:4b in 7h 55m 6s
2025-07-03 09:08:53,813 - energy_eval - INFO - Completed gemma3:4b in 13h 15m 20s
2025-07-03 09:08:53,813 - energy_eval - INFO - 
============================================================
Running model: gemma3:12b
============================================================
2025-07-03 09:08:53,813 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-03 15:27:30,365 - energy_eval - INFO - Completed q mode for gemma3:12b in 6h 18m 36s
2025-07-03 15:31:29,506 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0h 3m 59s
2025-07-03 15:32:06,925 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:1b': 'ollama', 'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'gemma3:1b': {'q': False, 'q+r': True}, 'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-03 15:32:06,926 - energy_eval - INFO - Using device: cpu
2025-07-03 15:32:08,475 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-03 15:32:08,475 - energy_eval - INFO - 
============================================================
Running model: gemma3:1b
============================================================
2025-07-03 15:32:08,475 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-03 15:32:08,485 - energy_eval - INFO - Completed q mode for gemma3:1b in 0:00:00
2025-07-03 15:35:50,089 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:41
2025-07-03 15:35:50,094 - energy_eval - INFO - Completed q+r mode for gemma3:1b in 0:00:00
2025-07-03 15:35:54,648 - energy_eval - INFO - Completed gemma3:1b in 0h 3m 46s
2025-07-03 15:35:54,648 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-03 15:35:54,648 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-03 15:35:54,653 - energy_eval - INFO - Completed q mode for gemma3:4b in 0:00:00
2025-07-03 15:39:41,734 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:47
2025-07-03 15:39:41,743 - energy_eval - INFO - Completed q+r mode for gemma3:4b in 0:00:00
2025-07-03 15:39:45,992 - energy_eval - INFO - Completed gemma3:4b in 0h 3m 51s
2025-07-03 15:39:45,992 - energy_eval - INFO - 
============================================================
Running model: gemma3:12b
============================================================
2025-07-03 15:39:45,992 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-03 15:39:45,997 - energy_eval - INFO - Completed q mode for gemma3:12b in 0:00:00
2025-07-03 15:43:30,584 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:44
2025-07-04 02:46:46,881 - energy_eval - INFO - Completed q+r mode for gemma3:12b in 11:03:16
2025-07-04 02:46:59,640 - energy_eval - INFO - Completed gemma3:12b in 11h 7m 13s
2025-07-04 02:46:59,640 - energy_eval - INFO - 
============================================================
Running model: gemma3:27b
============================================================
2025-07-04 02:46:59,640 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-04 10:09:00,267 - energy_eval - INFO - Completed q mode for gemma3:27b in 7:22:00
2025-07-04 10:09:00,492 - energy_eval - INFO - Completed gemma3:27b in 7h 22m 0s
2025-07-04 10:09:00,492 - energy_eval - INFO - 
============================================================
Experiment completed in 18h 36m 53s
============================================================
2025-07-04 10:59:32,207 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:1b': 'ollama', 'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'gemma3:1b': {'q': False, 'q+r': True}, 'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-04 10:59:32,207 - energy_eval - INFO - Using device: cpu
2025-07-04 10:59:32,207 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-07-04 10:59:32,219 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-04 10:59:32,219 - energy_eval - INFO - 
============================================================
Running model: gemma3:1b
============================================================
2025-07-04 10:59:32,219 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-04 11:14:05,749 - energy_eval - INFO - Completed q mode for gemma3:1b in 0:14:33
2025-07-04 11:18:09,031 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:04:03
2025-07-04 11:36:24,763 - energy_eval - INFO - Completed q+r mode for gemma3:1b in 0:18:15
2025-07-04 11:36:39,417 - energy_eval - INFO - Completed gemma3:1b in 0h 37m 7s
2025-07-04 11:36:39,417 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-04 11:36:39,417 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-04 11:51:39,179 - energy_eval - INFO - Completed q mode for gemma3:4b in 0:14:59
2025-07-04 11:55:33,871 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:54
2025-07-04 12:16:43,937 - energy_eval - INFO - Completed q+r mode for gemma3:4b in 0:21:10
2025-07-04 12:16:50,343 - energy_eval - INFO - Completed gemma3:4b in 0h 40m 10s
2025-07-04 12:16:50,343 - energy_eval - INFO - 
============================================================
Running model: gemma3:12b
============================================================
2025-07-04 12:16:50,343 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-04 12:33:03,459 - energy_eval - INFO - Completed q mode for gemma3:12b in 0:16:13
2025-07-04 12:37:01,346 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:57
2025-07-04 13:05:57,146 - energy_eval - INFO - Completed q+r mode for gemma3:12b in 0:28:55
2025-07-04 13:06:03,893 - energy_eval - INFO - Completed gemma3:12b in 0h 49m 13s
2025-07-04 13:06:03,893 - energy_eval - INFO - 
============================================================
Running model: gemma3:27b
============================================================
2025-07-04 13:06:03,893 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-04 13:24:43,221 - energy_eval - INFO - Completed q mode for gemma3:27b in 0:18:39
2025-07-04 13:24:43,370 - energy_eval - INFO - Completed gemma3:27b in 0h 18m 39s
2025-07-04 13:24:43,370 - energy_eval - INFO - 
============================================================
Experiment completed in 2h 25m 11s
============================================================
2025-07-04 15:32:01,872 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:1b': 'ollama', 'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'gemma3:1b': {'q': False, 'q+r': True}, 'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-04 15:32:01,872 - energy_eval - INFO - Using device: cpu
2025-07-04 15:32:02,800 - energy_eval - INFO - Loaded dataset with 7405 samples
2025-07-04 15:32:02,800 - energy_eval - INFO - 
============================================================
Running model: gemma3:1b
============================================================
2025-07-04 15:32:02,800 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-04 15:33:04,870 - energy_eval - WARNING - Experiment interrupted by user
2025-07-04 15:33:54,493 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:1b': 'ollama', 'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'gemma3:1b': {'q': False, 'q+r': True}, 'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-04 15:33:54,493 - energy_eval - INFO - Using device: cpu
2025-07-04 15:33:55,191 - energy_eval - INFO - Loaded dataset with 7405 samples
2025-07-04 15:33:55,191 - energy_eval - INFO - 
============================================================
Running model: gemma3:1b
============================================================
2025-07-04 15:33:55,191 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-04 22:52:34,076 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:1b': 'ollama', 'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'gemma3:1b': {'q': False, 'q+r': True}, 'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-04 22:52:34,076 - energy_eval - INFO - Using device: cpu
2025-07-04 22:52:34,076 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_128.jsonl
2025-07-04 22:52:34,096 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-04 22:52:34,096 - energy_eval - INFO - 
============================================================
Running model: gemma3:1b
============================================================
2025-07-04 22:52:34,096 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-04 23:05:53,033 - energy_eval - INFO - Completed q mode for gemma3:1b in 0:13:18
2025-07-04 23:09:31,754 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:38
2025-07-04 23:32:33,142 - energy_eval - INFO - Completed q+r mode for gemma3:1b in 0:23:01
2025-07-04 23:32:36,937 - energy_eval - INFO - Completed gemma3:1b in 0h 40m 2s
2025-07-04 23:32:36,938 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-04 23:32:36,938 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-04 23:45:54,759 - energy_eval - INFO - Completed q mode for gemma3:4b in 0:13:17
2025-07-04 23:49:31,692 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:36
2025-07-05 00:12:47,186 - energy_eval - INFO - Completed q+r mode for gemma3:4b in 0:23:15
2025-07-05 00:12:51,301 - energy_eval - INFO - Completed gemma3:4b in 0h 40m 14s
2025-07-05 00:12:51,301 - energy_eval - INFO - 
============================================================
Running model: gemma3:12b
============================================================
2025-07-05 00:12:51,301 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-05 00:26:31,528 - energy_eval - INFO - Completed q mode for gemma3:12b in 0:13:40
2025-07-05 00:30:09,141 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:37
2025-07-05 00:53:45,805 - energy_eval - INFO - Completed q+r mode for gemma3:12b in 0:23:36
2025-07-05 00:53:50,086 - energy_eval - INFO - Completed gemma3:12b in 0h 40m 58s
2025-07-05 00:53:50,086 - energy_eval - INFO - 
============================================================
Running model: gemma3:27b
============================================================
2025-07-05 00:53:50,086 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-05 01:07:40,756 - energy_eval - INFO - Completed q mode for gemma3:27b in 0:13:50
2025-07-05 01:07:40,851 - energy_eval - INFO - Completed gemma3:27b in 0h 13m 50s
2025-07-05 01:07:40,851 - energy_eval - INFO - 
============================================================
Experiment completed in 2h 15m 6s
============================================================
2025-07-05 08:01:51,116 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama', 'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False, 'q+r': True}, 'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-05 08:01:51,116 - energy_eval - INFO - Using device: cuda
2025-07-05 08:01:51,116 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_128.jsonl
2025-07-05 08:01:51,135 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-05 08:01:51,135 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-05 08:01:51,135 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-05 08:10:44,561 - energy_eval - WARNING - Experiment interrupted by user
2025-07-05 08:10:50,333 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama', 'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False, 'q+r': True}, 'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-05 08:10:50,333 - energy_eval - INFO - Using device: cuda
2025-07-05 08:10:50,333 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_128.jsonl
2025-07-05 08:10:50,346 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-05 08:10:50,346 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-05 08:10:50,346 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-05 08:15:18,624 - energy_eval - INFO - Completed q mode for deepseek-r1:1.5b in 0:04:28
2025-07-05 08:20:17,640 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:04:59
2025-07-05 08:22:09,384 - energy_eval - ERROR - Error processing sample 10: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:22:19,017 - energy_eval - ERROR - Error processing sample 11: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:22:28,825 - energy_eval - ERROR - Error processing sample 12: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:22:38,595 - energy_eval - ERROR - Error processing sample 13: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:22:48,439 - energy_eval - ERROR - Error processing sample 14: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:22:58,176 - energy_eval - ERROR - Error processing sample 15: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:23:07,974 - energy_eval - ERROR - Error processing sample 16: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:23:17,719 - energy_eval - ERROR - Error processing sample 17: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:23:27,586 - energy_eval - ERROR - Error processing sample 18: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:23:37,375 - energy_eval - ERROR - Error processing sample 19: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:23:47,235 - energy_eval - ERROR - Error processing sample 20: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:23:56,934 - energy_eval - ERROR - Error processing sample 21: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:24:06,760 - energy_eval - ERROR - Error processing sample 22: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:24:16,453 - energy_eval - ERROR - Error processing sample 23: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:24:26,112 - energy_eval - ERROR - Error processing sample 24: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:24:41,510 - energy_eval - WARNING - Experiment interrupted by user
2025-07-05 08:25:00,317 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama', 'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False, 'q+r': True}, 'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-05 08:25:00,317 - energy_eval - INFO - Using device: cuda
2025-07-05 08:25:00,317 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_128.jsonl
2025-07-05 08:25:00,331 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-05 08:25:00,331 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-05 08:25:00,331 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-05 08:25:00,333 - energy_eval - INFO - Completed q mode for deepseek-r1:1.5b in 0:00:00
2025-07-05 08:28:42,812 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:42
2025-07-05 08:49:06,891 - energy_eval - INFO - Completed q+r mode for deepseek-r1:1.5b in 0:20:24
2025-07-05 08:49:12,136 - energy_eval - INFO - Completed deepseek-r1:1.5b in 0h 24m 11s
2025-07-05 08:49:12,137 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:7b
============================================================
2025-07-05 08:49:12,137 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-05 09:01:14,694 - energy_eval - INFO - Completed q mode for deepseek-r1:7b in 0:12:02
2025-07-05 09:05:01,448 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:46
2025-07-05 09:27:03,564 - energy_eval - INFO - Completed q+r mode for deepseek-r1:7b in 0:22:02
2025-07-05 09:27:07,712 - energy_eval - INFO - Completed deepseek-r1:7b in 0h 37m 55s
2025-07-05 09:27:07,712 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-05 09:27:07,712 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-05 09:39:07,055 - energy_eval - INFO - Completed q mode for deepseek-r1:8b in 0:11:59
2025-07-05 09:41:10,111 - energy_eval - WARNING - Experiment interrupted by user
2025-07-05 09:41:30,152 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama', 'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False, 'q+r': True}, 'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-05 09:41:30,153 - energy_eval - INFO - Using device: cuda
2025-07-05 09:41:30,153 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_128.jsonl
2025-07-05 09:41:30,171 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-05 09:41:30,171 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-05 09:41:30,171 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-05 09:41:30,179 - energy_eval - INFO - Completed q mode for deepseek-r1:1.5b in 0:00:00
2025-07-05 09:45:10,718 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:40
2025-07-05 09:45:10,720 - energy_eval - INFO - Completed q+r mode for deepseek-r1:1.5b in 0:00:00
2025-07-05 09:45:14,775 - energy_eval - INFO - Completed deepseek-r1:1.5b in 0h 3m 44s
2025-07-05 09:45:14,775 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:7b
============================================================
2025-07-05 09:45:14,775 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-05 09:45:14,777 - energy_eval - INFO - Completed q mode for deepseek-r1:7b in 0:00:00
2025-07-05 09:48:54,057 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:39
2025-07-05 09:48:54,059 - energy_eval - INFO - Completed q+r mode for deepseek-r1:7b in 0:00:00
2025-07-05 09:48:58,158 - energy_eval - INFO - Completed deepseek-r1:7b in 0h 3m 43s
2025-07-05 09:48:58,158 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-05 09:48:58,158 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-05 09:48:58,160 - energy_eval - INFO - Completed q mode for deepseek-r1:8b in 0:00:00
2025-07-05 09:52:37,221 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:39
2025-07-05 10:14:55,809 - energy_eval - INFO - Completed q+r mode for deepseek-r1:8b in 0:22:18
2025-07-05 10:15:00,085 - energy_eval - INFO - Completed deepseek-r1:8b in 0h 26m 1s
2025-07-05 10:15:00,085 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-07-05 10:15:00,085 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-05 10:27:26,294 - energy_eval - INFO - Completed q mode for deepseek-r1:14b in 0:12:26
2025-07-05 10:31:07,955 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:41
2025-07-05 10:53:46,269 - energy_eval - INFO - Completed q+r mode for deepseek-r1:14b in 0:22:38
2025-07-05 10:53:50,388 - energy_eval - INFO - Completed deepseek-r1:14b in 0h 38m 50s
2025-07-05 10:53:50,388 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-05 10:53:50,388 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-05 11:06:24,188 - energy_eval - INFO - Completed q mode for deepseek-r1:32b in 0:12:33
2025-07-05 11:06:24,264 - energy_eval - INFO - Completed deepseek-r1:32b in 0h 12m 33s
2025-07-05 11:06:24,264 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 24m 54s
============================================================
2025-07-06 19:28:02,309 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama', 'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False, 'q+r': True}, 'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-06 19:28:02,309 - energy_eval - INFO - Using device: cuda
2025-07-06 19:28:03,233 - energy_eval - INFO - Loaded dataset with 7405 samples
2025-07-06 19:28:03,233 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-06 19:28:03,233 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-06 19:29:51,528 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-06 19:29:51,528 - energy_eval - INFO - Using device: cuda
2025-07-06 19:29:52,419 - energy_eval - INFO - Loaded dataset with 7405 samples
2025-07-06 19:29:52,419 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:7b
============================================================
2025-07-06 19:29:52,420 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-06 19:29:52,441 - energy_eval - INFO - Completed q mode for deepseek-r1:7b in 0:00:00
2025-07-06 19:31:13,207 - energy_eval - WARNING - Experiment interrupted by user
2025-07-06 19:31:32,772 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:1b': 'ollama', 'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:1b': {'q': False, 'q+r': True}, 'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-06 19:31:32,772 - energy_eval - INFO - Using device: cuda
2025-07-06 19:31:32,772 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_128.jsonl
2025-07-06 19:31:32,798 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-06 19:31:32,798 - energy_eval - INFO - 
============================================================
Running model: gemma3:1b
============================================================
2025-07-06 19:31:32,798 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-06 19:31:32,807 - energy_eval - INFO - Completed q mode for gemma3:1b in 0:00:00
2025-07-06 19:32:59,247 - energy_eval - WARNING - Experiment interrupted by user
2025-07-06 19:33:05,579 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:1b': 'ollama', 'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:1b': {'q': False, 'q+r': True}, 'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-06 19:33:05,579 - energy_eval - INFO - Using device: cuda
2025-07-06 19:33:06,273 - energy_eval - INFO - Loaded dataset with 7405 samples
2025-07-06 19:33:06,273 - energy_eval - INFO - 
============================================================
Running model: gemma3:1b
============================================================
2025-07-06 19:33:06,273 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-07 07:28:32,448 - energy_eval - INFO - Completed q mode for gemma3:1b in 11:55:26
2025-07-07 07:32:21,634 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:49
2025-07-07 08:33:36,503 - energy_eval - WARNING - Experiment interrupted by user
2025-07-07 14:14:58,129 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:32b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:32b': {'q': False}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-07 14:14:58,129 - energy_eval - INFO - Using device: cuda
2025-07-07 14:14:59,723 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-07 14:14:59,723 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-07 14:14:59,723 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-07 14:15:56,911 - energy_eval - ERROR - Error processing sample 4: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-07 14:17:16,639 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:32b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:32b': {'q': False}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-07 14:17:16,639 - energy_eval - INFO - Using device: cuda
2025-07-07 14:17:18,731 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-07 14:17:18,731 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-07 14:17:18,731 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-07 14:22:15,688 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:32b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:32b': {'q': False}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-07 14:22:15,688 - energy_eval - INFO - Using device: cuda
2025-07-07 14:22:17,074 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-07 14:22:17,074 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-07 14:22:17,074 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-08 00:59:26,238 - energy_eval - INFO - Completed q mode for deepseek-r1:32b in 10:37:09
2025-07-08 00:59:26,363 - energy_eval - INFO - Completed deepseek-r1:32b in 10h 37m 9s
2025-07-08 00:59:26,363 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-08 00:59:26,363 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-08 11:43:10,638 - energy_eval - INFO - Completed q mode for deepseek-r1:8b in 10:43:44
2025-07-08 11:47:16,390 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:04:05
2025-07-08 20:40:02,792 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:32b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:32b': {'q': False}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-08 20:40:02,792 - energy_eval - INFO - Using device: cuda
2025-07-08 20:40:02,807 - datasets.load - WARNING - Using the latest cached version of the dataset since google/boolq couldn't be found on the Hugging Face Hub
2025-07-08 20:40:02,824 - datasets.packaged_modules.cache.cache - WARNING - Found the latest cached dataset configuration 'default' at C:\Users\Ethan\.cache\huggingface\datasets\google___boolq\default\0.0.0\35b264d03638db9f4ce671b711558bf7ff0f80d5 (last modified on Tue Jun  3 13:36:06 2025).
2025-07-08 20:40:02,925 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-08 20:40:02,925 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-08 20:40:02,925 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-08 20:40:02,934 - energy_eval - INFO - Completed q mode for deepseek-r1:32b in 0:00:00
2025-07-08 20:40:03,013 - energy_eval - INFO - Completed deepseek-r1:32b in 0h 0m 0s
2025-07-08 20:40:03,013 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-08 20:40:03,013 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-08 20:40:03,028 - energy_eval - INFO - Completed q mode for deepseek-r1:8b in 0:00:00
2025-07-08 20:40:11,325 - energy_eval - WARNING - Experiment interrupted by user
2025-07-08 20:41:17,381 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:32b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:32b': {'q': False}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-08 20:41:17,381 - energy_eval - INFO - Using device: cuda
2025-07-08 20:41:17,396 - datasets.load - WARNING - Using the latest cached version of the dataset since google/boolq couldn't be found on the Hugging Face Hub
2025-07-08 20:41:17,396 - datasets.packaged_modules.cache.cache - WARNING - Found the latest cached dataset configuration 'default' at C:\Users\Ethan\.cache\huggingface\datasets\google___boolq\default\0.0.0\35b264d03638db9f4ce671b711558bf7ff0f80d5 (last modified on Tue Jun  3 13:36:06 2025).
2025-07-08 20:41:17,411 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-08 20:41:17,411 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-08 20:41:17,411 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-08 20:41:17,411 - energy_eval - INFO - Completed q mode for deepseek-r1:32b in 0:00:00
2025-07-08 20:41:17,506 - energy_eval - INFO - Completed deepseek-r1:32b in 0h 0m 0s
2025-07-08 20:41:17,506 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-08 20:41:17,506 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-08 20:41:17,506 - energy_eval - INFO - Completed q mode for deepseek-r1:8b in 0:00:00
2025-07-08 20:41:46,400 - energy_eval - WARNING - Experiment interrupted by user
2025-07-08 20:49:25,059 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:32b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:32b': {'q': False}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-08 20:49:25,059 - energy_eval - INFO - Using device: cuda
2025-07-08 20:49:27,268 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-08 20:49:27,268 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-08 20:49:27,268 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-08 20:49:27,268 - energy_eval - INFO - Completed q mode for deepseek-r1:32b in 0:00:00
2025-07-08 20:49:27,347 - energy_eval - INFO - Completed deepseek-r1:32b in 0h 0m 0s
2025-07-08 20:49:27,347 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-08 20:49:27,347 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-08 20:49:27,362 - energy_eval - INFO - Completed q mode for deepseek-r1:8b in 0:00:00
2025-07-08 20:53:05,686 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:38
2025-07-08 20:57:44,940 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 08:43:54,385 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 08:43:54,386 - energy_eval - INFO - Using device: cuda
2025-07-10 08:43:54,386 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 08:43:54,390 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 08:43:54,390 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 08:43:54,390 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-10 08:44:09,156 - energy_eval - INFO - [deepseek-r1:8b | q | QID 0] Prediction: <think>
Okay, user is asking if the World | Gold: True
2025-07-10 08:44:18,600 - energy_eval - INFO - [deepseek-r1:8b | q | QID 1] Prediction: <think>
Okay, user wants a yes/no answer | Gold: True
2025-07-10 08:44:27,498 - energy_eval - INFO - [deepseek-r1:8b | q | QID 2] Prediction: <think>
Okay, the question is "does Air | Gold: False
2025-07-10 08:44:36,349 - energy_eval - INFO - [deepseek-r1:8b | q | QID 3] Prediction: <think>
Okay, the user is asking if there | Gold: True
2025-07-10 08:44:45,394 - energy_eval - INFO - [deepseek-r1:8b | q | QID 4] Prediction: <think>
Hmm, the user wants me to answer | Gold: True
2025-07-10 08:44:54,465 - energy_eval - INFO - [deepseek-r1:8b | q | QID 5] Prediction: <think>
Okay, the user is asking whether Hope | Gold: True
2025-07-10 08:44:58,677 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 08:45:03,427 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=None, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 08:45:03,427 - energy_eval - INFO - Using device: cuda
2025-07-10 08:45:03,427 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 08:45:03,431 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 08:45:03,431 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 08:45:03,431 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-10 08:45:07,555 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 08:46:37,179 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=None, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 08:46:37,179 - energy_eval - INFO - Using device: cuda
2025-07-10 08:46:37,179 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 08:46:37,183 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 08:46:37,183 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 08:46:37,183 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-10 08:46:46,814 - energy_eval - INFO - [deepseek-r1:8b | q | QID 0] Prediction: <think>
Okay, user is asking if the World | Gold: True
2025-07-10 08:46:54,553 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 08:48:09,781 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=100, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 08:48:09,781 - energy_eval - INFO - Using device: cuda
2025-07-10 08:48:09,781 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 08:48:09,789 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 08:48:09,789 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 08:48:09,789 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-10 08:48:18,313 - energy_eval - INFO - Full output: <think>
Okay, user is asking if the World
2025-07-10 08:48:18,314 - energy_eval - INFO - [deepseek-r1:8b | q | QID 0] Prediction: <think>
Okay, user is asking if the World | Gold: True
2025-07-10 08:48:19,651 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 08:50:01,069 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=100, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 08:50:01,069 - energy_eval - INFO - Using device: cuda
2025-07-10 08:50:01,069 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 08:50:01,072 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 08:50:01,072 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 08:50:01,072 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-10 08:50:49,616 - energy_eval - INFO - Full output: <think>
Okay, user is asking if the World Cup has ever been won on penalties and wants a strict yes/no answer without explanation or punctuation. 

Hmm, this seems like a straightforward fact-check question about football history. The phrasing "has the world cup been won on penalties" suggests they're looking for confirmation whether penalty shootouts have determined any World Cup final winners.

I recall that before 1966, games were decided by extra time without penalties - just sudden death in the second half of extra time. But since then... Brazil vs Italy 1970 was a classic shootout win but wasn't official penalties yet. Then England beating Germany in 1966 via goal difference after extra time. 

Wait no, that's not what they're asking. They want to know if the final has ever been won purely on penalty kicks without any other method. The first true penalty shootout winner was West Germany beating Brazil in 1974 - but wait, that wasn't a full-time penalty shootout either. 

Actually looking deeper... Argentina vs England 1986 had extra time then penalties after goalless draw. That's the first official final decided by penalties according to FIFA rules at the time. And yes, it counts as being won on penalties.

But user just wants one word answer - no explanations needed. So "True" is correct here because there was indeed a World Cup final (1986) that went all the way to penalties after extra time.
</think>
True
2025-07-10 08:50:49,616 - energy_eval - INFO - [deepseek-r1:8b | q | QID 0] Prediction: <think>
Okay, user is asking if the World Cup has ever been won on penalties and wants a strict yes/no answer without explanation or punctuation. 

Hmm, this seems like a straightforward fact-check question about football history. The phrasing "has the world cup been won on penalties" suggests they're looking for confirmation whether penalty shootouts have determined any World Cup final winners.

I recall that before 1966, games were decided by extra time without penalties - just sudden death in the second half of extra time. But since then... Brazil vs Italy 1970 was a classic shootout win but wasn't official penalties yet. Then England beating Germany in 1966 via goal difference after extra time. 

Wait no, that's not what they're asking. They want to know if the final has ever been won purely on penalty kicks without any other method. The first true penalty shootout winner was West Germany beating Brazil in 1974 - but wait, that wasn't a full-time penalty shootout either. 

Actually looking deeper... Argentina vs England 1986 had extra time then penalties after goalless draw. That's the first official final decided by penalties according to FIFA rules at the time. And yes, it counts as being won on penalties.

But user just wants one word answer - no explanations needed. So "True" is correct here because there was indeed a World Cup final (1986) that went all the way to penalties after extra time.
</think>
True | Gold: True
2025-07-10 08:51:34,124 - energy_eval - INFO - Full output: <think>
Okay, user wants a yes/no answer about whether anyone has climbed Everest's North Face without explanation or extra words. 

Hmm, this seems like a straightforward factual question from someone who either needs quick confirmation for research or just testing basic knowledge. The phrasing is very rigid - "Answer ONLY with 'True' or 'False'" suggests they might be scripting something or want pure data extraction.

Wait no, actually the user didn't ask if it's possible but directly asked if anyone has done it. That's an important distinction. 

I know for sure that yes, people have climbed the North Face of Everest - Reinhold Messner and Ang Rita were among the first pioneers who did it in 1986-87 season. But since user insists on one-word answer only, I shouldn't mention details.

User seems to value precision over context here. Maybe they're building a database or creating multiple-choice tests where brevity matters. The no-punctuation rule makes me think this could be feeding into some automated system.

But the question itself is interesting - it's not asking about feasibility but actual achievement. That shows user understands mountaineering terminology well enough to specify "North Face" versus just general climbing questions.

Alright, keeping it simple then. Just need to confirm that yes, people have climbed it.
</think>
True
2025-07-10 08:51:34,124 - energy_eval - INFO - [deepseek-r1:8b | q | QID 1] Prediction: <think>
Okay, user wants a yes/no answer about whether anyone has climbed Everest's North Face without explanation or extra words. 

Hmm, this seems like a straightforward factual question from someone who either needs quick confirmation for research or just testing basic knowledge. The phrasing is very rigid - "Answer ONLY with 'True' or 'False'" suggests they might be scripting something or want pure data extraction.

Wait no, actually the user didn't ask if it's possible but directly asked if anyone has done it. That's an important distinction. 

I know for sure that yes, people have climbed the North Face of Everest - Reinhold Messner and Ang Rita were among the first pioneers who did it in 1986-87 season. But since user insists on one-word answer only, I shouldn't mention details.

User seems to value precision over context here. Maybe they're building a database or creating multiple-choice tests where brevity matters. The no-punctuation rule makes me think this could be feeding into some automated system.

But the question itself is interesting - it's not asking about feasibility but actual achievement. That shows user understands mountaineering terminology well enough to specify "North Face" versus just general climbing questions.

Alright, keeping it simple then. Just need to confirm that yes, people have climbed it.
</think>
True | Gold: True
2025-07-10 08:51:52,829 - energy_eval - INFO - Full output: <think>
Okay, the question is "does Air Force One fly with fighter escort?" I need to recall what I know about Air Force One and its security measures.

Air Force One is a heavily protected aircraft, so it's common for there to be an escort. The US Navy provides aerial refueling and protection during flights. So yes, fighter escorts are part of that security detail.
</think>
True
2025-07-10 08:51:52,830 - energy_eval - INFO - [deepseek-r1:8b | q | QID 2] Prediction: <think>
Okay, the question is "does Air Force One fly with fighter escort?" I need to recall what I know about Air Force One and its security measures.

Air Force One is a heavily protected aircraft, so it's common for there to be an escort. The US Navy provides aerial refueling and protection during flights. So yes, fighter escorts are part of that security detail.
</think>
True | Gold: False
2025-07-10 08:52:33,799 - energy_eval - INFO - Full output: <think>
Okay, the user is asking if there's been a Duke of Sussex before and wants me to answer only with "True" or "False". 

Hmm, I need to recall what historical titles exist in British peerage. The Duke of Sussex is an English title created by royal decree. It was first granted to Henry Fitzalan-Howard in 1867 as part of his earldom.

Wait, let me double-check that. Yes, the Duke of Sussex title dates back to Charles I creating it for his son Prince William in 1629. But after that initial creation, did anyone else hold this title? 

I remember Henry Fitzalan-Howard was indeed given the dukedom when he married Lady Florence Trevelyan. That makes sense because royal titles can be recreated or granted anew to descendants of previous holders.

The user seems curious about historical peerage but wants a quick confirmation without explanations. Maybe they're researching for something specific, like genealogy or history trivia. 

I should confirm that yes, there have been Dukes of Sussex before Henry Fitzalan-Howard in 1867. The title was established earlier and passed down through the Howard family lineage.

The answer is True because previous holders existed, even if not contemporaneously with modern understanding.
</think>
True
2025-07-10 08:52:33,799 - energy_eval - INFO - [deepseek-r1:8b | q | QID 3] Prediction: <think>
Okay, the user is asking if there's been a Duke of Sussex before and wants me to answer only with "True" or "False". 

Hmm, I need to recall what historical titles exist in British peerage. The Duke of Sussex is an English title created by royal decree. It was first granted to Henry Fitzalan-Howard in 1867 as part of his earldom.

Wait, let me double-check that. Yes, the Duke of Sussex title dates back to Charles I creating it for his son Prince William in 1629. But after that initial creation, did anyone else hold this title? 

I remember Henry Fitzalan-Howard was indeed given the dukedom when he married Lady Florence Trevelyan. That makes sense because royal titles can be recreated or granted anew to descendants of previous holders.

The user seems curious about historical peerage but wants a quick confirmation without explanations. Maybe they're researching for something specific, like genealogy or history trivia. 

I should confirm that yes, there have been Dukes of Sussex before Henry Fitzalan-Howard in 1867. The title was established earlier and passed down through the Howard family lineage.

The answer is True because previous holders existed, even if not contemporaneously with modern understanding.
</think>
True | Gold: True
2025-07-10 08:57:14,415 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=100, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 08:57:14,415 - energy_eval - INFO - Using device: cuda
2025-07-10 08:57:14,415 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 08:57:14,418 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 08:57:14,418 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 08:57:14,418 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-10 08:58:00,002 - energy_eval - INFO - Full output: <think>
Okay, user is asking if the World Cup has ever been won on penalties and wants a strict yes/no answer without explanation or punctuation. 

Hmm, this seems like a straightforward fact-check question about football history. The phrasing "has the world cup been won on penalties" suggests they're looking for confirmation whether penalty shootouts have determined any World Cup final winners.

I recall that before 1966, games were decided by extra time without penalties - just sudden death in the second half of extra time. But since then... Brazil vs Italy 1970 was a classic shootout win but wasn't official penalties yet. Then England beating Germany in 1966 via goal difference after extra time. 

Wait no, that's not what they're asking. They want to know if the final has ever been won purely on penalty kicks without any other method. The first true penalty shootout winner was West Germany beating Brazil in 1974 - but wait, that wasn't a full-time penalty shootout either. 

Actually looking deeper... Argentina vs England 1986 had extra time then penalties after goalless draw. That's the first official final decided by penalties according to FIFA rules at the time. And yes, it counts as being won on penalties.

But user just wants one word answer - no explanations needed. So "True" is correct here because there was indeed a World Cup final (1986) that went all the way to penalties after extra time.
</think>
True
2025-07-10 08:58:00,002 - energy_eval - INFO - Extracted output: <think>
Okay, user is asking if the World Cup has ever been won on penalties and wants a strict yes/no answer without explanation or punctuation. 

Hmm, this seems like a straightforward fact-check question about football history. The phrasing "has the world cup been won on penalties" suggests they're looking for confirmation whether penalty shootouts have determined any World Cup final winners.

I recall that before 1966, games were decided by extra time without penalties - just sudden death in the second half of extra time. But since then... Brazil vs Italy 1970 was a classic shootout win but wasn't official penalties yet. Then England beating Germany in 1966 via goal difference after extra time. 

Wait no, that's not what they're asking. They want to know if the final has ever been won purely on penalty kicks without any other method. The first true penalty shootout winner was West Germany beating Brazil in 1974 - but wait, that wasn't a full-time penalty shootout either. 

Actually looking deeper... Argentina vs England 1986 had extra time then penalties after goalless draw. That's the first official final decided by penalties according to FIFA rules at the time. And yes, it counts as being won on penalties.

But user just wants one word answer - no explanations needed. So "True" is correct here because there was indeed a World Cup final (1986) that went all the way to penalties after extra time.
</think>
True
2025-07-10 08:58:06,608 - energy_eval - INFO - [deepseek-r1:8b | q | QID 0] Prediction: True | Gold: True
2025-07-10 10:24:21,562 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:32b': {'q': False}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 10:24:21,562 - energy_eval - INFO - Using device: cuda
2025-07-10 10:24:21,563 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 10:24:21,566 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 10:28:06,819 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 10:28:06,819 - energy_eval - INFO - Using device: cuda
2025-07-10 10:28:06,819 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 10:28:06,824 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 10:28:06,824 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 10:28:06,824 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 10:28:06,827 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 10:29:06,162 - energy_eval - ERROR - Inference failed for idx=1: 'bool' object has no attribute 'lower'
2025-07-10 10:29:43,155 - energy_eval - ERROR - Inference failed for idx=6: 'bool' object has no attribute 'lower'
2025-07-10 10:29:52,702 - energy_eval - ERROR - Inference failed for idx=0: 'bool' object has no attribute 'lower'
2025-07-10 10:30:14,618 - energy_eval - ERROR - Inference failed for idx=7: 'bool' object has no attribute 'lower'
2025-07-10 10:30:27,542 - energy_eval - ERROR - Inference failed for idx=5: 'bool' object has no attribute 'lower'
2025-07-10 10:30:52,148 - energy_eval - ERROR - Inference failed for idx=3: 'bool' object has no attribute 'lower'
2025-07-10 10:30:57,404 - energy_eval - ERROR - Inference failed for idx=2: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:31:14,693 - energy_eval - ERROR - Inference failed for idx=4: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:31:30,506 - energy_eval - ERROR - Inference failed for idx=8: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:31:41,817 - energy_eval - ERROR - Inference failed for idx=9: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:32:00,646 - energy_eval - ERROR - Inference failed for idx=10: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:32:18,271 - energy_eval - ERROR - Inference failed for idx=11: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:32:41,864 - energy_eval - ERROR - Inference failed for idx=12: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:33:02,977 - energy_eval - ERROR - Inference failed for idx=13: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:33:12,083 - energy_eval - ERROR - Inference failed for idx=14: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:33:25,055 - energy_eval - ERROR - Inference failed for idx=16: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:33:34,143 - energy_eval - ERROR - Inference failed for idx=15: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:33:38,685 - energy_eval - ERROR - Inference failed for idx=17: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:34:11,972 - energy_eval - ERROR - Inference failed for idx=18: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:34:21,681 - energy_eval - ERROR - Inference failed for idx=19: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:34:48,112 - energy_eval - ERROR - Inference failed for idx=20: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:34:53,789 - energy_eval - ERROR - Inference failed for idx=21: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:35:12,329 - energy_eval - ERROR - Inference failed for idx=22: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:35:31,191 - energy_eval - ERROR - Inference failed for idx=23: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:35:34,272 - energy_eval - ERROR - Inference failed for idx=24: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:35:52,396 - energy_eval - ERROR - Inference failed for idx=25: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:38:03,622 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 10:38:03,622 - energy_eval - INFO - Using device: cuda
2025-07-10 10:38:03,622 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 10:38:03,625 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 10:38:03,625 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 10:38:03,625 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 10:38:03,628 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 10:38:40,675 - energy_eval - ERROR - Inference failed for idx=7: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:39:33,725 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 10:39:33,725 - energy_eval - INFO - Using device: cuda
2025-07-10 10:39:33,725 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 10:39:33,727 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 10:39:33,728 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 10:39:33,728 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 10:39:33,730 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 10:40:09,571 - energy_eval - ERROR - Inference failed for idx=2: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:40:31,317 - energy_eval - ERROR - Inference failed for idx=0: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:43:50,353 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 10:43:50,353 - energy_eval - INFO - Using device: cuda
2025-07-10 10:43:50,353 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 10:43:50,356 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 10:43:50,356 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 10:43:50,356 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 10:43:50,359 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 10:44:21,952 - energy_eval - ERROR - Inference failed for idx=2: 'EmissionsTracker' object has no attribute 'duration'
2025-07-10 10:44:38,367 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 10:44:38,367 - energy_eval - INFO - Using device: cuda
2025-07-10 10:44:38,367 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 10:44:38,370 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 10:44:38,370 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 10:44:38,370 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 10:44:38,373 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 10:45:12,067 - energy_eval - ERROR - Inference failed for idx=3: 'EmissionsTracker' object has no attribute 'id'
2025-07-10 10:45:17,063 - energy_eval - ERROR - Inference failed for idx=4: 'EmissionsTracker' object has no attribute 'id'
2025-07-10 10:45:32,850 - energy_eval - ERROR - Inference failed for idx=2: 'EmissionsTracker' object has no attribute 'id'
2025-07-10 10:45:47,341 - energy_eval - ERROR - Inference failed for idx=7: 'EmissionsTracker' object has no attribute 'id'
2025-07-10 10:46:42,604 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 10:46:42,604 - energy_eval - INFO - Using device: cuda
2025-07-10 10:46:42,604 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 10:46:42,607 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 10:46:42,607 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 10:46:42,607 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 10:46:42,610 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 10:47:29,435 - energy_eval - ERROR - Inference failed for idx=0: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:47:30,714 - energy_eval - ERROR - Inference failed for idx=6: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:49:53,303 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 10:49:53,303 - energy_eval - INFO - Using device: cuda
2025-07-10 10:49:53,303 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 10:49:53,306 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 10:49:53,306 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 10:49:53,306 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 10:49:53,309 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 10:50:19,955 - energy_eval - ERROR - Inference failed for idx=1: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:53:37,690 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 10:53:37,690 - energy_eval - INFO - Using device: cuda
2025-07-10 10:53:37,690 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 10:53:37,692 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 10:53:37,692 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 10:53:37,692 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 10:53:37,695 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 10:54:22,375 - energy_eval - ERROR - Inference failed for idx=0: 'EmissionsTracker' object has no attribute 'duration'
2025-07-10 10:54:35,312 - energy_eval - ERROR - Inference failed for idx=6: 'EmissionsTracker' object has no attribute 'duration'
2025-07-10 10:55:45,397 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 10:55:45,397 - energy_eval - INFO - Using device: cuda
2025-07-10 10:55:45,397 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 10:55:45,407 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 10:55:45,407 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 10:55:45,407 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 10:55:45,417 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 11:00:17,407 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:00:17,407 - energy_eval - INFO - Using device: cuda
2025-07-10 11:00:17,408 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:00:17,417 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:00:17,417 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 11:00:17,417 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 11:00:17,426 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 11:10:57,356 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:10:57,357 - energy_eval - INFO - Using device: cuda
2025-07-10 11:10:57,357 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:10:57,360 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:10:57,360 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 11:10:57,360 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 11:10:57,363 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 11:13:11,872 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:13:11,873 - energy_eval - INFO - Using device: cuda
2025-07-10 11:13:11,873 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:13:11,876 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:13:11,876 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 11:13:11,876 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 11:13:11,879 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 11:17:40,584 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:17:40,584 - energy_eval - INFO - Using device: cuda
2025-07-10 11:17:40,584 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:17:40,587 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:19:50,895 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:19:50,895 - energy_eval - INFO - Using device: cuda
2025-07-10 11:19:50,895 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:19:50,898 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:26:41,813 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:26:41,813 - energy_eval - INFO - Using device: cuda
2025-07-10 11:26:41,814 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:26:41,818 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:29:35,407 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:29:35,407 - energy_eval - INFO - Using device: cuda
2025-07-10 11:29:35,407 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:29:35,411 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:32:29,415 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:32:29,416 - energy_eval - INFO - Using device: cuda
2025-07-10 11:32:29,416 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:32:29,420 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:35:50,015 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:35:50,015 - energy_eval - INFO - Using device: cuda
2025-07-10 11:35:50,015 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:35:50,019 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:46:29,425 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:46:29,426 - energy_eval - INFO - Using device: cuda
2025-07-10 11:46:29,426 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:46:29,430 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:47:11,427 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 11:47:36,749 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:47:36,749 - energy_eval - INFO - Using device: cuda
2025-07-10 11:47:36,749 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:47:36,752 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:47:36,752 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 11:47:36,752 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 11:47:36,769 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 11:51:14,493 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 11:55:48,261 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:55:48,261 - energy_eval - INFO - Using device: cuda
2025-07-10 11:55:48,261 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:55:48,264 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:55:48,264 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 11:55:48,264 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 11:55:48,267 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 11:56:17,549 - energy_eval - ERROR - Critical error: Server disconnected
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 565, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 78, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 211, in run_model_mode
    responses = asyncio.run(run_async_inference(jobs, model_name))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 560, in run_async_inference
    return await asyncio.gather(*tasks)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 548, in generate_async
    async with session.post(
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 1470, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 761, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 739, in _connect_and_send_request
    await resp.start(conn)
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client_reqrep.py", line 512, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/streams.py", line 672, in read
    await self._waiter
aiohttp.client_exceptions.ServerDisconnectedError: Server disconnected
2025-07-10 11:56:48,564 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:56:48,564 - energy_eval - INFO - Using device: cuda
2025-07-10 11:56:48,564 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:56:48,567 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:56:48,567 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 11:56:48,567 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 11:56:48,570 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 11:57:07,836 - energy_eval - ERROR - Critical error: Server disconnected
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 565, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 78, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 211, in run_model_mode
    responses = asyncio.run(run_async_inference(jobs, model_name))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 560, in run_async_inference
    return await asyncio.gather(*tasks)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 548, in generate_async
    async with session.post(
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 1470, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 761, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 739, in _connect_and_send_request
    await resp.start(conn)
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client_reqrep.py", line 512, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/streams.py", line 672, in read
    await self._waiter
aiohttp.client_exceptions.ServerDisconnectedError: Server disconnected
2025-07-10 11:57:34,439 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:57:34,439 - energy_eval - INFO - Using device: cuda
2025-07-10 11:57:34,440 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:57:34,442 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:57:34,442 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 11:57:34,443 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 11:57:34,445 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 12:01:26,692 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 12:02:17,587 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 12:02:17,587 - energy_eval - INFO - Using device: cuda
2025-07-10 12:02:17,587 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 12:02:17,590 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 12:02:17,590 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 12:02:17,590 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 12:02:17,593 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 12:07:18,321 - energy_eval - ERROR - Critical error: 
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client_reqrep.py", line 512, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/streams.py", line 672, in read
    await self._waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 567, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 212, in run_model_mode
    responses = asyncio.run(run_async_inference(jobs, model_name))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 560, in run_async_inference
    return await tqdm_asyncio.gather(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/tqdm/asyncio.py", line 79, in gather
    res = [await f for f in cls.as_completed(ifs, loop=loop, timeout=timeout,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/tqdm/asyncio.py", line 79, in <listcomp>
    res = [await f for f in cls.as_completed(ifs, loop=loop, timeout=timeout,
           ^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/tasks.py", line 615, in _wait_for_one
    return f.result()  # May raise f.exception().
           ^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/tqdm/asyncio.py", line 76, in wrap_awaitable
    return i, await f
              ^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 548, in generate_async
    async with session.post(
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 1470, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 761, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 739, in _connect_and_send_request
    await resp.start(conn)
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client_reqrep.py", line 507, in start
    with self._timer:
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError
2025-07-10 12:07:18,557 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-128' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at /home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/tqdm/asyncio.py:75> exception=TimeoutError()>
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client_reqrep.py", line 512, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/streams.py", line 672, in read
    await self._waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/tqdm/asyncio.py", line 76, in wrap_awaitable
    return i, await f
              ^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 548, in generate_async
    async with session.post(
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 1470, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 761, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 739, in _connect_and_send_request
    await resp.start(conn)
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client_reqrep.py", line 507, in start
    with self._timer:
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError
2025-07-10 12:35:03,828 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 12:35:03,828 - energy_eval - INFO - Using device: cuda
2025-07-10 12:35:03,828 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 12:35:03,832 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 12:35:03,832 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 12:35:03,832 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 12:35:03,836 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 12:35:03,838 - energy_eval - ERROR - Critical error: <coroutine object as_completed.<locals>._wait_for_one at 0x7bb27d76da40>
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 598, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 285, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 233, in async_process
    idx, sample, ret_metrics = task_to_job[completed_task]
                               ~~~~~~~~~~~^^^^^^^^^^^^^^^^
KeyError: <coroutine object as_completed.<locals>._wait_for_one at 0x7bb27d76da40>
2025-07-10 12:35:36,786 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 12:35:36,786 - energy_eval - INFO - Using device: cuda
2025-07-10 12:35:36,786 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 12:35:36,789 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 12:35:36,789 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 12:35:36,789 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 12:35:36,792 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 12:35:36,793 - energy_eval - ERROR - Critical error: 'async for' requires an object with __aiter__ method, got generator
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 609, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 296, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 229, in async_process
    async for completed_coro in tqdm_asyncio.as_completed(tasks, total=len(tasks),
TypeError: 'async for' requires an object with __aiter__ method, got generator
2025-07-10 12:39:34,476 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 12:39:34,476 - energy_eval - INFO - Using device: cuda
2025-07-10 12:39:34,476 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 12:39:34,479 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 12:39:34,479 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 12:39:34,479 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 12:39:34,482 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 12:39:34,483 - energy_eval - ERROR - Critical error: Could not match completed coroutine to task
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 613, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 300, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 244, in async_process
    raise RuntimeError(
RuntimeError: Could not match completed coroutine to task
2025-07-10 12:41:33,140 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 12:41:33,140 - energy_eval - INFO - Using device: cuda
2025-07-10 12:41:33,140 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 12:41:33,143 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 12:41:33,143 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 12:41:33,143 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 12:41:33,146 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 12:41:33,148 - energy_eval - ERROR - Critical error: 'coroutine' object has no attribute 'job_info'
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 590, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 277, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 228, in async_process
    idx, sample, ret_metrics = completed_task.job_info
                               ^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'coroutine' object has no attribute 'job_info'
2025-07-10 12:42:43,876 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 12:42:43,876 - energy_eval - INFO - Using device: cuda
2025-07-10 12:42:43,876 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 12:42:43,881 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 12:42:43,881 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 12:42:43,881 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 12:42:43,886 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 12:42:43,888 - energy_eval - ERROR - Critical error: 'coroutine' object has no attribute 'job_info'
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 592, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 279, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 228, in async_process
    idx, sample, ret_metrics = completed_task.job_info
                               ^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'coroutine' object has no attribute 'job_info'
2025-07-10 12:57:16,156 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 12:57:16,156 - energy_eval - INFO - Using device: cuda
2025-07-10 12:57:16,156 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 12:57:16,160 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 12:57:16,160 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 12:57:16,160 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 12:57:16,164 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 12:57:16,166 - energy_eval - ERROR - Critical error: <coroutine object as_completed.<locals>._wait_for_one at 0x70d923141b10>
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 596, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 283, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 232, in async_process
    idx, sample, ret_metrics = task_to_job[future]
                               ~~~~~~~~~~~^^^^^^^^
KeyError: <coroutine object as_completed.<locals>._wait_for_one at 0x70d923141b10>
2025-07-10 13:01:34,241 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 13:01:34,241 - energy_eval - INFO - Using device: cuda
2025-07-10 13:01:34,241 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 13:01:34,244 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 13:01:34,244 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 13:01:34,244 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 13:01:34,249 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 13:01:34,251 - energy_eval - ERROR - Critical error: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 608, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 295, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 246, in async_process
    full_output = resp["response"]
                  ~~~~^^^^^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2025-07-10 13:03:25,395 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 13:03:25,395 - energy_eval - INFO - Using device: cuda
2025-07-10 13:03:25,395 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 13:03:25,398 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 13:03:25,398 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 13:03:25,398 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 13:03:25,401 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 13:03:25,403 - energy_eval - ERROR - Critical error: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 608, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 295, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 246, in async_process
    full_output = resp["response"]
                  ~~~~^^^^^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2025-07-10 13:03:50,347 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 13:03:50,347 - energy_eval - INFO - Using device: cuda
2025-07-10 13:03:50,347 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 13:03:50,350 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 13:03:50,350 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 13:03:50,350 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 13:03:50,353 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 13:03:50,355 - energy_eval - ERROR - Critical error: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 609, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 296, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 247, in async_process
    full_output = resp["response"]
                  ~~~~^^^^^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2025-07-10 13:04:27,186 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 13:04:27,186 - energy_eval - INFO - Using device: cuda
2025-07-10 13:04:27,186 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 13:04:27,189 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 13:04:27,189 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 13:04:27,189 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 13:04:27,192 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 13:04:27,194 - energy_eval - ERROR - Critical error: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 609, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 296, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 247, in async_process
    full_output = resp["response"]
                  ~~~~^^^^^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2025-07-10 13:04:46,692 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 13:04:46,692 - energy_eval - INFO - Using device: cuda
2025-07-10 13:04:46,692 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 13:04:46,695 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 13:04:46,695 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 13:04:46,695 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 13:04:46,698 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 13:04:46,699 - energy_eval - ERROR - Critical error: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 610, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 297, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 248, in async_process
    full_output = resp["response"]
                  ~~~~^^^^^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2025-07-10 13:04:55,935 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 13:04:55,936 - energy_eval - INFO - Using device: cuda
2025-07-10 13:04:55,936 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 13:04:55,939 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 13:04:55,939 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 13:04:55,939 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 13:04:55,942 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 13:04:55,944 - energy_eval - ERROR - Critical error: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 611, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 298, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 249, in async_process
    full_output = resp["response"]
                  ~~~~^^^^^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2025-07-10 13:14:46,081 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 13:14:46,081 - energy_eval - INFO - Using device: cuda
2025-07-10 13:14:46,081 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 13:14:46,085 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 13:14:46,085 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 13:14:46,085 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 13:14:46,088 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 13:15:57,334 - energy_eval - ERROR - Critical error: tuple indices must be integers or slices, not str
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 602, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 296, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 247, in async_process
    full_output = resp["response"]
                  ~~~~^^^^^^^^^^^^
TypeError: tuple indices must be integers or slices, not str
2025-07-10 13:16:27,880 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 13:16:27,880 - energy_eval - INFO - Using device: cuda
2025-07-10 13:16:27,880 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 13:16:27,883 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 13:16:27,883 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 13:16:27,883 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 13:16:27,886 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 13:17:41,540 - energy_eval - ERROR - Critical error: tuple indices must be integers or slices, not str
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 603, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 297, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 248, in async_process
    full_output = resp["response"]
                  ~~~~^^^^^^^^^^^^
TypeError: tuple indices must be integers or slices, not str
2025-07-10 13:18:40,925 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 13:18:40,925 - energy_eval - INFO - Using device: cuda
2025-07-10 13:18:40,925 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 13:18:40,928 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 13:18:40,928 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 13:18:40,928 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 13:18:40,931 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 13:29:56,211 - energy_eval - INFO - Completed q for deepseek-r1:1.5b in 0h 11m 15s
2025-07-10 13:29:56,310 - energy_eval - INFO - Completed deepseek-r1:1.5b in 0h 11m 15s
2025-07-10 13:29:56,310 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 11m 15s
============================================================
2025-07-10 13:32:01,316 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 13:32:01,316 - energy_eval - INFO - Using device: cuda
2025-07-10 13:32:01,316 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 13:32:01,319 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 13:32:01,319 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 13:32:01,319 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {}
++++++++++++++++++++++++++++++
2025-07-10 13:32:01,411 - energy_eval - INFO - Completed deepseek-r1:1.5b in 0h 0m 0s
2025-07-10 13:32:01,411 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 0m 0s
============================================================
2025-07-10 13:33:11,696 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 13:33:11,696 - energy_eval - INFO - Using device: cuda
2025-07-10 13:33:11,696 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 13:33:11,699 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 13:33:11,699 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 13:33:11,699 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 13:33:11,702 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 16:54:37,105 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 16:54:37,105 - energy_eval - INFO - Using device: cuda
2025-07-10 16:54:40,014 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-10 16:58:14,680 - energy_eval - INFO - Loaded wiki in 0h3m34s
2025-07-10 16:58:14,681 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 16:58:14,681 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-10 16:58:14,742 - energy_eval - INFO - Built 3270 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 17:47:35,619 - energy_eval - ERROR - Critical error: 'EmissionsTracker' object has no attribute 'final_emissions_data'
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 598, in <module>
    run()
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 79, in run
    run_model_mode(
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 292, in run_model_mode
    asyncio.run(async_process())
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\asyncio\runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\asyncio\runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 253, in async_process
    pred, inf_metrics = process_boolq_prediction(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 557, in process_boolq_prediction
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,518 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-441' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,532 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-501' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,532 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-504' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,533 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-508' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,535 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-586' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,535 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-622' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,535 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-633' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,535 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-636' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,536 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-654' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,536 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-670' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,536 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-706' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,536 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-724' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,537 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-732' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,537 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-735' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,537 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-745' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,537 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-746' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,539 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-747' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,539 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-748' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,539 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-749' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,539 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-750' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,540 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-751' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,540 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-752' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,540 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-753' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,541 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-754' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,541 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-755' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,541 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-756' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,541 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-757' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,542 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-758' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 21:42:46,867 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 21:42:46,867 - energy_eval - INFO - Using device: cuda
2025-07-10 21:42:48,941 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-10 21:45:05,963 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 21:46:05,358 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 21:46:05,358 - energy_eval - INFO - Using device: cuda
2025-07-10 21:46:07,911 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-10 21:47:26,818 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 21:48:24,565 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 21:48:24,565 - energy_eval - INFO - Using device: cuda
2025-07-10 21:48:27,055 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-10 21:49:51,419 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 21:50:03,983 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 21:50:03,983 - energy_eval - INFO - Using device: cuda
2025-07-10 21:50:03,983 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-07-10 21:50:04,003 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 21:53:38,295 - energy_eval - INFO - Loaded wiki in 0h3m34s
2025-07-10 21:53:38,295 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 21:53:38,295 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-10 21:53:38,298 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 22:03:32,282 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 9m 53s
2025-07-10 22:14:11,609 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 22:14:11,609 - energy_eval - INFO - Using device: cuda
2025-07-10 22:14:11,609 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-07-10 22:14:11,612 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 22:15:07,264 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 22:16:33,303 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 22:16:33,304 - energy_eval - INFO - Using device: cuda
2025-07-10 22:16:33,304 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-07-10 22:16:33,307 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 22:16:33,307 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-10 22:20:07,819 - energy_eval - INFO - Loaded wiki in 0h3m34s
2025-07-10 22:20:07,820 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 22:20:07,820 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-10 22:20:07,824 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 22:20:07,825 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 0m 0s
2025-07-10 22:29:48,406 - energy_eval - INFO - Built 128 prompts in 0h 9m 40s (+ retrieval: 0h 0m 3s, 0.0009 kWh, 0.0003 kg)
2025-07-10 22:39:44,331 - energy_eval - INFO - Completed q+r for deepseek-r1:8b in 0h 19m 36s
2025-07-10 22:39:50,476 - energy_eval - INFO - Completed deepseek-r1:8b in 0h 19m 42s
2025-07-10 22:39:50,476 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-07-10 22:39:50,476 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-10 22:39:50,483 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 22:49:53,498 - energy_eval - INFO - Completed q for deepseek-r1:14b in 0h 10m 3s
2025-07-10 22:59:33,377 - energy_eval - INFO - Built 128 prompts in 0h 9m 39s (+ retrieval: 0h 0m 3s, 0.0009 kWh, 0.0003 kg)
2025-07-10 23:09:29,436 - energy_eval - INFO - Completed q+r for deepseek-r1:14b in 0h 19m 35s
2025-07-10 23:09:34,486 - energy_eval - INFO - Completed deepseek-r1:14b in 0h 29m 44s
2025-07-10 23:09:34,486 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-10 23:09:34,486 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 23:09:34,492 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 23:23:26,737 - energy_eval - INFO - Completed q for deepseek-r1:32b in 0h 13m 52s
2025-07-10 23:23:31,482 - energy_eval - INFO - Completed deepseek-r1:32b in 0h 13m 56s
2025-07-10 23:23:31,482 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 6m 58s
============================================================
2025-07-10 23:52:02,767 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 23:52:02,767 - energy_eval - INFO - Using device: cuda
2025-07-10 23:52:05,090 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-10 23:52:05,090 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-10 23:55:05,066 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 23:57:56,321 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word.Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word.Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-10 23:57:56,321 - energy_eval - INFO - Using device: cuda
2025-07-10 23:57:58,456 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-10 23:57:58,456 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 00:01:36,381 - energy_eval - INFO - Loaded wiki in 0h3m37s
2025-07-11 00:01:36,381 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-11 00:01:36,381 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 00:01:36,597 - energy_eval - INFO - Built 3270 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 00:04:47,271 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 00:04:47,271 - energy_eval - INFO - Using device: cuda
2025-07-11 00:04:49,360 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-11 00:04:49,360 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 00:08:38,835 - energy_eval - INFO - Loaded wiki in 0h3m49s
2025-07-11 00:08:38,835 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-11 00:08:38,835 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 00:08:38,902 - energy_eval - INFO - Built 3253 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 00:16:52,045 - __main__ - INFO - Starting run: deepseek-mini
2025-07-11 00:16:52,045 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 00:16:52,046 - energy_eval - INFO - Using device: cuda
2025-07-11 00:16:52,046 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-07-11 00:16:52,050 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-11 00:16:52,050 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 00:17:27,727 - __main__ - INFO - Starting run: deepseek-mini
2025-07-11 00:17:27,728 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 00:17:27,728 - energy_eval - INFO - Using device: cuda
2025-07-11 00:17:27,728 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-07-11 00:17:27,731 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-11 00:17:27,731 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 00:19:46,441 - __main__ - INFO - Starting run: deepseek-mini
2025-07-11 00:19:46,441 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 00:19:46,441 - energy_eval - INFO - Using device: cuda
2025-07-11 00:19:46,441 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-07-11 00:19:46,441 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-11 00:19:46,441 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 00:23:24,967 - energy_eval - INFO - Loaded wiki in 0h3m38s
2025-07-11 00:23:24,967 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-11 00:23:24,967 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 00:23:24,967 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 00:34:11,430 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 10m 46s
2025-07-11 00:43:51,359 - energy_eval - INFO - Built 128 prompts in 0h 9m 39s (+ retrieval: 0h 0m 3s, 0.0009 kWh, 0.0003 kg)
2025-07-11 00:53:49,359 - energy_eval - INFO - Completed q+r for deepseek-r1:8b in 0h 19m 37s
2025-07-11 00:53:55,123 - energy_eval - INFO - Completed deepseek-r1:8b in 0h 30m 30s
2025-07-11 00:53:55,123 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-07-11 00:53:55,123 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 00:53:55,123 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 01:04:01,077 - energy_eval - INFO - Completed q for deepseek-r1:14b in 0h 10m 5s
2025-07-11 01:13:42,014 - energy_eval - INFO - Built 128 prompts in 0h 9m 40s (+ retrieval: 0h 0m 3s, 0.0009 kWh, 0.0003 kg)
2025-07-11 01:23:42,431 - energy_eval - INFO - Completed q+r for deepseek-r1:14b in 0h 19m 41s
2025-07-11 01:23:47,188 - energy_eval - INFO - Completed deepseek-r1:14b in 0h 29m 52s
2025-07-11 01:23:47,188 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-11 01:23:47,188 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-11 01:23:47,204 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 01:38:16,056 - energy_eval - INFO - Completed q for deepseek-r1:32b in 0h 14m 28s
2025-07-11 01:38:20,767 - energy_eval - INFO - Completed deepseek-r1:32b in 0h 14m 33s
2025-07-11 01:38:20,767 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 18m 34s
============================================================
2025-07-11 01:38:24,544 - __main__ - INFO - Starting run: deepseek-full
2025-07-11 01:38:24,544 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_full.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 01:38:24,544 - energy_eval - INFO - Using device: cuda
2025-07-11 01:38:26,433 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-11 01:38:26,449 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 01:41:57,949 - energy_eval - INFO - Loaded wiki in 0h3m31s
2025-07-11 01:41:57,949 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-11 01:41:57,949 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 01:41:58,013 - energy_eval - INFO - Built 3270 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 02:05:54,750 - __main__ - ERROR - Run deepseek-full failed: 'EmissionsTracker' object has no attribute 'final_emissions_data'
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\run_multiple.py", line 11, in safe_run
    run()
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 86, in run
    run_model_mode(
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 299, in run_model_mode
    asyncio.run(async_process())
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\asyncio\runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\asyncio\runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 260, in async_process
    pred, inf_metrics = process_boolq_prediction(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 564, in process_boolq_prediction
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-11 02:05:58,463 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-709' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:585> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 587, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-11 02:05:58,467 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-710' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:585> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 587, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-11 02:05:58,467 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-711' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:585> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 587, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-11 02:05:58,467 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-712' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:585> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 587, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-11 02:05:58,467 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-713' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:585> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 587, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-11 02:05:58,467 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-714' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:585> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 587, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-11 02:05:58,467 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-715' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:585> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 587, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-11 02:05:58,467 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-716' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:585> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 587, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-11 02:05:58,467 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-717' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:585> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 587, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-11 02:05:58,467 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-718' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:585> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 587, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-11 02:05:58,467 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-719' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:585> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 587, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-11 02:05:58,467 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-720' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:585> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 587, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-11 02:05:58,467 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-721' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:585> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 587, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-11 02:05:58,467 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-722' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:585> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 587, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-11 02:05:58,467 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-723' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:585> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 587, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-11 02:05:58,467 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-724' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:585> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 587, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-11 07:03:04,258 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 07:03:04,258 - energy_eval - INFO - Using device: cuda
2025-07-11 07:03:07,021 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-11 07:03:07,021 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 07:03:26,647 - energy_eval - WARNING - Experiment interrupted by user
2025-07-11 07:03:46,025 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 07:03:46,025 - energy_eval - INFO - Using device: cuda
2025-07-11 07:03:47,983 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-11 07:03:47,983 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 07:06:56,319 - energy_eval - WARNING - Experiment interrupted by user
2025-07-11 07:07:05,741 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 07:07:05,741 - energy_eval - INFO - Using device: cuda
2025-07-11 07:07:05,741 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-07-11 07:07:05,756 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-11 07:07:05,756 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 07:10:41,044 - energy_eval - INFO - Loaded wiki in 0h3m35s
2025-07-11 07:10:41,044 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-11 07:10:41,044 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 07:10:41,047 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 07:20:31,964 - energy_eval - INFO - Completed q for gemma3:4b in 0h 9m 50s
2025-07-11 07:30:12,970 - energy_eval - INFO - Built 128 prompts in 0h 9m 41s (+ retrieval: 0h 0m 3s, 0.0009 kWh, 0.0003 kg)
2025-07-11 07:35:04,320 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 07:35:04,321 - energy_eval - INFO - Using device: cuda
2025-07-11 07:35:04,321 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-07-11 07:35:04,324 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-11 07:35:04,324 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 07:35:40,836 - energy_eval - WARNING - Experiment interrupted by user
2025-07-11 07:35:46,851 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 07:35:46,851 - energy_eval - INFO - Using device: cuda
2025-07-11 07:35:46,851 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-07-11 07:35:46,855 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-11 07:35:46,855 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 07:39:27,791 - energy_eval - INFO - Loaded wiki in 0h3m40s
2025-07-11 07:39:27,792 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-11 07:39:27,792 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 07:39:27,794 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 07:39:27,804 - energy_eval - INFO - Completed q for gemma3:4b in 0h 0m 0s
2025-07-11 07:48:46,607 - energy_eval - INFO - Built 124 prompts in 0h 9m 18s (+ retrieval: 0h 0m 3s, 0.0009 kWh, 0.0003 kg)
2025-07-11 07:58:13,006 - energy_eval - INFO - Completed q+r for gemma3:4b in 0h 18m 45s
2025-07-11 07:58:19,101 - energy_eval - INFO - Completed gemma3:4b in 0h 18m 51s
2025-07-11 07:58:19,101 - energy_eval - INFO - 
============================================================
Running model: gemma3:12b
============================================================
2025-07-11 07:58:19,101 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 07:58:19,104 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 08:02:21,036 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 08:02:21,036 - energy_eval - INFO - Using device: cuda
2025-07-11 08:02:21,036 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-07-11 08:02:21,040 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-11 08:02:21,040 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 08:06:05,811 - energy_eval - INFO - Loaded wiki in 0h3m44s
2025-07-11 08:06:05,811 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-11 08:06:05,811 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 08:06:05,818 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 08:06:05,819 - energy_eval - INFO - Completed q for gemma3:4b in 0h 0m 0s
2025-07-11 08:06:05,821 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 08:06:05,821 - energy_eval - INFO - Completed q+r for gemma3:4b in 0h 0m 0s
2025-07-11 08:06:11,108 - energy_eval - INFO - Completed gemma3:4b in 0h 0m 5s
2025-07-11 08:06:11,108 - energy_eval - INFO - 
============================================================
Running model: gemma3:12b
============================================================
2025-07-11 08:06:11,109 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 08:06:11,112 - energy_eval - INFO - Built 80 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 08:12:23,705 - energy_eval - INFO - Completed q for gemma3:12b in 0h 6m 12s
2025-07-11 08:22:02,726 - energy_eval - INFO - Built 128 prompts in 0h 9m 39s (+ retrieval: 0h 0m 3s, 0.0009 kWh, 0.0003 kg)
2025-07-11 08:31:51,008 - energy_eval - INFO - Completed q+r for gemma3:12b in 0h 19m 27s
2025-07-11 08:31:56,460 - energy_eval - INFO - Completed gemma3:12b in 0h 25m 45s
2025-07-11 08:31:56,460 - energy_eval - INFO - 
============================================================
Running model: gemma3:27b
============================================================
2025-07-11 08:31:56,460 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-11 08:31:56,463 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 08:41:49,698 - energy_eval - INFO - Completed q for gemma3:27b in 0h 9m 53s
2025-07-11 08:41:54,390 - energy_eval - INFO - Completed gemma3:27b in 0h 9m 57s
2025-07-11 08:41:54,390 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 39m 33s
============================================================
2025-07-11 08:56:45,563 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='full.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 08:56:45,563 - energy_eval - INFO - Using device: cuda
2025-07-11 08:56:47,896 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-11 08:56:47,896 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 09:00:34,595 - energy_eval - INFO - Loaded wiki in 0h3m46s
2025-07-11 09:00:34,595 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-11 09:00:34,595 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 09:00:34,661 - energy_eval - INFO - Built 3270 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 09:26:19,896 - energy_eval - WARNING - Experiment interrupted by user
2025-07-11 09:28:09,017 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='full.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 09:28:09,017 - energy_eval - INFO - Using device: cuda
2025-07-11 09:28:13,391 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-11 09:28:13,391 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 09:31:52,956 - energy_eval - INFO - Loaded wiki in 0h3m39s
2025-07-11 09:31:52,956 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-11 09:31:52,956 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 09:31:53,035 - energy_eval - INFO - Built 3242 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 09:34:32,661 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='full.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 09:34:32,661 - energy_eval - INFO - Using device: cuda
2025-07-11 09:34:40,959 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-11 09:34:40,959 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 09:38:16,032 - energy_eval - INFO - Loaded wiki in 0h3m35s
2025-07-11 09:38:16,032 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-11 09:38:16,032 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 09:38:16,103 - energy_eval - INFO - Built 3234 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 13:56:39,932 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_dev_128.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 13:56:39,932 - energy_eval - INFO - Using device: cuda
2025-07-11 13:56:39,932 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_dev_128.jsonl
2025-07-11 13:56:39,944 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-11 13:56:39,944 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 14:00:24,275 - energy_eval - INFO - Loaded wiki in 0h3m44s
2025-07-11 14:00:24,275 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-11 14:00:24,275 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 14:00:24,278 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 14:50:19,563 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 49m 55s
2025-07-11 15:00:01,029 - energy_eval - INFO - Built 128 prompts in 0h 9m 41s (+ retrieval: 0h 0m 4s, 0.0013 kWh, 0.0004 kg)
2025-07-11 15:35:42,100 - energy_eval - INFO - Completed q+r for deepseek-r1:8b in 0h 45m 22s
2025-07-11 15:36:00,314 - energy_eval - INFO - Completed deepseek-r1:8b in 1h 35m 36s
2025-07-11 15:36:00,314 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-07-11 15:36:00,314 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 15:36:00,317 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 15:48:33,559 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_dev_128.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 15:48:33,559 - energy_eval - INFO - Using device: cuda
2025-07-11 15:48:33,559 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_dev_128.jsonl
2025-07-11 15:48:33,567 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-11 15:48:33,567 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 15:52:16,383 - energy_eval - INFO - Loaded wiki in 0h3m42s
2025-07-11 15:52:16,383 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-11 15:52:16,383 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 15:52:16,383 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-8b_q_128_dev_think.csv
2025-07-11 15:52:16,386 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 16:08:52,357 - energy_eval - ERROR - Critical error: 'choices'
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 553, in <module>
    run()
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 83, in run
    run_model_mode(
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 222, in run_model_mode
    full_output, inf_metrics = inference(
                               ^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 91, in inference
    text = inference_ollama(prompt, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 33, in inference_ollama
    return resp.get("response") or resp["choices"][0]["text"]
                                   ~~~~^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\venv\Lib\site-packages\ollama\_types.py", line 33, in __getitem__
    raise KeyError(key)
KeyError: 'choices'
2025-07-11 16:15:20,540 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_dev_128.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 16:15:20,540 - energy_eval - INFO - Using device: cuda
2025-07-11 16:15:20,541 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_dev_128.jsonl
2025-07-11 16:15:20,544 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-11 16:15:20,544 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 16:18:00,453 - energy_eval - WARNING - Experiment interrupted by user
2025-07-11 16:21:38,167 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_dev_128.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 16:21:38,168 - energy_eval - INFO - Using device: cuda
2025-07-11 16:21:38,168 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_dev_128.jsonl
2025-07-11 16:21:38,178 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-11 16:21:38,178 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 16:25:42,197 - energy_eval - INFO - Loaded wiki in 0h4m4s
2025-07-11 16:25:42,197 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-11 16:25:42,197 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 16:25:42,198 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-8b_q_128_dev_think.csv
2025-07-11 16:25:42,206 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 16:42:22,297 - energy_eval - ERROR - Critical error: list index out of range
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 553, in <module>
    run()
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 83, in run
    run_model_mode(
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 226, in run_model_mode
    pred = extract_prediction(full_output)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 474, in extract_prediction
    return lines[-1]
           ~~~~~^^^^
IndexError: list index out of range
2025-07-11 16:45:18,397 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_dev_128.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 16:45:18,397 - energy_eval - INFO - Using device: cuda
2025-07-11 16:45:18,397 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_dev_128.jsonl
2025-07-11 16:45:18,400 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-11 16:45:18,400 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 16:48:54,793 - energy_eval - INFO - Loaded wiki in 0h3m36s
2025-07-11 16:48:54,793 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-11 16:48:54,793 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 16:48:54,793 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-8b_q_128_dev_think.csv
2025-07-11 16:48:54,797 - energy_eval - INFO - Built 76 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 16:54:04,582 - energy_eval - WARNING - Experiment interrupted by user
2025-07-11 16:54:37,141 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_dev_128.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 16:54:37,141 - energy_eval - INFO - Using device: cuda
2025-07-11 16:54:37,142 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_dev_128.jsonl
2025-07-11 16:54:37,145 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-11 16:54:37,145 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 16:54:38,621 - energy_eval - WARNING - Experiment interrupted by user
2025-07-11 16:54:50,744 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_dev_128.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 16:54:50,744 - energy_eval - INFO - Using device: cuda
2025-07-11 16:54:50,744 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_dev_128.jsonl
2025-07-11 16:54:50,748 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-11 16:54:50,748 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 16:58:24,447 - energy_eval - INFO - Loaded wiki in 0h3m33s
2025-07-11 16:58:24,448 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-11 16:58:24,448 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 16:58:24,448 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-8b_q_128_dev_think.csv
2025-07-11 16:58:24,451 - energy_eval - INFO - Built 60 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 17:31:18,982 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 32m 54s
2025-07-11 17:31:18,982 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-8b_q+r_128_dev_think.csv
2025-07-11 17:40:58,550 - energy_eval - INFO - Built 128 prompts in 0h 9m 39s (+ retrieval: 0h 0m 3s, 0.0009 kWh, 0.0003 kg)
2025-07-11 18:08:53,637 - energy_eval - INFO - Completed q+r for deepseek-r1:8b in 0h 37m 34s
2025-07-11 18:08:59,651 - energy_eval - INFO - Completed deepseek-r1:8b in 1h 10m 35s
2025-07-11 18:08:59,651 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 14m 8s
============================================================
2025-07-11 18:10:47,023 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_dev_128.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 18:10:47,023 - energy_eval - INFO - Using device: cuda
2025-07-11 18:10:47,023 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_dev_128.jsonl
2025-07-11 18:10:47,026 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-11 18:10:47,026 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 18:14:22,094 - energy_eval - INFO - Loaded wiki in 0h3m35s
2025-07-11 18:14:22,094 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-11 18:14:22,094 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 18:14:22,094 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-8b_q_128_dev_think.csv
2025-07-11 18:14:22,100 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 18:14:30,652 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 0m 8s
2025-07-11 18:14:30,652 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-8b_q+r_128_dev_think.csv
2025-07-11 18:14:30,653 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 18:14:37,655 - energy_eval - INFO - Completed q+r for deepseek-r1:8b in 0h 0m 7s
2025-07-11 18:14:42,998 - energy_eval - INFO - Completed deepseek-r1:8b in 0h 0m 20s
2025-07-11 18:14:42,998 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-07-11 18:14:42,998 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 18:14:42,998 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-14b_q_128_dev_think.csv
2025-07-11 18:14:43,001 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 18:45:54,423 - energy_eval - INFO - Completed q for deepseek-r1:14b in 0h 31m 11s
2025-07-11 18:45:54,423 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-14b_q+r_128_dev_think.csv
2025-07-11 18:55:32,738 - energy_eval - INFO - Built 128 prompts in 0h 9m 38s (+ retrieval: 0h 0m 3s, 0.0009 kWh, 0.0003 kg)
2025-07-11 19:23:13,544 - energy_eval - INFO - Completed q+r for deepseek-r1:14b in 0h 37m 19s
2025-07-11 19:23:18,963 - energy_eval - INFO - Completed deepseek-r1:14b in 1h 8m 35s
2025-07-11 19:23:18,963 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-11 19:23:18,963 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-11 19:23:18,964 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-32b_q_128_dev_think.csv
2025-07-11 19:23:18,966 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 20:05:17,231 - energy_eval - INFO - Completed q for deepseek-r1:32b in 0h 41m 58s
2025-07-11 20:05:21,927 - energy_eval - INFO - Completed deepseek-r1:32b in 0h 42m 2s
2025-07-11 20:05:21,927 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 54m 34s
============================================================
2025-07-11 22:57:02,213 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_dev_128.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-11 22:57:02,213 - energy_eval - INFO - Using device: cuda
2025-07-11 22:57:02,213 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_dev_128.jsonl
2025-07-11 22:57:02,217 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-11 22:57:02,217 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 23:00:43,096 - energy_eval - INFO - Loaded wiki in 0h3m40s
2025-07-11 23:00:43,096 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-11 23:00:43,096 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 23:00:43,097 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-8b_q_128_dev.csv
2025-07-11 23:00:43,099 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-11 23:25:45,672 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 25m 2s
2025-07-11 23:25:45,672 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-8b_q+r_128_dev.csv
2025-07-11 23:35:24,829 - energy_eval - INFO - Built 128 prompts in 0h 9m 39s (+ retrieval: 0h 0m 3s, 0.0008 kWh, 0.0002 kg)
2025-07-11 23:55:59,283 - energy_eval - INFO - Completed q+r for deepseek-r1:8b in 0h 30m 13s
2025-07-11 23:56:05,356 - energy_eval - INFO - Completed deepseek-r1:8b in 0h 55m 22s
2025-07-11 23:56:05,356 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-07-11 23:56:05,356 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 23:56:05,356 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-14b_q_128_dev.csv
2025-07-11 23:56:05,359 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-12 00:16:46,126 - energy_eval - INFO - Completed q for deepseek-r1:14b in 0h 20m 40s
2025-07-12 00:16:46,126 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-14b_q+r_128_dev.csv
2025-07-12 00:26:25,659 - energy_eval - INFO - Built 128 prompts in 0h 9m 39s (+ retrieval: 0h 0m 3s, 0.0008 kWh, 0.0002 kg)
2025-07-12 00:46:44,662 - energy_eval - INFO - Completed q+r for deepseek-r1:14b in 0h 29m 58s
2025-07-12 00:46:49,493 - energy_eval - INFO - Completed deepseek-r1:14b in 0h 50m 44s
2025-07-12 00:46:49,493 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-12 00:46:49,493 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-12 00:46:49,493 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-32b_q_128_dev.csv
2025-07-12 00:46:49,496 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-12 01:07:13,345 - energy_eval - INFO - Completed q for deepseek-r1:32b in 0h 20m 23s
2025-07-12 01:07:18,075 - energy_eval - INFO - Completed deepseek-r1:32b in 0h 20m 28s
2025-07-12 01:07:18,076 - energy_eval - INFO - 
============================================================
Experiment completed in 2h 10m 15s
============================================================
2025-07-12 07:13:01,336 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='full.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-12 07:13:01,337 - energy_eval - INFO - Using device: cuda
2025-07-12 07:13:04,054 - energy_eval - INFO - Loaded dataset with 9427 samples
2025-07-12 07:13:04,054 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-12 07:16:47,929 - energy_eval - INFO - Loaded wiki in 0h3m43s
2025-07-12 07:16:47,930 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-12 07:16:47,930 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-12 07:16:47,930 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-8b_q_think.csv
2025-07-12 07:16:48,110 - energy_eval - INFO - Built 9427 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-12 08:08:54,862 - energy_eval - WARNING - Experiment interrupted by user
2025-07-12 08:09:20,382 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='full.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-12 08:09:20,382 - energy_eval - INFO - Using device: cuda
2025-07-12 08:09:23,049 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-12 08:09:23,049 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-12 08:12:58,345 - energy_eval - INFO - Loaded wiki in 0h3m35s
2025-07-12 08:12:58,345 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-12 08:12:58,345 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-12 08:12:58,345 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-8b_q_think.csv
2025-07-12 08:12:58,410 - energy_eval - INFO - Built 3270 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-12 15:01:35,415 - energy_eval - INFO - Completed q for deepseek-r1:8b in 6h 48m 37s
2025-07-12 15:01:35,415 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-8b_q+r_think.csv
2025-07-12 16:33:54,190 - energy_eval - INFO - Built 3270 prompts in 1h 32m 18s (+ retrieval: 0h 1m 47s, 0.0284 kWh, 0.0083 kg)
2025-07-12 22:08:31,348 - energy_eval - INFO - Completed q+r for deepseek-r1:8b in 7h 6m 55s
2025-07-12 22:08:55,350 - energy_eval - INFO - Completed deepseek-r1:8b in 13h 55m 57s
2025-07-12 22:08:55,350 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-07-12 22:08:55,351 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-12 22:08:55,351 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-14b_q_think.csv
2025-07-12 22:08:55,432 - energy_eval - INFO - Built 3270 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-13 06:18:13,185 - energy_eval - INFO - Completed q for deepseek-r1:14b in 8h 9m 17s
2025-07-13 06:18:13,185 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-14b_q+r_think.csv
2025-07-13 07:43:07,638 - energy_eval - INFO - Built 3270 prompts in 1h 24m 54s (+ retrieval: 0h 1m 36s, 0.0238 kWh, 0.0070 kg)
2025-07-13 14:45:50,952 - energy_eval - INFO - Completed q+r for deepseek-r1:14b in 8h 27m 37s
2025-07-13 14:46:13,281 - energy_eval - INFO - Completed deepseek-r1:14b in 16h 37m 17s
2025-07-13 14:46:13,281 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-13 14:46:13,281 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-13 14:46:13,281 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-32b_q_think.csv
2025-07-13 14:46:13,355 - energy_eval - INFO - Built 3270 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-13 17:30:19,438 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='full.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-13 17:30:19,438 - energy_eval - INFO - Using device: cuda
2025-07-13 17:30:22,130 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-13 17:30:22,131 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-13 17:34:05,457 - energy_eval - INFO - Loaded wiki in 0h3m43s
2025-07-13 17:34:05,457 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-13 17:34:05,457 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-13 17:34:05,458 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-8b_q_think.csv
2025-07-13 17:34:05,482 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-13 17:34:17,679 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 0m 12s
2025-07-13 17:34:17,679 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-8b_q+r_think.csv
2025-07-13 17:34:17,700 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-13 17:34:20,625 - energy_eval - INFO - Completed q+r for deepseek-r1:8b in 0h 0m 2s
2025-07-13 17:34:26,428 - energy_eval - INFO - Completed deepseek-r1:8b in 0h 0m 20s
2025-07-13 17:34:26,428 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-07-13 17:34:26,428 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-13 17:34:26,428 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-14b_q_think.csv
2025-07-13 17:34:26,444 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-13 17:34:34,354 - energy_eval - INFO - Completed q for deepseek-r1:14b in 0h 0m 7s
2025-07-13 17:34:34,354 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-14b_q+r_think.csv
2025-07-13 17:34:34,372 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-13 17:34:36,187 - energy_eval - INFO - Completed q+r for deepseek-r1:14b in 0h 0m 1s
2025-07-13 17:34:41,369 - energy_eval - INFO - Completed deepseek-r1:14b in 0h 0m 14s
2025-07-13 17:34:41,369 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-13 17:34:41,370 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-13 17:34:41,370 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-32b_q_think.csv
2025-07-13 17:34:41,481 - energy_eval - INFO - Built 3270 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-13 21:56:53,045 - __main__ - INFO - Starting run: deepseek-big-think
2025-07-13 21:56:53,045 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='full.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-13 21:56:53,045 - energy_eval - INFO - Using device: cuda
2025-07-13 21:56:58,515 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-13 21:56:58,515 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-13 22:00:50,064 - energy_eval - INFO - Loaded wiki in 0h3m51s
2025-07-13 22:00:50,064 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-13 22:00:50,064 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-13 22:00:50,064 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-8b_q_think.csv
2025-07-13 22:00:50,072 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-13 22:01:01,367 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 0m 11s
2025-07-13 22:01:01,367 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-8b_q+r_think.csv
2025-07-13 22:01:01,374 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-13 22:01:07,432 - energy_eval - INFO - Completed q+r for deepseek-r1:8b in 0h 0m 6s
2025-07-13 22:01:12,908 - energy_eval - INFO - Completed deepseek-r1:8b in 0h 0m 22s
2025-07-13 22:01:12,908 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-07-13 22:01:12,908 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-13 22:01:12,908 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-14b_q_think.csv
2025-07-13 22:01:12,908 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-13 22:01:23,481 - energy_eval - INFO - Completed q for deepseek-r1:14b in 0h 0m 10s
2025-07-13 22:01:23,481 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-14b_q+r_think.csv
2025-07-13 22:01:23,494 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-13 22:01:28,496 - energy_eval - INFO - Completed q+r for deepseek-r1:14b in 0h 0m 5s
2025-07-13 22:01:33,506 - energy_eval - INFO - Completed deepseek-r1:14b in 0h 0m 20s
2025-07-13 22:01:33,506 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-13 22:01:33,506 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-13 22:01:33,506 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-32b_q_think.csv
2025-07-13 22:01:33,621 - energy_eval - INFO - Built 3270 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-14 15:49:13,179 - energy_eval - INFO - Completed q for deepseek-r1:32b in 17h 47m 39s
2025-07-14 15:49:22,947 - energy_eval - INFO - Completed deepseek-r1:32b in 17h 47m 49s
2025-07-14 15:49:22,947 - energy_eval - INFO - 
============================================================
Experiment completed in 17h 52m 29s
============================================================
2025-07-14 15:49:28,514 - __main__ - INFO - Starting run: gemma3-big
2025-07-14 15:49:28,514 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='google/boolq', dataset_file='full.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-14 15:49:28,514 - energy_eval - INFO - Using device: cuda
2025-07-14 15:49:30,611 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-14 15:49:30,611 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-14 15:53:05,833 - energy_eval - INFO - Loaded wiki in 0h3m35s
2025-07-14 15:53:05,833 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-14 15:53:05,833 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-14 15:53:05,833 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_gemma3-4b_q_think.csv
2025-07-14 15:53:05,897 - energy_eval - INFO - Built 3270 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-14 15:53:10,487 - __main__ - ERROR - Run gemma3-big failed: registry.ollama.ai/library/gemma3:4b does not support thinking (status code: 400)
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\run_multiple.py", line 11, in safe_run
    run()
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 86, in run
    run_model_mode(
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 222, in run_model_mode
    _ = inference("Ready?", model_name, mode_tag, provider)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 96, in inference
    text = inference_ollama(prompt, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 22, in inference_ollama
    resp = generate(
           ^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\venv\Lib\site-packages\ollama\_client.py", line 247, in generate
    return self._request(
           ^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\venv\Lib\site-packages\ollama\_client.py", line 180, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\venv\Lib\site-packages\ollama\_client.py", line 124, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: registry.ollama.ai/library/gemma3:4b does not support thinking (status code: 400)
2025-07-14 16:16:54,861 - __main__ - INFO - Starting run: gemma3-big
2025-07-14 16:16:54,861 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='google/boolq', dataset_file='full.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-14 16:16:54,861 - energy_eval - INFO - Using device: cuda
2025-07-14 16:16:57,676 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-14 16:16:57,676 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-14 16:21:26,113 - energy_eval - INFO - Loaded wiki in 0h4m28s
2025-07-14 16:21:26,113 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-14 16:21:26,113 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-14 16:21:26,113 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_gemma3-4b_q.csv
2025-07-14 16:21:26,225 - energy_eval - INFO - Built 3270 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-15 00:46:14,629 - energy_eval - INFO - Completed q for gemma3:4b in 8h 24m 48s
2025-07-15 00:46:14,629 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_gemma3-4b_q+r.csv
2025-07-15 05:05:21,327 - energy_eval - INFO - Built 3270 prompts in 4h 19m 6s (+ retrieval: 0h 1m 29s, 0.0229 kWh, 0.0043 kg)
2025-07-15 13:57:18,204 - energy_eval - INFO - Completed q+r for gemma3:4b in 13h 11m 3s
2025-07-15 13:57:42,801 - energy_eval - INFO - Completed gemma3:4b in 21h 36m 16s
2025-07-15 13:57:42,801 - energy_eval - INFO - 
============================================================
Running model: gemma3:12b
============================================================
2025-07-15 13:57:42,802 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-15 13:57:42,802 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_gemma3-12b_q.csv
2025-07-15 13:57:42,879 - energy_eval - INFO - Built 3270 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-15 22:31:24,269 - energy_eval - INFO - Completed q for gemma3:12b in 8h 33m 41s
2025-07-15 22:31:24,270 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_gemma3-12b_q+r.csv
2025-07-16 02:39:04,312 - energy_eval - INFO - Built 3270 prompts in 4h 7m 40s (+ retrieval: 0h 1m 35s, 0.0245 kWh, 0.0072 kg)
2025-07-16 11:15:00,039 - energy_eval - INFO - Completed q+r for gemma3:12b in 12h 43m 35s
2025-07-16 11:15:19,858 - energy_eval - INFO - Completed gemma3:12b in 21h 17m 37s
2025-07-16 11:15:19,858 - energy_eval - INFO - 
============================================================
Running model: gemma3:27b
============================================================
2025-07-16 11:15:19,858 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-16 11:15:19,858 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_gemma3-27b_q.csv
2025-07-16 11:15:19,946 - energy_eval - INFO - Built 3270 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-16 19:47:56,150 - energy_eval - INFO - Completed q for gemma3:27b in 8h 32m 36s
2025-07-16 19:48:01,934 - energy_eval - INFO - Completed gemma3:27b in 8h 32m 42s
2025-07-16 19:48:01,934 - energy_eval - INFO - 
============================================================
Experiment completed in 51h 31m 7s
============================================================
2025-07-17 20:44:42,896 - __main__ - INFO - Starting run: deepseek-big-think
2025-07-17 20:44:42,896 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='full.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-17 20:44:42,897 - energy_eval - INFO - Using device: cuda
2025-07-17 20:44:46,049 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-17 20:44:46,049 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-17 20:49:06,099 - energy_eval - INFO - Loaded wiki in 0h4m20s
2025-07-17 20:49:06,099 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-17 20:49:06,099 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-17 20:49:06,099 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-8b_q_think.csv
2025-07-17 20:49:06,117 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-17 20:49:21,399 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 0m 15s
2025-07-17 20:49:21,399 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-8b_q+r_think.csv
2025-07-17 20:49:21,422 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-17 20:49:29,643 - energy_eval - INFO - Completed q+r for deepseek-r1:8b in 0h 0m 8s
2025-07-17 20:49:37,286 - energy_eval - INFO - Completed deepseek-r1:8b in 0h 0m 31s
2025-07-17 20:49:37,287 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-07-17 20:49:37,287 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-17 20:49:37,287 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-14b_q_think.csv
2025-07-17 20:49:37,299 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-17 20:49:49,281 - energy_eval - INFO - Completed q for deepseek-r1:14b in 0h 0m 11s
2025-07-17 20:49:49,282 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-14b_q+r_think.csv
2025-07-17 20:49:49,289 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-17 20:49:54,584 - energy_eval - INFO - Completed q+r for deepseek-r1:14b in 0h 0m 5s
2025-07-17 20:50:01,401 - energy_eval - INFO - Completed deepseek-r1:14b in 0h 0m 24s
2025-07-17 20:50:01,402 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-17 20:50:01,402 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-17 20:50:01,402 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-32b_q_think.csv
2025-07-17 20:50:01,408 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-17 20:50:27,785 - energy_eval - INFO - Completed q for deepseek-r1:32b in 0h 0m 26s
2025-07-17 20:50:34,758 - energy_eval - INFO - Completed deepseek-r1:32b in 0h 0m 33s
2025-07-17 20:50:34,759 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 5m 51s
============================================================
2025-07-17 20:50:41,508 - __main__ - INFO - Starting run: gemma3-big
2025-07-17 20:50:41,508 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='google/boolq', dataset_file='full.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-17 20:50:41,508 - energy_eval - INFO - Using device: cuda
2025-07-17 20:50:43,558 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-17 20:50:43,559 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-17 20:54:46,935 - energy_eval - INFO - Loaded wiki in 0h4m3s
2025-07-17 20:54:46,936 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-17 20:54:46,936 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-17 20:54:46,936 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_gemma3-4b_q.csv
2025-07-17 20:54:46,953 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-17 20:55:02,097 - energy_eval - INFO - Completed q for gemma3:4b in 0h 0m 15s
2025-07-17 20:55:02,097 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_gemma3-4b_q+r.csv
2025-07-17 20:55:02,107 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-17 20:55:07,287 - energy_eval - INFO - Completed q+r for gemma3:4b in 0h 0m 5s
2025-07-17 20:55:14,634 - energy_eval - INFO - Completed gemma3:4b in 0h 0m 27s
2025-07-17 20:55:14,634 - energy_eval - INFO - 
============================================================
Running model: gemma3:12b
============================================================
2025-07-17 20:55:14,635 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-17 20:55:14,635 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_gemma3-12b_q.csv
2025-07-17 20:55:14,645 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-17 20:55:25,691 - energy_eval - INFO - Completed q for gemma3:12b in 0h 0m 11s
2025-07-17 20:55:25,691 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_gemma3-12b_q+r.csv
2025-07-17 20:55:25,702 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-17 20:55:30,832 - energy_eval - INFO - Completed q+r for gemma3:12b in 0h 0m 5s
2025-07-17 20:55:37,372 - energy_eval - INFO - Completed gemma3:12b in 0h 0m 22s
2025-07-17 20:55:37,372 - energy_eval - INFO - 
============================================================
Running model: gemma3:27b
============================================================
2025-07-17 20:55:37,373 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-17 20:55:37,373 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_gemma3-27b_q.csv
2025-07-17 20:55:37,378 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-17 20:55:58,604 - energy_eval - INFO - Completed q for gemma3:27b in 0h 0m 21s
2025-07-17 20:56:05,251 - energy_eval - INFO - Completed gemma3:27b in 0h 0m 27s
2025-07-17 20:56:05,251 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 5m 23s
============================================================
2025-07-17 22:07:05,270 - __main__ - INFO - Starting run: deepseek-big-think
2025-07-17 22:07:05,271 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='full.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-17 22:07:05,271 - energy_eval - INFO - Using device: cuda
2025-07-17 22:07:08,543 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-17 22:07:08,544 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-17 22:12:08,122 - energy_eval - INFO - Loaded wiki in 0h4m59s
2025-07-17 22:12:08,122 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-17 22:12:08,122 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-17 22:12:08,122 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-8b_q_think.csv
2025-07-17 22:12:08,160 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-17 22:12:34,324 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 0m 26s
2025-07-17 22:12:34,324 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-8b_q+r_think.csv
2025-07-18 02:45:40,674 - energy_eval - INFO - Built 3270 prompts in 4h 33m 6s (+ retrieval: 0h 2m 10s, 0.0416 kWh, 0.0108 kg)
2025-07-18 18:52:18,601 - energy_eval - INFO - Completed q+r for deepseek-r1:8b in 20h 39m 44s
2025-07-18 18:52:42,159 - energy_eval - INFO - Completed deepseek-r1:8b in 20h 40m 34s
2025-07-18 18:52:42,159 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-07-18 18:52:42,159 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-18 18:52:42,159 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-14b_q_think.csv
2025-07-18 18:52:42,185 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-18 18:52:59,780 - energy_eval - INFO - Completed q for deepseek-r1:14b in 0h 0m 17s
2025-07-18 18:52:59,780 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-14b_q+r_think.csv
2025-07-18 18:52:59,800 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-18 18:53:08,355 - energy_eval - INFO - Completed q+r for deepseek-r1:14b in 0h 0m 8s
2025-07-18 18:53:14,723 - energy_eval - INFO - Completed deepseek-r1:14b in 0h 0m 32s
2025-07-18 18:53:14,724 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-18 18:53:14,724 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-18 18:53:14,724 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\boolq_deepseek-r1-32b_q_think.csv
2025-07-18 18:53:14,747 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-18 18:54:23,866 - energy_eval - INFO - Completed q for deepseek-r1:32b in 0h 1m 9s
2025-07-18 18:54:32,499 - energy_eval - INFO - Completed deepseek-r1:32b in 0h 1m 17s
2025-07-18 18:54:32,499 - energy_eval - INFO - 
============================================================
Experiment completed in 20h 47m 27s
============================================================
2025-07-18 18:54:42,554 - __main__ - INFO - Starting run: gemma3-big
2025-07-18 18:54:42,559 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='google/boolq', dataset_file='full.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-18 18:54:42,559 - energy_eval - INFO - Using device: cuda
2025-07-18 18:54:44,513 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-18 18:54:44,513 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-19 10:27:16,336 - __main__ - INFO - Starting run: deepseek-think
2025-07-19 10:27:16,336 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='boolq_mini_dev_128.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-19 10:27:16,336 - energy_eval - INFO - Using device: cuda
2025-07-19 10:27:16,336 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_dev_128.jsonl
2025-07-19 10:27:16,343 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-19 10:27:16,343 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-19 10:30:52,431 - energy_eval - INFO - Loaded wiki in 0h3m36s
2025-07-19 10:30:52,431 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-19 10:30:52,431 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-19 10:30:52,454 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q_128_dev_think.csv
2025-07-19 10:30:52,454 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-19 11:16:12,753 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 45m 20s
2025-07-19 11:16:12,753 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q+r_128_dev_think.csv
2025-07-19 11:16:17,270 - __main__ - ERROR - Run deepseek-think failed: 'context'
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\run_multiple.py", line 12, in safe_run
    run()
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 84, in run
    run_model_mode(
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 208, in run_model_mode
    ctx, ret_metrics = retrieve_context(sample, wiki_data)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 375, in retrieve_context
    sent for section in sample["context"]["sentences"] for sent in section
                        ~~~~~~^^^^^^^^^^^
KeyError: 'context'
2025-07-19 11:16:17,278 - __main__ - INFO - Starting run: deepseek-no-think
2025-07-19 11:16:17,278 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='boolq_mini_128.json', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-19 11:16:17,278 - energy_eval - INFO - Using device: cuda
2025-07-19 11:16:18,887 - energy_eval - INFO - Loaded dataset with 90447 samples
2025-07-19 11:16:18,887 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-19 11:20:16,370 - energy_eval - INFO - Loaded wiki in 0h3m57s
2025-07-19 11:20:16,370 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-19 11:20:16,370 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-19 11:20:16,370 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q_128.csv
2025-07-19 11:20:25,254 - energy_eval - INFO - Built 90447 prompts in 0h 0m 8s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-19 18:30:24,800 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='boolq_mini_dev_128.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-19 18:30:24,800 - energy_eval - INFO - Using device: cuda
2025-07-19 18:30:24,800 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_dev_128.jsonl
2025-07-19 18:30:24,816 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-19 18:30:24,816 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-19 18:50:09,536 - energy_eval - INFO - Loaded wiki in 0h19m44s
2025-07-19 18:50:09,536 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-19 18:50:09,537 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-19 18:50:09,542 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q_128_dev_think.csv
2025-07-19 18:50:09,558 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-19 18:50:23,840 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 0m 14s
2025-07-19 18:50:23,840 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q+r_128_dev_think.csv
2025-07-19 23:26:44,919 - energy_eval - WARNING - Experiment interrupted by user
2025-07-19 23:26:48,853 - __main__ - INFO - Starting run: deepseek-think
2025-07-19 23:26:48,854 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='boolq_mini_dev_128.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-19 23:26:48,854 - energy_eval - INFO - Using device: cuda
2025-07-19 23:26:48,854 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_dev_128.jsonl
2025-07-19 23:26:48,864 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-19 23:26:48,864 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-19 23:30:43,966 - energy_eval - INFO - Loaded wiki in 0h3m55s
2025-07-19 23:30:43,966 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-19 23:30:43,967 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-19 23:30:43,968 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q_128_dev_think.csv
2025-07-19 23:30:44,061 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-19 23:30:55,846 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 0m 11s
2025-07-19 23:30:55,846 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q+r_128_dev_think.csv
2025-07-19 23:40:38,841 - energy_eval - INFO - Built 128 prompts in 0h 9m 42s (+ retrieval: 0h 0m 3s, 0.0010 kWh, 0.0003 kg)
2025-07-20 00:25:55,879 - energy_eval - INFO - Completed q+r for deepseek-r1:8b in 0h 55m 0s
2025-07-20 00:26:13,289 - energy_eval - INFO - Completed deepseek-r1:8b in 0h 55m 29s
2025-07-20 00:26:13,289 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-07-20 00:26:13,289 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-20 00:26:13,289 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-14b_q_128_dev_think.csv
2025-07-20 00:26:13,292 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-20 01:02:46,118 - energy_eval - INFO - Completed q for deepseek-r1:14b in 0h 36m 32s
2025-07-20 01:02:46,119 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-14b_q+r_128_dev_think.csv
2025-07-20 01:16:54,809 - energy_eval - INFO - Built 128 prompts in 0h 14m 8s (+ retrieval: 0h 0m 4s, 0.0013 kWh, 0.0003 kg)
2025-07-20 01:45:14,141 - energy_eval - INFO - Completed q+r for deepseek-r1:14b in 0h 42m 28s
2025-07-20 01:45:20,433 - energy_eval - INFO - Completed deepseek-r1:14b in 1h 19m 7s
2025-07-20 01:45:20,433 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-20 01:45:20,433 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-20 01:45:20,434 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-32b_q_128_dev_think.csv
2025-07-20 01:45:20,436 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-20 02:33:51,796 - energy_eval - INFO - Completed q for deepseek-r1:32b in 0h 48m 31s
2025-07-20 02:33:56,617 - energy_eval - INFO - Completed deepseek-r1:32b in 0h 48m 36s
2025-07-20 02:33:56,617 - energy_eval - INFO - 
============================================================
Experiment completed in 3h 7m 7s
============================================================
2025-07-20 02:34:03,388 - __main__ - INFO - Starting run: deepseek-no-think
2025-07-20 02:34:03,388 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='boolq_mini_128.json', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-20 02:34:03,388 - energy_eval - INFO - Using device: cuda
2025-07-20 02:34:04,984 - energy_eval - INFO - Loaded dataset with 90447 samples
2025-07-20 02:34:04,985 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-20 02:37:41,193 - energy_eval - INFO - Loaded wiki in 0h3m36s
2025-07-20 02:37:41,193 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-20 02:37:41,193 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-20 02:37:41,193 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q_128.csv
2025-07-20 02:37:50,869 - energy_eval - INFO - Built 89935 prompts in 0h 0m 9s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-20 10:27:59,480 - __main__ - INFO - Starting run: deepseek-think
2025-07-20 10:27:59,480 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-20 10:27:59,482 - energy_eval - INFO - Using device: cuda
2025-07-20 10:28:00,624 - energy_eval - INFO - Loaded dataset with 90447 samples
2025-07-20 10:28:00,624 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-20 10:31:42,526 - energy_eval - INFO - Loaded wiki in 0h3m41s
2025-07-20 10:31:42,526 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-20 10:31:42,526 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-20 10:31:42,527 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q_128_dev_think.csv
2025-07-20 10:31:51,855 - energy_eval - INFO - Built 90447 prompts in 0h 0m 9s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-20 10:56:03,087 - __main__ - INFO - Starting run: deepseek-think
2025-07-20 10:56:03,088 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-20 10:56:03,088 - energy_eval - INFO - Using device: cuda
2025-07-20 10:56:03,088 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-20 10:56:03,104 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-20 10:56:03,105 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-20 11:00:00,452 - energy_eval - INFO - Loaded wiki in 0h3m57s
2025-07-20 11:00:00,452 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-20 11:00:00,452 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-20 11:00:00,453 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q_128_dev_think.csv
2025-07-20 11:00:00,473 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-20 11:48:57,062 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 48m 56s
2025-07-20 11:48:57,063 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q+r_128_dev_think.csv
2025-07-20 11:49:01,624 - __main__ - ERROR - Run deepseek-think failed: 'passage'
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\run_multiple.py", line 12, in safe_run
    run()
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 84, in run
    run_model_mode(
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 208, in run_model_mode
    ctx, ret_metrics = retrieve_context(sample, wiki_data)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 375, in retrieve_context
    sent for section in sample["passage"] for sent in section
                        ~~~~~~^^^^^^^^^^^
KeyError: 'passage'
2025-07-20 11:49:07,254 - __main__ - INFO - Starting run: deepseek-no-think
2025-07-20 11:49:07,255 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-20 11:49:07,255 - energy_eval - INFO - Using device: cuda
2025-07-20 11:49:07,255 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-20 11:49:07,278 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-20 11:49:07,278 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-20 11:52:42,685 - energy_eval - INFO - Loaded wiki in 0h3m35s
2025-07-20 11:52:42,685 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-20 11:52:42,685 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-20 11:52:42,685 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q_128_dev.csv
2025-07-20 11:52:42,698 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-20 12:26:08,115 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 33m 25s
2025-07-20 12:26:08,115 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q+r_128_dev.csv
2025-07-20 12:26:12,633 - __main__ - ERROR - Run deepseek-no-think failed: 'passage'
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\run_multiple.py", line 12, in safe_run
    run()
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 84, in run
    run_model_mode(
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 208, in run_model_mode
    ctx, ret_metrics = retrieve_context(sample, wiki_data)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 375, in retrieve_context
    sent for section in sample["passage"] for sent in section
                        ~~~~~~^^^^^^^^^^^
KeyError: 'passage'
2025-07-20 13:12:35,909 - __main__ - INFO - Starting run: deepseek-think
2025-07-20 13:12:35,909 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-20 13:12:35,909 - energy_eval - INFO - Using device: cuda
2025-07-20 13:12:35,909 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-20 13:12:35,934 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-20 13:12:35,934 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-20 13:16:43,744 - energy_eval - INFO - Loaded wiki in 0h4m7s
2025-07-20 13:16:43,744 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-20 13:16:43,744 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-20 13:16:43,750 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q_128_dev_think.csv
2025-07-20 13:16:43,778 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-20 13:17:09,495 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 0m 25s
2025-07-20 13:17:09,495 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q+r_128_dev_think.csv
2025-07-20 13:17:14,651 - __main__ - ERROR - Run deepseek-think failed: name 'passage' is not defined
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\run_multiple.py", line 12, in safe_run
    run()
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 84, in run
    run_model_mode(
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 208, in run_model_mode
    ctx, ret_metrics = retrieve_context(sample, wiki_data)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 374, in retrieve_context
    if passage in sample.keys():
       ^^^^^^^
NameError: name 'passage' is not defined
2025-07-20 13:17:20,425 - __main__ - INFO - Starting run: deepseek-no-think
2025-07-20 13:17:20,426 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-20 13:17:20,426 - energy_eval - INFO - Using device: cuda
2025-07-20 13:17:20,426 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-20 13:17:20,442 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-20 13:17:20,442 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-20 13:21:48,102 - energy_eval - INFO - Loaded wiki in 0h4m27s
2025-07-20 13:21:48,102 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-20 13:21:48,102 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-20 13:21:48,102 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q_128_dev.csv
2025-07-20 13:21:48,121 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-20 13:21:54,476 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 0m 6s
2025-07-20 13:21:54,476 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q+r_128_dev.csv
2025-07-20 13:21:59,504 - __main__ - ERROR - Run deepseek-no-think failed: name 'passage' is not defined
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\run_multiple.py", line 12, in safe_run
    run()
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 84, in run
    run_model_mode(
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 208, in run_model_mode
    ctx, ret_metrics = retrieve_context(sample, wiki_data)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 374, in retrieve_context
    if passage in sample.keys():
       ^^^^^^^
NameError: name 'passage' is not defined
2025-07-20 13:36:11,487 - __main__ - INFO - Starting run: deepseek-think
2025-07-20 13:36:11,487 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-20 13:36:11,487 - energy_eval - INFO - Using device: cuda
2025-07-20 13:36:11,487 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-20 13:36:11,504 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-20 13:36:11,504 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-20 13:40:37,597 - energy_eval - INFO - Loaded wiki in 0h4m26s
2025-07-20 13:40:37,598 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-20 13:40:37,598 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-20 13:40:37,599 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q_128_dev_think.csv
2025-07-20 13:40:37,621 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-20 13:41:01,277 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 0m 23s
2025-07-20 13:41:01,277 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q+r_128_dev_think.csv
2025-07-20 13:51:56,034 - energy_eval - INFO - Built 128 prompts in 0h 10m 54s (+ retrieval: 0h 0m 10s, 0.0035 kWh, 0.0010 kg)
2025-07-20 20:12:54,699 - __main__ - INFO - Starting run: deepseek-think
2025-07-20 20:12:54,699 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-20 20:12:54,699 - energy_eval - INFO - Using device: cuda
2025-07-20 20:12:54,699 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-20 20:12:54,715 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-20 20:12:54,715 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-20 20:16:53,807 - energy_eval - INFO - Loaded wiki in 0h3m59s
2025-07-20 20:16:53,807 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-20 20:16:53,807 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-20 20:16:53,807 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q_128_dev_think.csv
2025-07-20 20:16:53,823 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-20 20:17:14,561 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 0m 20s
2025-07-20 20:17:14,561 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q+r_128_dev_think.csv
2025-07-20 20:17:52,010 - energy_eval - INFO - Built 8 prompts in 0h 0m 37s (+ retrieval: 0h 0m 0s, 0.0002 kWh, 0.0001 kg)
2025-07-20 20:21:18,208 - energy_eval - INFO - Completed q+r for deepseek-r1:8b in 0h 4m 3s
2025-07-20 20:21:26,311 - energy_eval - INFO - Completed deepseek-r1:8b in 0h 4m 32s
2025-07-20 20:21:26,311 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-07-20 20:21:26,311 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-20 20:21:26,311 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-14b_q_128_dev_think.csv
2025-07-20 20:21:26,327 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-20 20:44:58,051 - energy_eval - INFO - Completed q for deepseek-r1:14b in 0h 23m 31s
2025-07-20 20:44:58,051 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-14b_q+r_128_dev_think.csv
2025-07-20 20:54:58,483 - energy_eval - INFO - Built 128 prompts in 0h 10m 0s (+ retrieval: 0h 0m 12s, 0.0032 kWh, 0.0009 kg)
2025-07-20 21:15:41,438 - energy_eval - INFO - Completed q+r for deepseek-r1:14b in 0h 30m 43s
2025-07-20 21:15:48,197 - energy_eval - INFO - Completed deepseek-r1:14b in 0h 54m 21s
2025-07-20 21:15:48,197 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-20 21:15:48,197 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-20 21:15:48,197 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-32b_q_128_dev_think.csv
2025-07-20 21:15:48,213 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-20 21:52:02,682 - energy_eval - INFO - Completed q for deepseek-r1:32b in 0h 36m 14s
2025-07-20 21:52:09,306 - energy_eval - INFO - Completed deepseek-r1:32b in 0h 36m 21s
2025-07-20 21:52:09,306 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 39m 14s
============================================================
2025-07-20 21:52:15,015 - __main__ - INFO - Starting run: deepseek-no-think
2025-07-20 21:52:15,015 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-20 21:52:15,015 - energy_eval - INFO - Using device: cuda
2025-07-20 21:52:15,015 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-20 21:52:15,050 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-20 21:52:15,050 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-20 21:57:09,509 - energy_eval - INFO - Loaded wiki in 0h4m54s
2025-07-20 21:57:09,509 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-20 21:57:09,509 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-20 21:57:09,509 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q_128_dev.csv
2025-07-20 21:57:09,535 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-20 21:57:18,397 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 0m 8s
2025-07-20 21:57:18,397 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q+r_128_dev.csv
2025-07-20 22:07:16,387 - energy_eval - INFO - Built 128 prompts in 0h 9m 57s (+ retrieval: 0h 0m 11s, 0.0031 kWh, 0.0009 kg)
2025-07-20 22:19:27,169 - energy_eval - INFO - Completed q+r for deepseek-r1:8b in 0h 22m 8s
2025-07-20 22:19:35,316 - energy_eval - INFO - Completed deepseek-r1:8b in 0h 22m 25s
2025-07-20 22:19:35,316 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-07-20 22:19:35,316 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-20 22:19:35,316 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-14b_q_128_dev.csv
2025-07-20 22:19:35,340 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-20 22:31:09,736 - energy_eval - INFO - Completed q for deepseek-r1:14b in 0h 11m 34s
2025-07-20 22:31:09,737 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-14b_q+r_128_dev.csv
2025-07-20 22:41:07,773 - energy_eval - INFO - Built 128 prompts in 0h 9m 58s (+ retrieval: 0h 0m 10s, 0.0029 kWh, 0.0008 kg)
2025-07-20 22:53:28,012 - energy_eval - INFO - Completed q+r for deepseek-r1:14b in 0h 22m 18s
2025-07-20 22:53:45,715 - energy_eval - INFO - Completed deepseek-r1:14b in 0h 34m 10s
2025-07-20 22:53:45,715 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-20 22:53:45,716 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-20 22:53:45,716 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-32b_q_128_dev.csv
2025-07-20 22:53:45,738 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-20 23:06:19,896 - energy_eval - INFO - Completed q for deepseek-r1:32b in 0h 12m 34s
2025-07-20 23:06:27,724 - energy_eval - INFO - Completed deepseek-r1:32b in 0h 12m 42s
2025-07-20 23:06:27,724 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 14m 12s
============================================================
2025-07-21 00:50:53,961 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:1b': 'ollama', 'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:1b': {'q': False, 'q+r': True}, 'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-21 00:50:53,961 - energy_eval - INFO - Using device: cuda
2025-07-21 00:50:53,961 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-21 00:50:53,985 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-21 00:50:53,985 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-21 00:51:05,427 - energy_eval - WARNING - Experiment interrupted by user
2025-07-21 00:51:17,850 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:1b': 'ollama', 'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='full.jsonl', config='fullwiki', split='train', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:1b': {'q': False, 'q+r': True}, 'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-21 00:51:17,850 - energy_eval - INFO - Using device: cuda
2025-07-21 00:51:19,629 - energy_eval - INFO - Loaded dataset with 90447 samples
2025-07-21 00:51:19,629 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-21 00:52:04,591 - energy_eval - WARNING - Experiment interrupted by user
2025-07-21 00:52:17,222 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:1b': 'ollama', 'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='full.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:1b': {'q': False, 'q+r': True}, 'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-21 00:52:17,222 - energy_eval - INFO - Using device: cuda
2025-07-21 00:52:18,317 - energy_eval - INFO - Loaded dataset with 7405 samples
2025-07-21 00:52:18,317 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-21 00:56:33,961 - energy_eval - INFO - Loaded wiki in 0h4m15s
2025-07-21 00:56:33,961 - energy_eval - INFO - 
============================================================
Running model: gemma3:1b
============================================================
2025-07-21 00:56:33,961 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-21 00:56:33,976 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_gemma3-1b_q_think.csv
2025-07-21 00:56:35,015 - energy_eval - INFO - Built 7405 prompts in 0h 0m 1s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-21 00:56:39,925 - energy_eval - ERROR - Critical error: registry.ollama.ai/library/gemma3:1b does not support thinking (status code: 400)
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 499, in <module>
    run()
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 84, in run
    run_model_mode(
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 225, in run_model_mode
    _ = inference("Ready?", model_name, mode_tag, provider)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 102, in inference
    text = inference_ollama(prompt, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 26, in inference_ollama
    resp = generate(
           ^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\venv\Lib\site-packages\ollama\_client.py", line 247, in generate
    return self._request(
           ^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\venv\Lib\site-packages\ollama\_client.py", line 180, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\venv\Lib\site-packages\ollama\_client.py", line 124, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: registry.ollama.ai/library/gemma3:1b does not support thinking (status code: 400)
2025-07-21 06:48:33,198 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:1b': 'ollama', 'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='full.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:1b': {'q': False, 'q+r': True}, 'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-21 06:48:33,198 - energy_eval - INFO - Using device: cuda
2025-07-21 06:48:34,641 - energy_eval - INFO - Loaded dataset with 7405 samples
2025-07-21 06:48:34,641 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-21 06:52:13,133 - energy_eval - INFO - Loaded wiki in 0h3m38s
2025-07-21 06:52:13,133 - energy_eval - INFO - 
============================================================
Running model: gemma3:1b
============================================================
2025-07-21 06:52:13,133 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-21 06:52:13,133 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_gemma3-1b_q.csv
2025-07-21 06:52:14,020 - energy_eval - INFO - Built 7405 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-21 16:50:26,079 - energy_eval - INFO - Completed q for gemma3:1b in 9h 58m 12s
2025-07-21 16:50:26,079 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_gemma3-1b_q+r.csv
2025-07-22 02:22:40,080 - energy_eval - INFO - Built 7405 prompts in 9h 32m 14s (+ retrieval: 0h 7m 50s, 0.1330 kWh, 0.0383 kg)
2025-07-22 13:17:13,580 - energy_eval - INFO - Completed q+r for gemma3:1b in 20h 26m 47s
2025-07-22 13:17:35,237 - energy_eval - INFO - Completed gemma3:1b in 30h 25m 22s
2025-07-22 13:17:35,237 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-22 13:17:35,238 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-22 13:17:35,238 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_gemma3-4b_q.csv
2025-07-22 13:17:36,072 - energy_eval - INFO - Built 7405 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-22 23:16:43,449 - energy_eval - INFO - Completed q for gemma3:4b in 9h 59m 8s
2025-07-22 23:16:43,449 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_gemma3-4b_q+r.csv
2025-07-23 08:42:18,555 - energy_eval - INFO - Built 7405 prompts in 9h 25m 35s (+ retrieval: 0h 6m 24s, 0.1020 kWh, 0.0292 kg)
2025-07-23 19:15:19,184 - energy_eval - INFO - Completed q+r for gemma3:4b in 19h 58m 35s
2025-07-23 19:15:44,119 - energy_eval - INFO - Completed gemma3:4b in 29h 58m 8s
2025-07-23 19:15:44,119 - energy_eval - INFO - 
============================================================
Running model: gemma3:12b
============================================================
2025-07-23 19:15:44,119 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-23 19:15:44,119 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_gemma3-12b_q.csv
2025-07-23 19:15:45,195 - energy_eval - INFO - Built 7405 prompts in 0h 0m 1s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-24 05:36:31,681 - energy_eval - INFO - Completed q for gemma3:12b in 10h 20m 47s
2025-07-24 05:36:31,688 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_gemma3-12b_q+r.csv
2025-07-24 15:03:13,893 - energy_eval - INFO - Built 7405 prompts in 9h 26m 42s (+ retrieval: 0h 6m 57s, 0.1114 kWh, 0.0320 kg)
2025-07-25 02:11:53,973 - energy_eval - INFO - Completed q+r for gemma3:12b in 20h 35m 22s
2025-07-25 02:12:11,488 - energy_eval - INFO - Completed gemma3:12b in 30h 56m 27s
2025-07-25 02:12:11,488 - energy_eval - INFO - 
============================================================
Running model: gemma3:27b
============================================================
2025-07-25 02:12:11,488 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-25 02:12:11,488 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_gemma3-27b_q.csv
2025-07-25 02:12:12,234 - energy_eval - INFO - Built 7405 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-25 12:23:45,644 - energy_eval - INFO - Completed q for gemma3:27b in 10h 11m 34s
2025-07-25 12:24:08,208 - energy_eval - INFO - Completed gemma3:27b in 10h 11m 56s
2025-07-25 12:24:08,208 - energy_eval - INFO - 
============================================================
Experiment completed in 101h 35m 35s
============================================================
2025-07-25 15:41:50,745 - __main__ - INFO - Starting run: gemma3_dev_testing
2025-07-25 15:41:50,745 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-25 15:41:50,745 - energy_eval - INFO - Using device: cuda
2025-07-25 15:41:50,745 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_dev_128.jsonl
2025-07-25 15:41:50,755 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-25 15:41:50,755 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-25 15:42:12,139 - __main__ - INFO - Starting run: gemma3_dev_testing
2025-07-25 15:42:12,140 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-25 15:42:12,140 - energy_eval - INFO - Using device: cuda
2025-07-25 15:42:12,140 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_dev_128.jsonl
2025-07-25 15:42:12,143 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-25 15:42:12,143 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-25 15:46:20,680 - energy_eval - INFO - Loaded wiki in 0h4m8s
2025-07-25 15:46:20,680 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-25 15:46:20,680 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-25 15:46:20,682 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\boolq_gemma3-4b_q_128_dev.csv
2025-07-25 15:46:20,686 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-25 16:07:22,027 - energy_eval - INFO - Completed q for gemma3:4b in 0h 21m 1s
2025-07-25 16:07:22,027 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\boolq_gemma3-4b_q+r_128_dev.csv
2025-07-25 16:12:46,141 - __main__ - INFO - Starting run: gemma3_dev_testing
2025-07-25 16:12:46,142 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-25 16:12:46,142 - energy_eval - INFO - Using device: cuda
2025-07-25 16:12:46,142 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_dev_128.jsonl
2025-07-25 16:12:46,146 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-25 16:12:46,146 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-25 16:16:57,492 - energy_eval - INFO - Loaded wiki in 0h4m11s
2025-07-25 16:16:57,492 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-25 16:16:57,492 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-25 16:16:57,493 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\boolq_gemma3-4b_q_128_dev0.csv
2025-07-25 16:16:57,496 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-25 16:18:59,512 - __main__ - INFO - Starting run: gemma3_dev_testing
2025-07-25 16:18:59,512 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-25 16:18:59,512 - energy_eval - INFO - Using device: cuda
2025-07-25 16:18:59,512 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_dev_128.jsonl
2025-07-25 16:18:59,516 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-25 16:18:59,516 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-25 16:19:40,789 - __main__ - INFO - Starting run: gemma3_dev_testing
2025-07-25 16:19:40,790 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-25 16:19:40,790 - energy_eval - INFO - Using device: cuda
2025-07-25 16:19:40,790 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_dev_128.jsonl
2025-07-25 16:19:40,794 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-25 16:19:40,794 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-25 16:24:14,935 - energy_eval - INFO - Loaded wiki in 0h4m34s
2025-07-25 16:24:14,936 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-25 16:24:14,936 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-07-25 16:24:14,941 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\boolq_gemma3-4b_q+r_128_dev_0.csv
2025-07-25 16:34:41,002 - energy_eval - INFO - Built 128 prompts in 0h 10m 26s (+ retrieval: 0h 0m 5s, 0.0016 kWh, 0.0005 kg)
2025-07-25 16:56:19,347 - energy_eval - INFO - Completed q+r for gemma3:4b in 0h 32m 4s
2025-07-25 16:56:27,455 - energy_eval - INFO - Completed gemma3:4b in 0h 32m 12s
2025-07-25 16:56:27,455 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 36m 46s
============================================================
2025-07-25 16:56:32,595 - __main__ - INFO - Starting run: gemma3_dev_testing
2025-07-25 16:56:32,595 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-25 16:56:32,596 - energy_eval - INFO - Using device: cuda
2025-07-25 16:56:32,596 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_dev_128.jsonl
2025-07-25 16:56:32,601 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-25 16:56:32,601 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-25 17:00:52,953 - energy_eval - INFO - Loaded wiki in 0h4m20s
2025-07-25 17:00:52,954 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-25 17:00:52,954 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-07-25 17:00:52,954 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\boolq_gemma3-4b_q+r_128_dev_1.csv
2025-07-25 17:11:13,959 - energy_eval - INFO - Built 128 prompts in 0h 10m 21s (+ retrieval: 0h 0m 4s, 0.0016 kWh, 0.0005 kg)
2025-07-25 17:32:46,211 - energy_eval - INFO - Completed q+r for gemma3:4b in 0h 31m 53s
2025-07-25 17:33:02,068 - energy_eval - INFO - Completed gemma3:4b in 0h 32m 9s
2025-07-25 17:33:02,069 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 36m 29s
============================================================
2025-07-25 17:33:09,345 - __main__ - INFO - Starting run: gemma3_dev_testing
2025-07-25 17:33:09,346 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-25 17:33:09,346 - energy_eval - INFO - Using device: cuda
2025-07-25 17:33:09,346 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_dev_128.jsonl
2025-07-25 17:33:09,355 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-25 17:33:09,355 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-25 17:37:23,317 - energy_eval - INFO - Loaded wiki in 0h4m13s
2025-07-25 17:37:23,317 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-25 17:37:23,317 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-07-25 17:37:23,319 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\boolq_gemma3-4b_q+r_128_dev_2.csv
2025-07-25 17:47:45,065 - energy_eval - INFO - Built 128 prompts in 0h 10m 21s (+ retrieval: 0h 0m 4s, 0.0015 kWh, 0.0004 kg)
2025-07-25 18:09:19,879 - energy_eval - INFO - Completed q+r for gemma3:4b in 0h 31m 56s
2025-07-25 18:09:32,075 - energy_eval - INFO - Completed gemma3:4b in 0h 32m 8s
2025-07-25 18:09:32,076 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 36m 22s
============================================================
2025-07-25 18:09:38,006 - __main__ - INFO - Starting run: gemma3_dev_testing
2025-07-25 18:09:38,006 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-25 18:09:38,006 - energy_eval - INFO - Using device: cuda
2025-07-25 18:09:38,006 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_dev_128.jsonl
2025-07-25 18:09:38,015 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-25 18:09:38,015 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-25 18:13:49,659 - energy_eval - INFO - Loaded wiki in 0h4m11s
2025-07-25 18:13:49,659 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-25 18:13:49,659 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-07-25 18:13:49,659 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\boolq_gemma3-4b_q+r_128_dev_3.csv
2025-07-25 18:24:15,380 - energy_eval - INFO - Built 128 prompts in 0h 10m 25s (+ retrieval: 0h 0m 5s, 0.0015 kWh, 0.0004 kg)
2025-07-25 18:45:44,721 - energy_eval - INFO - Completed q+r for gemma3:4b in 0h 31m 55s
2025-07-25 18:45:58,045 - energy_eval - INFO - Completed gemma3:4b in 0h 32m 8s
2025-07-25 18:45:58,045 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 36m 20s
============================================================
2025-07-25 18:46:04,718 - __main__ - INFO - Starting run: gemma3_dev_testing
2025-07-25 18:46:04,718 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-25 18:46:04,719 - energy_eval - INFO - Using device: cuda
2025-07-25 18:46:04,719 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_dev_128.jsonl
2025-07-25 18:46:04,725 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-25 18:46:04,725 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-25 18:50:20,355 - energy_eval - INFO - Loaded wiki in 0h4m15s
2025-07-25 18:50:20,355 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-25 18:50:20,355 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-07-25 18:50:20,355 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\boolq_gemma3-4b_q+r_128_dev_4.csv
2025-07-25 19:00:42,754 - energy_eval - INFO - Built 128 prompts in 0h 10m 22s (+ retrieval: 0h 0m 4s, 0.0015 kWh, 0.0005 kg)
2025-07-25 19:22:07,464 - energy_eval - INFO - Completed q+r for gemma3:4b in 0h 31m 47s
2025-07-25 19:22:15,035 - energy_eval - INFO - Completed gemma3:4b in 0h 31m 54s
2025-07-25 19:22:15,035 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 36m 10s
============================================================
2025-07-28 22:29:37,044 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='full.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-28 22:29:37,044 - energy_eval - INFO - Using device: cuda
2025-07-28 22:29:38,182 - energy_eval - INFO - Loaded dataset with 7405 samples
2025-07-28 22:29:38,182 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-28 22:29:44,155 - energy_eval - WARNING - Experiment interrupted by user
2025-07-28 22:30:00,559 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='full.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-28 22:30:00,559 - energy_eval - INFO - Using device: cuda
2025-07-28 22:30:01,453 - energy_eval - INFO - Loaded dataset with 7405 samples
2025-07-28 22:30:01,453 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-28 22:30:11,396 - energy_eval - WARNING - Experiment interrupted by user
2025-07-28 22:30:16,997 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='full.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-28 22:30:16,997 - energy_eval - INFO - Using device: cuda
2025-07-28 22:30:17,786 - energy_eval - INFO - Loaded dataset with 7405 samples
2025-07-28 22:30:17,786 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-28 22:30:27,788 - energy_eval - WARNING - Experiment interrupted by user
2025-07-28 22:30:39,173 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='full.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-28 22:30:39,173 - energy_eval - INFO - Using device: cuda
2025-07-28 22:30:42,387 - energy_eval - INFO - Loaded dataset with 7405 samples
2025-07-28 22:30:42,387 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-28 22:34:32,035 - energy_eval - INFO - Loaded wiki in 0h3m49s
2025-07-28 22:34:32,035 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-28 22:34:32,035 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-28 22:34:32,040 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q_think.csv
2025-07-28 22:34:32,852 - energy_eval - INFO - Built 7405 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-29 10:16:57,566 - energy_eval - INFO - Completed q for deepseek-r1:8b in 11h 42m 25s
2025-07-29 10:16:57,566 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\hotpot_deepseek-r1-8b_q+r_think.csv
2025-07-29 19:44:34,930 - energy_eval - INFO - Built 7405 prompts in 9h 27m 37s (+ retrieval: 0h 7m 0s, 0.1130 kWh, 0.0331 kg)
2025-07-29 22:00:55,705 - energy_eval - WARNING - Experiment interrupted by user
2025-07-29 22:11:19,240 - __main__ - INFO - Starting run: gemma3_4_dev_testing - 0
2025-07-29 22:11:19,240 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama'}, dataset_name=('hotpotqa/hotpot_qa',), dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-29 22:11:19,240 - energy_eval - INFO - Using device: cuda
2025-07-29 22:11:19,240 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-29 22:11:19,262 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-29 22:11:19,263 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-29 22:11:58,117 - __main__ - INFO - Starting run: gemma3_4_dev_testing - 0
2025-07-29 22:11:58,117 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-29 22:11:58,117 - energy_eval - INFO - Using device: cuda
2025-07-29 22:11:58,117 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-29 22:11:58,130 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-29 22:11:58,130 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-29 22:15:35,039 - energy_eval - INFO - Loaded wiki in 0h3m36s
2025-07-29 22:15:35,039 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-29 22:15:35,039 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-29 22:15:35,044 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-4b_q_128_dev_0.csv
2025-07-29 22:15:35,059 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-29 22:26:06,436 - energy_eval - INFO - Completed q for gemma3:4b in 0h 10m 31s
2025-07-29 22:26:06,436 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-4b_q+r_128_dev_0.csv
2025-07-29 22:35:53,927 - energy_eval - INFO - Built 128 prompts in 0h 9m 47s (+ retrieval: 0h 0m 7s, 0.0019 kWh, 0.0006 kg)
2025-07-29 22:46:37,797 - energy_eval - INFO - Completed q+r for gemma3:4b in 0h 20m 31s
2025-07-29 22:46:43,844 - energy_eval - INFO - Completed gemma3:4b in 0h 31m 8s
2025-07-29 22:46:43,845 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 34m 45s
============================================================
2025-07-29 22:46:47,888 - __main__ - INFO - Starting run: gemma3_4_dev_testing - 1
2025-07-29 22:46:47,888 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-29 22:46:47,888 - energy_eval - INFO - Using device: cuda
2025-07-29 22:46:47,888 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-29 22:46:47,900 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-29 22:46:47,900 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-29 22:50:15,245 - energy_eval - INFO - Loaded wiki in 0h3m27s
2025-07-29 22:50:15,245 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-29 22:50:15,245 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-29 22:50:15,245 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-4b_q_128_dev_1.csv
2025-07-29 22:50:15,258 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-29 23:00:38,198 - energy_eval - INFO - Completed q for gemma3:4b in 0h 10m 22s
2025-07-29 23:00:38,198 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-4b_q+r_128_dev_1.csv
2025-07-29 23:10:25,441 - energy_eval - INFO - Built 128 prompts in 0h 9m 47s (+ retrieval: 0h 0m 7s, 0.0019 kWh, 0.0006 kg)
2025-07-29 23:21:09,149 - energy_eval - INFO - Completed q+r for gemma3:4b in 0h 20m 30s
2025-07-29 23:21:14,395 - energy_eval - INFO - Completed gemma3:4b in 0h 30m 59s
2025-07-29 23:21:14,395 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 34m 26s
============================================================
2025-07-29 23:21:18,497 - __main__ - INFO - Starting run: gemma3_4_dev_testing - 2
2025-07-29 23:21:18,497 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-29 23:21:18,497 - energy_eval - INFO - Using device: cuda
2025-07-29 23:21:18,497 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-29 23:21:18,510 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-29 23:21:18,510 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-29 23:24:44,853 - energy_eval - INFO - Loaded wiki in 0h3m26s
2025-07-29 23:24:44,853 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-29 23:24:44,853 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-29 23:24:44,853 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-4b_q_128_dev_2.csv
2025-07-29 23:24:44,867 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-29 23:35:08,760 - energy_eval - INFO - Completed q for gemma3:4b in 0h 10m 23s
2025-07-29 23:35:08,760 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-4b_q+r_128_dev_2.csv
2025-07-29 23:44:56,002 - energy_eval - INFO - Built 128 prompts in 0h 9m 47s (+ retrieval: 0h 0m 6s, 0.0019 kWh, 0.0005 kg)
2025-07-29 23:55:38,931 - energy_eval - INFO - Completed q+r for gemma3:4b in 0h 20m 30s
2025-07-29 23:55:44,196 - energy_eval - INFO - Completed gemma3:4b in 0h 30m 59s
2025-07-29 23:55:44,196 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 34m 25s
============================================================
2025-07-29 23:55:48,224 - __main__ - INFO - Starting run: gemma3_4_dev_testing - 3
2025-07-29 23:55:48,225 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-29 23:55:48,225 - energy_eval - INFO - Using device: cuda
2025-07-29 23:55:48,225 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-29 23:55:48,237 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-29 23:55:48,237 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-29 23:59:15,438 - energy_eval - INFO - Loaded wiki in 0h3m27s
2025-07-29 23:59:15,439 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-29 23:59:15,439 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-29 23:59:15,439 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-4b_q_128_dev_3.csv
2025-07-29 23:59:15,452 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 00:09:39,322 - energy_eval - INFO - Completed q for gemma3:4b in 0h 10m 23s
2025-07-30 00:09:39,322 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-4b_q+r_128_dev_3.csv
2025-07-30 00:19:26,886 - energy_eval - INFO - Built 128 prompts in 0h 9m 47s (+ retrieval: 0h 0m 7s, 0.0019 kWh, 0.0006 kg)
2025-07-30 00:30:10,282 - energy_eval - INFO - Completed q+r for gemma3:4b in 0h 20m 30s
2025-07-30 00:30:15,521 - energy_eval - INFO - Completed gemma3:4b in 0h 31m 0s
2025-07-30 00:30:15,521 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 34m 27s
============================================================
2025-07-30 00:30:19,633 - __main__ - INFO - Starting run: gemma3_4_dev_testing - 4
2025-07-30 00:30:19,634 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 00:30:19,634 - energy_eval - INFO - Using device: cuda
2025-07-30 00:30:19,634 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 00:30:19,646 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 00:30:19,646 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 00:33:46,460 - energy_eval - INFO - Loaded wiki in 0h3m26s
2025-07-30 00:33:46,460 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-30 00:33:46,460 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 00:33:46,461 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-4b_q_128_dev_4.csv
2025-07-30 00:33:46,473 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 00:44:11,297 - energy_eval - INFO - Completed q for gemma3:4b in 0h 10m 24s
2025-07-30 00:44:11,297 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-4b_q+r_128_dev_4.csv
2025-07-30 00:53:58,190 - energy_eval - INFO - Built 128 prompts in 0h 9m 46s (+ retrieval: 0h 0m 6s, 0.0019 kWh, 0.0005 kg)
2025-07-30 01:04:41,656 - energy_eval - INFO - Completed q+r for gemma3:4b in 0h 20m 30s
2025-07-30 01:04:46,901 - energy_eval - INFO - Completed gemma3:4b in 0h 31m 0s
2025-07-30 01:04:46,901 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 34m 27s
============================================================
2025-07-30 01:04:50,987 - __main__ - INFO - Starting run: gemma3_12_dev_testing - 0
2025-07-30 01:04:50,988 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:12b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:12b': {'q': False, 'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 01:04:50,988 - energy_eval - INFO - Using device: cuda
2025-07-30 01:04:50,988 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 01:04:51,007 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 01:04:51,007 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 01:08:18,343 - energy_eval - INFO - Loaded wiki in 0h3m27s
2025-07-30 01:08:18,343 - energy_eval - INFO - 
============================================================
Running model: gemma3:12b
============================================================
2025-07-30 01:08:18,343 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 01:08:18,343 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-12b_q_128_dev_0.csv
2025-07-30 01:08:18,356 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 01:18:50,482 - energy_eval - INFO - Completed q for gemma3:12b in 0h 10m 32s
2025-07-30 01:18:50,482 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-12b_q+r_128_dev_0.csv
2025-07-30 01:28:39,128 - energy_eval - INFO - Built 128 prompts in 0h 9m 48s (+ retrieval: 0h 0m 6s, 0.0019 kWh, 0.0005 kg)
2025-07-30 01:39:44,039 - energy_eval - INFO - Completed q+r for gemma3:12b in 0h 20m 53s
2025-07-30 01:39:49,321 - energy_eval - INFO - Completed gemma3:12b in 0h 31m 30s
2025-07-30 01:39:49,321 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 34m 58s
============================================================
2025-07-30 01:39:53,504 - __main__ - INFO - Starting run: gemma3_12_dev_testing - 1
2025-07-30 01:39:53,504 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:12b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:12b': {'q': False, 'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 01:39:53,504 - energy_eval - INFO - Using device: cuda
2025-07-30 01:39:53,505 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 01:39:53,523 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 01:39:53,523 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 01:43:22,058 - energy_eval - INFO - Loaded wiki in 0h3m28s
2025-07-30 01:43:22,058 - energy_eval - INFO - 
============================================================
Running model: gemma3:12b
============================================================
2025-07-30 01:43:22,058 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 01:43:22,059 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-12b_q_128_dev_1.csv
2025-07-30 01:43:22,077 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 01:53:49,638 - energy_eval - INFO - Completed q for gemma3:12b in 0h 10m 27s
2025-07-30 01:53:49,638 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-12b_q+r_128_dev_1.csv
2025-07-30 02:03:37,373 - energy_eval - INFO - Built 128 prompts in 0h 9m 47s (+ retrieval: 0h 0m 6s, 0.0019 kWh, 0.0005 kg)
2025-07-30 02:14:42,378 - energy_eval - INFO - Completed q+r for gemma3:12b in 0h 20m 52s
2025-07-30 02:14:47,657 - energy_eval - INFO - Completed gemma3:12b in 0h 31m 25s
2025-07-30 02:14:47,657 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 34m 54s
============================================================
2025-07-30 02:14:51,903 - __main__ - INFO - Starting run: gemma3_12_dev_testing - 2
2025-07-30 02:14:51,904 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:12b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:12b': {'q': False, 'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 02:14:51,904 - energy_eval - INFO - Using device: cuda
2025-07-30 02:14:51,904 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 02:14:51,916 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 02:14:51,916 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 02:18:19,857 - energy_eval - INFO - Loaded wiki in 0h3m27s
2025-07-30 02:18:19,857 - energy_eval - INFO - 
============================================================
Running model: gemma3:12b
============================================================
2025-07-30 02:18:19,857 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 02:18:19,857 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-12b_q_128_dev_2.csv
2025-07-30 02:18:19,878 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 02:28:49,988 - energy_eval - INFO - Completed q for gemma3:12b in 0h 10m 30s
2025-07-30 02:28:49,988 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-12b_q+r_128_dev_2.csv
2025-07-30 02:38:37,815 - energy_eval - INFO - Built 128 prompts in 0h 9m 47s (+ retrieval: 0h 0m 6s, 0.0019 kWh, 0.0005 kg)
2025-07-30 02:49:43,325 - energy_eval - INFO - Completed q+r for gemma3:12b in 0h 20m 53s
2025-07-30 02:49:48,566 - energy_eval - INFO - Completed gemma3:12b in 0h 31m 28s
2025-07-30 02:49:48,566 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 34m 56s
============================================================
2025-07-30 02:49:52,734 - __main__ - INFO - Starting run: gemma3_12_dev_testing - 3
2025-07-30 02:49:52,735 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:12b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:12b': {'q': False, 'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 02:49:52,735 - energy_eval - INFO - Using device: cuda
2025-07-30 02:49:52,735 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 02:49:52,751 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 02:49:52,751 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 02:53:22,381 - energy_eval - INFO - Loaded wiki in 0h3m29s
2025-07-30 02:53:22,382 - energy_eval - INFO - 
============================================================
Running model: gemma3:12b
============================================================
2025-07-30 02:53:22,382 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 02:53:22,382 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-12b_q_128_dev_3.csv
2025-07-30 02:53:22,395 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 03:03:51,737 - energy_eval - INFO - Completed q for gemma3:12b in 0h 10m 29s
2025-07-30 03:03:51,737 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-12b_q+r_128_dev_3.csv
2025-07-30 03:13:39,883 - energy_eval - INFO - Built 128 prompts in 0h 9m 48s (+ retrieval: 0h 0m 6s, 0.0019 kWh, 0.0005 kg)
2025-07-30 03:24:48,106 - energy_eval - INFO - Completed q+r for gemma3:12b in 0h 20m 56s
2025-07-30 03:24:53,376 - energy_eval - INFO - Completed gemma3:12b in 0h 31m 30s
2025-07-30 03:24:53,376 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 35m 0s
============================================================
2025-07-30 03:24:57,574 - __main__ - INFO - Starting run: gemma3_12_dev_testing - 4
2025-07-30 03:24:57,575 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:12b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:12b': {'q': False, 'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 03:24:57,575 - energy_eval - INFO - Using device: cuda
2025-07-30 03:24:57,575 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 03:24:57,592 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 03:24:57,592 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 03:28:25,154 - energy_eval - INFO - Loaded wiki in 0h3m27s
2025-07-30 03:28:25,155 - energy_eval - INFO - 
============================================================
Running model: gemma3:12b
============================================================
2025-07-30 03:28:25,155 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 03:28:25,155 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-12b_q_128_dev_4.csv
2025-07-30 03:28:25,167 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 03:38:57,471 - energy_eval - INFO - Completed q for gemma3:12b in 0h 10m 32s
2025-07-30 03:38:57,472 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-12b_q+r_128_dev_4.csv
2025-07-30 03:48:46,133 - energy_eval - INFO - Built 128 prompts in 0h 9m 48s (+ retrieval: 0h 0m 6s, 0.0019 kWh, 0.0005 kg)
2025-07-30 03:59:52,242 - energy_eval - INFO - Completed q+r for gemma3:12b in 0h 20m 54s
2025-07-30 03:59:57,542 - energy_eval - INFO - Completed gemma3:12b in 0h 31m 32s
2025-07-30 03:59:57,542 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 34m 59s
============================================================
2025-07-30 04:00:01,684 - __main__ - INFO - Starting run: gemma3_27_dev_testing - 0
2025-07-30 04:00:01,684 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:27b': {'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 04:00:01,684 - energy_eval - INFO - Using device: cuda
2025-07-30 04:00:01,684 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 04:00:01,697 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 04:00:01,697 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 04:03:29,891 - energy_eval - INFO - Loaded wiki in 0h3m28s
2025-07-30 04:03:29,891 - energy_eval - INFO - 
============================================================
Running model: gemma3:27b
============================================================
2025-07-30 04:03:29,891 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 04:03:29,891 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-27b_q+r_128_dev_0.csv
2025-07-30 04:13:21,681 - energy_eval - INFO - Built 128 prompts in 0h 9m 51s (+ retrieval: 0h 0m 6s, 0.0019 kWh, 0.0005 kg)
2025-07-30 04:25:14,598 - energy_eval - INFO - Completed q+r for gemma3:27b in 0h 21m 44s
2025-07-30 04:25:19,907 - energy_eval - INFO - Completed gemma3:27b in 0h 21m 50s
2025-07-30 04:25:19,907 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 25m 18s
============================================================
2025-07-30 04:25:24,241 - __main__ - INFO - Starting run: gemma3_27_dev_testing - 1
2025-07-30 04:25:24,241 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:27b': {'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 04:25:24,241 - energy_eval - INFO - Using device: cuda
2025-07-30 04:25:24,242 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 04:25:24,260 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 04:25:24,260 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 04:28:57,258 - energy_eval - INFO - Loaded wiki in 0h3m32s
2025-07-30 04:28:57,258 - energy_eval - INFO - 
============================================================
Running model: gemma3:27b
============================================================
2025-07-30 04:28:57,258 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 04:28:57,258 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-27b_q+r_128_dev_1.csv
2025-07-30 04:38:46,107 - energy_eval - INFO - Built 128 prompts in 0h 9m 48s (+ retrieval: 0h 0m 7s, 0.0019 kWh, 0.0006 kg)
2025-07-30 04:50:40,934 - energy_eval - INFO - Completed q+r for gemma3:27b in 0h 21m 43s
2025-07-30 04:50:46,274 - energy_eval - INFO - Completed gemma3:27b in 0h 21m 49s
2025-07-30 04:50:46,275 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 25m 22s
============================================================
2025-07-30 04:50:50,467 - __main__ - INFO - Starting run: gemma3_27_dev_testing - 2
2025-07-30 04:50:50,467 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:27b': {'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 04:50:50,468 - energy_eval - INFO - Using device: cuda
2025-07-30 04:50:50,468 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 04:50:50,484 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 04:50:50,484 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 04:54:22,499 - energy_eval - INFO - Loaded wiki in 0h3m32s
2025-07-30 04:54:22,499 - energy_eval - INFO - 
============================================================
Running model: gemma3:27b
============================================================
2025-07-30 04:54:22,499 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 04:54:22,499 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-27b_q+r_128_dev_2.csv
2025-07-30 05:04:13,107 - energy_eval - INFO - Built 128 prompts in 0h 9m 50s (+ retrieval: 0h 0m 7s, 0.0019 kWh, 0.0006 kg)
2025-07-30 05:16:08,462 - energy_eval - INFO - Completed q+r for gemma3:27b in 0h 21m 45s
2025-07-30 05:16:13,778 - energy_eval - INFO - Completed gemma3:27b in 0h 21m 51s
2025-07-30 05:16:13,778 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 25m 23s
============================================================
2025-07-30 05:16:18,025 - __main__ - INFO - Starting run: gemma3_27_dev_testing - 3
2025-07-30 05:16:18,026 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:27b': {'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 05:16:18,026 - energy_eval - INFO - Using device: cuda
2025-07-30 05:16:18,026 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 05:16:18,049 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 05:16:18,049 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 05:19:51,906 - energy_eval - INFO - Loaded wiki in 0h3m33s
2025-07-30 05:19:51,906 - energy_eval - INFO - 
============================================================
Running model: gemma3:27b
============================================================
2025-07-30 05:19:51,906 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 05:19:51,907 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-27b_q+r_128_dev_3.csv
2025-07-30 05:29:41,513 - energy_eval - INFO - Built 128 prompts in 0h 9m 49s (+ retrieval: 0h 0m 7s, 0.0019 kWh, 0.0006 kg)
2025-07-30 05:41:34,001 - energy_eval - INFO - Completed q+r for gemma3:27b in 0h 21m 42s
2025-07-30 05:41:39,360 - energy_eval - INFO - Completed gemma3:27b in 0h 21m 47s
2025-07-30 05:41:39,360 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 25m 21s
============================================================
2025-07-30 05:41:43,418 - __main__ - INFO - Starting run: gemma3_27_dev_testing - 4
2025-07-30 05:41:43,419 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:27b': {'q+r': True}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 05:41:43,419 - energy_eval - INFO - Using device: cuda
2025-07-30 05:41:43,419 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 05:41:43,435 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 05:41:43,435 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 05:45:16,810 - energy_eval - INFO - Loaded wiki in 0h3m33s
2025-07-30 05:45:16,810 - energy_eval - INFO - 
============================================================
Running model: gemma3:27b
============================================================
2025-07-30 05:45:16,811 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 05:45:16,811 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-27b_q+r_128_dev_4.csv
2025-07-30 05:55:07,142 - energy_eval - INFO - Built 128 prompts in 0h 9m 50s (+ retrieval: 0h 0m 7s, 0.0019 kWh, 0.0005 kg)
2025-07-30 06:07:01,132 - energy_eval - INFO - Completed q+r for gemma3:27b in 0h 21m 44s
2025-07-30 06:07:06,466 - energy_eval - INFO - Completed gemma3:27b in 0h 21m 49s
2025-07-30 06:07:06,466 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 25m 23s
============================================================
2025-07-30 06:07:10,753 - __main__ - INFO - Starting run: deepseek_8_dev_testing - 0
2025-07-30 06:07:10,754 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 06:07:10,754 - energy_eval - INFO - Using device: cuda
2025-07-30 06:07:10,754 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 06:07:10,782 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 06:07:10,782 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 06:10:43,406 - energy_eval - INFO - Loaded wiki in 0h3m32s
2025-07-30 06:10:43,406 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-30 06:10:43,406 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 06:10:43,406 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-4b_q_128_dev_think_0.csv
2025-07-30 06:10:43,423 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 06:10:48,115 - __main__ - ERROR - Run deepseek_8_dev_testing failed: registry.ollama.ai/library/gemma3:4b does not support thinking (status code: 400)
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\run_multiple.py", line 12, in safe_run
    run(file_suffix)
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 84, in run
    run_model_mode(
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 228, in run_model_mode
    _ = inference("Ready?", model_name, mode_tag, provider)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 102, in inference
    text = inference_ollama(prompt, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 26, in inference_ollama
    resp = generate(
           ^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\venv\Lib\site-packages\ollama\_client.py", line 247, in generate
    return self._request(
           ^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\venv\Lib\site-packages\ollama\_client.py", line 180, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\venv\Lib\site-packages\ollama\_client.py", line 124, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: registry.ollama.ai/library/gemma3:4b does not support thinking (status code: 400)
2025-07-30 06:10:48,143 - __main__ - INFO - Starting run: deepseek_8_dev_testing - 1
2025-07-30 06:10:48,144 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 06:10:48,144 - energy_eval - INFO - Using device: cuda
2025-07-30 06:10:48,144 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 06:10:52,319 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 06:10:52,320 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 06:14:20,427 - energy_eval - INFO - Loaded wiki in 0h3m28s
2025-07-30 06:14:20,427 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-30 06:14:20,427 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 06:14:20,427 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-4b_q_128_dev_think_1.csv
2025-07-30 06:14:20,440 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 06:14:25,056 - __main__ - ERROR - Run deepseek_8_dev_testing failed: registry.ollama.ai/library/gemma3:4b does not support thinking (status code: 400)
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\run_multiple.py", line 12, in safe_run
    run(file_suffix)
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 84, in run
    run_model_mode(
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 228, in run_model_mode
    _ = inference("Ready?", model_name, mode_tag, provider)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 102, in inference
    text = inference_ollama(prompt, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 26, in inference_ollama
    resp = generate(
           ^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\venv\Lib\site-packages\ollama\_client.py", line 247, in generate
    return self._request(
           ^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\venv\Lib\site-packages\ollama\_client.py", line 180, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\venv\Lib\site-packages\ollama\_client.py", line 124, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: registry.ollama.ai/library/gemma3:4b does not support thinking (status code: 400)
2025-07-30 06:14:25,057 - __main__ - INFO - Starting run: deepseek_8_dev_testing - 2
2025-07-30 06:14:25,057 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 06:14:25,057 - energy_eval - INFO - Using device: cuda
2025-07-30 06:14:25,057 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 06:14:29,242 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 06:14:29,242 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 06:17:55,990 - energy_eval - INFO - Loaded wiki in 0h3m26s
2025-07-30 06:17:55,990 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-30 06:17:55,990 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 06:17:55,990 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-4b_q_128_dev_think_2.csv
2025-07-30 06:17:56,003 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 06:18:00,612 - __main__ - ERROR - Run deepseek_8_dev_testing failed: registry.ollama.ai/library/gemma3:4b does not support thinking (status code: 400)
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\run_multiple.py", line 12, in safe_run
    run(file_suffix)
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 84, in run
    run_model_mode(
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 228, in run_model_mode
    _ = inference("Ready?", model_name, mode_tag, provider)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 102, in inference
    text = inference_ollama(prompt, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 26, in inference_ollama
    resp = generate(
           ^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\venv\Lib\site-packages\ollama\_client.py", line 247, in generate
    return self._request(
           ^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\venv\Lib\site-packages\ollama\_client.py", line 180, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\venv\Lib\site-packages\ollama\_client.py", line 124, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: registry.ollama.ai/library/gemma3:4b does not support thinking (status code: 400)
2025-07-30 06:18:00,613 - __main__ - INFO - Starting run: deepseek_8_dev_testing - 3
2025-07-30 06:18:00,613 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 06:18:00,613 - energy_eval - INFO - Using device: cuda
2025-07-30 06:18:00,613 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 06:18:05,007 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 06:18:05,008 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 06:21:28,937 - energy_eval - INFO - Loaded wiki in 0h3m23s
2025-07-30 06:21:28,937 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-30 06:21:28,937 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 06:21:28,937 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-4b_q_128_dev_think_3.csv
2025-07-30 06:21:28,950 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 06:21:33,556 - __main__ - ERROR - Run deepseek_8_dev_testing failed: registry.ollama.ai/library/gemma3:4b does not support thinking (status code: 400)
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\run_multiple.py", line 12, in safe_run
    run(file_suffix)
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 84, in run
    run_model_mode(
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 228, in run_model_mode
    _ = inference("Ready?", model_name, mode_tag, provider)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 102, in inference
    text = inference_ollama(prompt, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 26, in inference_ollama
    resp = generate(
           ^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\venv\Lib\site-packages\ollama\_client.py", line 247, in generate
    return self._request(
           ^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\venv\Lib\site-packages\ollama\_client.py", line 180, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\venv\Lib\site-packages\ollama\_client.py", line 124, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: registry.ollama.ai/library/gemma3:4b does not support thinking (status code: 400)
2025-07-30 06:21:37,844 - __main__ - INFO - Starting run: deepseek_8_dev_testing - 4
2025-07-30 06:21:37,845 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:4b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:4b': {'q': False, 'q+r': True}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 06:21:37,845 - energy_eval - INFO - Using device: cuda
2025-07-30 06:21:37,846 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 06:21:37,868 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 06:21:37,869 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 06:25:17,428 - energy_eval - INFO - Loaded wiki in 0h3m39s
2025-07-30 06:25:17,428 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-30 06:25:17,428 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 06:25:17,428 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-4b_q_128_dev_think_4.csv
2025-07-30 06:25:17,441 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 06:25:22,134 - __main__ - ERROR - Run deepseek_8_dev_testing failed: registry.ollama.ai/library/gemma3:4b does not support thinking (status code: 400)
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\run_multiple.py", line 12, in safe_run
    run(file_suffix)
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 84, in run
    run_model_mode(
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 228, in run_model_mode
    _ = inference("Ready?", model_name, mode_tag, provider)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 102, in inference
    text = inference_ollama(prompt, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 26, in inference_ollama
    resp = generate(
           ^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\venv\Lib\site-packages\ollama\_client.py", line 247, in generate
    return self._request(
           ^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\venv\Lib\site-packages\ollama\_client.py", line 180, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\venv\Lib\site-packages\ollama\_client.py", line 124, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: registry.ollama.ai/library/gemma3:4b does not support thinking (status code: 400)
2025-07-30 06:25:22,135 - __main__ - INFO - Starting run: deepseek_14_dev_testing - 0
2025-07-30 06:25:22,135 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 06:25:22,135 - energy_eval - INFO - Using device: cuda
2025-07-30 06:25:22,135 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 06:25:26,330 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 06:25:26,330 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 06:28:55,103 - energy_eval - INFO - Loaded wiki in 0h3m28s
2025-07-30 06:28:55,103 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-30 06:28:55,103 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 06:28:55,103 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-8b_q_128_dev_think_0.csv
2025-07-30 06:28:55,115 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 07:26:40,021 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 57m 44s
2025-07-30 07:26:40,022 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-8b_q+r_128_dev_think_0.csv
2025-07-30 07:36:28,183 - energy_eval - INFO - Built 128 prompts in 0h 9m 48s (+ retrieval: 0h 0m 7s, 0.0019 kWh, 0.0006 kg)
2025-07-30 08:07:05,680 - energy_eval - INFO - Completed q+r for deepseek-r1:8b in 0h 40m 25s
2025-07-30 08:07:11,782 - energy_eval - INFO - Completed deepseek-r1:8b in 1h 38m 16s
2025-07-30 08:07:11,782 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 41m 49s
============================================================
2025-07-30 08:07:16,464 - __main__ - INFO - Starting run: deepseek_14_dev_testing - 1
2025-07-30 08:07:16,465 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 08:07:16,465 - energy_eval - INFO - Using device: cuda
2025-07-30 08:07:16,465 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 08:07:16,479 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 08:07:16,479 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 13:41:39,865 - __main__ - INFO - Starting run: gemma3_27_dev_testing - 0
2025-07-30 13:41:39,865 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:27b': {'q': False}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 13:41:39,865 - energy_eval - INFO - Using device: cuda
2025-07-30 13:41:39,865 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 13:41:39,884 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 13:41:39,884 - energy_eval - INFO - 
============================================================
Running model: gemma3:27b
============================================================
2025-07-30 13:41:39,884 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-30 13:41:39,884 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-27b_q_128_dev_0.csv
2025-07-30 13:41:39,897 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 13:52:38,328 - energy_eval - INFO - Completed q for gemma3:27b in 0h 10m 58s
2025-07-30 13:52:38,409 - energy_eval - INFO - Completed gemma3:27b in 0h 10m 58s
2025-07-30 13:52:38,409 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 10m 58s
============================================================
2025-07-30 13:52:38,409 - __main__ - INFO - Starting run: gemma3_27_dev_testing - 1
2025-07-30 13:52:38,409 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:27b': {'q': False}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 13:52:38,409 - energy_eval - INFO - Using device: cuda
2025-07-30 13:52:38,410 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 13:52:38,421 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 13:52:38,421 - energy_eval - INFO - 
============================================================
Running model: gemma3:27b
============================================================
2025-07-30 13:52:38,421 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-30 13:52:38,421 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-27b_q_128_dev_1.csv
2025-07-30 13:52:38,433 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 14:03:24,948 - energy_eval - INFO - Completed q for gemma3:27b in 0h 10m 46s
2025-07-30 14:03:25,047 - energy_eval - INFO - Completed gemma3:27b in 0h 10m 46s
2025-07-30 14:03:25,047 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 10m 46s
============================================================
2025-07-30 14:03:25,047 - __main__ - INFO - Starting run: gemma3_27_dev_testing - 2
2025-07-30 14:03:25,047 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:27b': {'q': False}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 14:03:25,048 - energy_eval - INFO - Using device: cuda
2025-07-30 14:03:25,048 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 14:03:25,058 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 14:03:25,058 - energy_eval - INFO - 
============================================================
Running model: gemma3:27b
============================================================
2025-07-30 14:03:25,058 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-30 14:03:25,058 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-27b_q_128_dev_2.csv
2025-07-30 14:03:25,071 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 14:14:10,775 - energy_eval - INFO - Completed q for gemma3:27b in 0h 10m 45s
2025-07-30 14:14:10,856 - energy_eval - INFO - Completed gemma3:27b in 0h 10m 45s
2025-07-30 14:14:10,856 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 10m 45s
============================================================
2025-07-30 14:14:10,856 - __main__ - INFO - Starting run: gemma3_27_dev_testing - 3
2025-07-30 14:14:10,856 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:27b': {'q': False}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 14:14:10,856 - energy_eval - INFO - Using device: cuda
2025-07-30 14:14:10,856 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 14:14:10,867 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 14:14:10,867 - energy_eval - INFO - 
============================================================
Running model: gemma3:27b
============================================================
2025-07-30 14:14:10,867 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-30 14:14:10,867 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-27b_q_128_dev_3.csv
2025-07-30 14:14:10,879 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 14:24:57,780 - energy_eval - INFO - Completed q for gemma3:27b in 0h 10m 46s
2025-07-30 14:24:57,860 - energy_eval - INFO - Completed gemma3:27b in 0h 10m 46s
2025-07-30 14:24:57,860 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 10m 47s
============================================================
2025-07-30 14:24:57,860 - __main__ - INFO - Starting run: gemma3_27_dev_testing - 4
2025-07-30 14:24:57,861 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:27b': {'q': False}}, think=False, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 14:24:57,861 - energy_eval - INFO - Using device: cuda
2025-07-30 14:24:57,861 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 14:24:57,871 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 14:24:57,871 - energy_eval - INFO - 
============================================================
Running model: gemma3:27b
============================================================
2025-07-30 14:24:57,871 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-30 14:24:57,872 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_gemma3-27b_q_128_dev_4.csv
2025-07-30 14:24:57,886 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 14:35:48,569 - energy_eval - INFO - Completed q for gemma3:27b in 0h 10m 50s
2025-07-30 14:35:48,683 - energy_eval - INFO - Completed gemma3:27b in 0h 10m 50s
2025-07-30 14:35:48,684 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 10m 50s
============================================================
2025-07-30 14:35:48,684 - __main__ - INFO - Starting run: deepseek_8_dev_testing - 0
2025-07-30 14:35:48,684 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 14:35:48,684 - energy_eval - INFO - Using device: cuda
2025-07-30 14:35:48,684 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 14:35:48,705 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 14:35:48,705 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 14:40:01,241 - energy_eval - INFO - Loaded wiki in 0h4m12s
2025-07-30 14:40:01,241 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-30 14:40:01,241 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 14:40:01,241 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-8b_q_128_dev_think_0.csv
2025-07-30 14:40:01,265 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 16:14:40,132 - energy_eval - INFO - Completed q for deepseek-r1:8b in 1h 34m 38s
2025-07-30 16:14:40,133 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-8b_q+r_128_dev_think_0.csv
2025-07-30 16:24:40,998 - energy_eval - INFO - Built 128 prompts in 0h 10m 0s (+ retrieval: 0h 0m 11s, 0.0034 kWh, 0.0010 kg)
2025-07-30 16:55:18,632 - energy_eval - INFO - Completed q+r for deepseek-r1:8b in 0h 40m 38s
2025-07-30 16:55:40,615 - energy_eval - INFO - Completed deepseek-r1:8b in 2h 15m 39s
2025-07-30 16:55:40,615 - energy_eval - INFO - 
============================================================
Experiment completed in 2h 19m 51s
============================================================
2025-07-30 16:55:47,646 - __main__ - INFO - Starting run: deepseek_8_dev_testing - 1
2025-07-30 16:55:47,647 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 16:55:47,647 - energy_eval - INFO - Using device: cuda
2025-07-30 16:55:47,647 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 16:55:47,662 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 16:55:47,662 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 16:59:28,437 - energy_eval - INFO - Loaded wiki in 0h3m40s
2025-07-30 16:59:28,438 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-30 16:59:28,438 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 16:59:28,438 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-8b_q_128_dev_think_1.csv
2025-07-30 16:59:28,451 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 17:10:15,637 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 10m 47s
2025-07-30 17:10:15,637 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-8b_q+r_128_dev_think_1.csv
2025-07-30 17:20:03,127 - energy_eval - INFO - Built 128 prompts in 0h 9m 47s (+ retrieval: 0h 0m 7s, 0.0019 kWh, 0.0006 kg)
2025-07-30 17:50:36,606 - energy_eval - INFO - Completed q+r for deepseek-r1:8b in 0h 40m 20s
2025-07-30 17:50:41,906 - energy_eval - INFO - Completed deepseek-r1:8b in 0h 51m 13s
2025-07-30 17:50:41,906 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 54m 54s
============================================================
2025-07-30 17:50:45,935 - __main__ - INFO - Starting run: deepseek_8_dev_testing - 2
2025-07-30 17:50:45,935 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 17:50:45,935 - energy_eval - INFO - Using device: cuda
2025-07-30 17:50:45,935 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 17:50:45,948 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 17:50:45,948 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 17:54:19,402 - energy_eval - INFO - Loaded wiki in 0h3m33s
2025-07-30 17:54:19,402 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-30 17:54:19,402 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 17:54:19,403 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-8b_q_128_dev_think_2.csv
2025-07-30 17:54:19,416 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 18:05:10,836 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 10m 51s
2025-07-30 18:05:10,837 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-8b_q+r_128_dev_think_2.csv
2025-07-30 18:14:58,656 - energy_eval - INFO - Built 128 prompts in 0h 9m 47s (+ retrieval: 0h 0m 7s, 0.0019 kWh, 0.0006 kg)
2025-07-30 18:45:31,594 - energy_eval - INFO - Completed q+r for deepseek-r1:8b in 0h 40m 20s
2025-07-30 18:45:36,870 - energy_eval - INFO - Completed deepseek-r1:8b in 0h 51m 17s
2025-07-30 18:45:36,870 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 54m 50s
============================================================
2025-07-30 18:45:40,981 - __main__ - INFO - Starting run: deepseek_8_dev_testing - 3
2025-07-30 18:45:40,982 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 18:45:40,982 - energy_eval - INFO - Using device: cuda
2025-07-30 18:45:40,982 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 18:45:40,994 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 18:45:40,994 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 18:49:16,863 - energy_eval - INFO - Loaded wiki in 0h3m35s
2025-07-30 18:49:16,863 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-30 18:49:16,863 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 18:49:16,863 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-8b_q_128_dev_think_3.csv
2025-07-30 18:49:16,877 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 19:00:03,615 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 10m 46s
2025-07-30 19:00:03,616 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-8b_q+r_128_dev_think_3.csv
2025-07-30 19:09:51,512 - energy_eval - INFO - Built 128 prompts in 0h 9m 47s (+ retrieval: 0h 0m 7s, 0.0019 kWh, 0.0006 kg)
2025-07-30 19:49:39,616 - energy_eval - INFO - Completed q+r for deepseek-r1:8b in 0h 49m 36s
2025-07-30 19:49:46,228 - energy_eval - INFO - Completed deepseek-r1:8b in 1h 0m 29s
2025-07-30 19:49:46,228 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 4m 5s
============================================================
2025-07-30 19:49:50,843 - __main__ - INFO - Starting run: deepseek_8_dev_testing - 4
2025-07-30 19:49:50,844 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 19:49:50,844 - energy_eval - INFO - Using device: cuda
2025-07-30 19:49:50,844 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 19:49:50,861 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 19:49:50,861 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 19:53:46,037 - energy_eval - INFO - Loaded wiki in 0h3m55s
2025-07-30 19:53:46,038 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-30 19:53:46,038 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 19:53:46,038 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-8b_q_128_dev_think_4.csv
2025-07-30 19:53:46,052 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 20:04:51,820 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 11m 5s
2025-07-30 20:04:51,821 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-8b_q+r_128_dev_think_4.csv
2025-07-30 20:14:44,380 - energy_eval - INFO - Built 128 prompts in 0h 9m 52s (+ retrieval: 0h 0m 8s, 0.0024 kWh, 0.0007 kg)
2025-07-30 21:05:41,722 - energy_eval - INFO - Completed q+r for deepseek-r1:8b in 1h 0m 49s
2025-07-30 21:05:47,955 - energy_eval - INFO - Completed deepseek-r1:8b in 1h 12m 1s
2025-07-30 21:05:47,955 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 15m 57s
============================================================
2025-07-30 21:05:52,716 - __main__ - INFO - Starting run: deepseek_14_dev_testing - 0
2025-07-30 21:05:52,716 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:14b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:14b': {'q': False, 'q+r': True}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 21:05:52,716 - energy_eval - INFO - Using device: cuda
2025-07-30 21:05:52,716 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 21:05:52,730 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 21:05:52,730 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 21:09:46,717 - energy_eval - INFO - Loaded wiki in 0h3m53s
2025-07-30 21:09:46,717 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-07-30 21:09:46,717 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 21:09:46,717 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-14b_q_128_dev_think_0.csv
2025-07-30 21:09:46,732 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 21:46:15,503 - energy_eval - INFO - Completed q for deepseek-r1:14b in 0h 36m 28s
2025-07-30 21:46:15,503 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-14b_q+r_128_dev_think_0.csv
2025-07-30 21:56:08,582 - energy_eval - INFO - Built 128 prompts in 0h 9m 53s (+ retrieval: 0h 0m 7s, 0.0022 kWh, 0.0007 kg)
2025-07-30 22:26:43,859 - energy_eval - INFO - Completed q+r for deepseek-r1:14b in 0h 40m 28s
2025-07-30 22:26:49,599 - energy_eval - INFO - Completed deepseek-r1:14b in 1h 17m 2s
2025-07-30 22:26:49,599 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 20m 56s
============================================================
2025-07-30 22:26:54,248 - __main__ - INFO - Starting run: deepseek_14_dev_testing - 1
2025-07-30 22:26:54,249 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:14b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:14b': {'q': False, 'q+r': True}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 22:26:54,249 - energy_eval - INFO - Using device: cuda
2025-07-30 22:26:54,249 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 22:26:54,264 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 22:26:54,264 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 22:30:49,580 - energy_eval - INFO - Loaded wiki in 0h3m55s
2025-07-30 22:30:49,580 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-07-30 22:30:49,580 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 22:30:49,580 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-14b_q_128_dev_think_1.csv
2025-07-30 22:30:49,596 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-30 23:07:19,041 - energy_eval - INFO - Completed q for deepseek-r1:14b in 0h 36m 29s
2025-07-30 23:07:19,041 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-14b_q+r_128_dev_think_1.csv
2025-07-30 23:17:10,425 - energy_eval - INFO - Built 128 prompts in 0h 9m 51s (+ retrieval: 0h 0m 8s, 0.0023 kWh, 0.0007 kg)
2025-07-30 23:47:56,720 - energy_eval - INFO - Completed q+r for deepseek-r1:14b in 0h 40m 37s
2025-07-30 23:48:02,510 - energy_eval - INFO - Completed deepseek-r1:14b in 1h 17m 12s
2025-07-30 23:48:02,510 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 21m 8s
============================================================
2025-07-30 23:48:07,120 - __main__ - INFO - Starting run: deepseek_14_dev_testing - 2
2025-07-30 23:48:07,120 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:14b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:14b': {'q': False, 'q+r': True}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-30 23:48:07,120 - energy_eval - INFO - Using device: cuda
2025-07-30 23:48:07,120 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-30 23:48:07,137 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-30 23:48:07,138 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-30 23:52:01,491 - energy_eval - INFO - Loaded wiki in 0h3m54s
2025-07-30 23:52:01,491 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-07-30 23:52:01,491 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-30 23:52:01,492 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-14b_q_128_dev_think_2.csv
2025-07-30 23:52:01,510 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-31 00:28:44,378 - energy_eval - INFO - Completed q for deepseek-r1:14b in 0h 36m 42s
2025-07-31 00:28:44,378 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-14b_q+r_128_dev_think_2.csv
2025-07-31 00:38:35,382 - energy_eval - INFO - Built 128 prompts in 0h 9m 51s (+ retrieval: 0h 0m 8s, 0.0022 kWh, 0.0007 kg)
2025-07-31 01:09:12,029 - energy_eval - INFO - Completed q+r for deepseek-r1:14b in 0h 40m 27s
2025-07-31 01:09:18,585 - energy_eval - INFO - Completed deepseek-r1:14b in 1h 17m 17s
2025-07-31 01:09:18,585 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 21m 11s
============================================================
2025-07-31 01:09:23,328 - __main__ - INFO - Starting run: deepseek_14_dev_testing - 3
2025-07-31 01:09:23,329 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:14b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:14b': {'q': False, 'q+r': True}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-31 01:09:23,329 - energy_eval - INFO - Using device: cuda
2025-07-31 01:09:23,329 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-31 01:09:23,352 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-31 01:09:23,352 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-31 01:13:18,203 - energy_eval - INFO - Loaded wiki in 0h3m54s
2025-07-31 01:13:18,203 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-07-31 01:13:18,203 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-31 01:13:18,203 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-14b_q_128_dev_think_3.csv
2025-07-31 01:13:18,221 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-31 01:49:59,797 - energy_eval - INFO - Completed q for deepseek-r1:14b in 0h 36m 41s
2025-07-31 01:49:59,797 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-14b_q+r_128_dev_think_3.csv
2025-07-31 01:59:51,692 - energy_eval - INFO - Built 128 prompts in 0h 9m 51s (+ retrieval: 0h 0m 8s, 0.0023 kWh, 0.0007 kg)
2025-07-31 02:30:39,545 - energy_eval - INFO - Completed q+r for deepseek-r1:14b in 0h 40m 39s
2025-07-31 02:30:45,342 - energy_eval - INFO - Completed deepseek-r1:14b in 1h 17m 27s
2025-07-31 02:30:45,342 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 21m 22s
============================================================
2025-07-31 02:30:49,934 - __main__ - INFO - Starting run: deepseek_14_dev_testing - 4
2025-07-31 02:30:49,935 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:14b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:14b': {'q': False, 'q+r': True}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-31 02:30:49,935 - energy_eval - INFO - Using device: cuda
2025-07-31 02:30:49,935 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-31 02:30:49,958 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-31 02:30:49,958 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-31 02:34:43,638 - energy_eval - INFO - Loaded wiki in 0h3m53s
2025-07-31 02:34:43,638 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-07-31 02:34:43,638 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-31 02:34:43,638 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-14b_q_128_dev_think_4.csv
2025-07-31 02:34:43,652 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-31 03:11:26,315 - energy_eval - INFO - Completed q for deepseek-r1:14b in 0h 36m 42s
2025-07-31 03:11:26,315 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-14b_q+r_128_dev_think_4.csv
2025-07-31 03:21:16,926 - energy_eval - INFO - Built 128 prompts in 0h 9m 50s (+ retrieval: 0h 0m 8s, 0.0024 kWh, 0.0007 kg)
2025-07-31 03:52:08,389 - energy_eval - INFO - Completed q+r for deepseek-r1:14b in 0h 40m 42s
2025-07-31 03:52:14,148 - energy_eval - INFO - Completed deepseek-r1:14b in 1h 17m 30s
2025-07-31 03:52:14,148 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 21m 24s
============================================================
2025-07-31 03:52:18,839 - __main__ - INFO - Starting run: deepseek_32_dev_testing - 0
2025-07-31 03:52:18,840 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-31 03:52:18,840 - energy_eval - INFO - Using device: cuda
2025-07-31 03:52:18,840 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-31 03:52:18,859 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-31 03:52:18,859 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-31 03:52:18,859 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-31 03:52:18,859 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-32b_q_128_dev_think_0.csv
2025-07-31 03:52:18,874 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-31 05:22:20,303 - energy_eval - INFO - Completed q for deepseek-r1:32b in 1h 30m 1s
2025-07-31 05:22:20,446 - energy_eval - INFO - Completed deepseek-r1:32b in 1h 30m 1s
2025-07-31 05:22:20,446 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 30m 1s
============================================================
2025-07-31 05:22:20,446 - __main__ - INFO - Starting run: deepseek_32_dev_testing - 1
2025-07-31 05:22:20,447 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-31 05:22:20,447 - energy_eval - INFO - Using device: cuda
2025-07-31 05:22:20,447 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-31 05:22:20,460 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-31 05:22:20,460 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-31 05:22:20,460 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-31 05:22:20,460 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-32b_q_128_dev_think_1.csv
2025-07-31 05:22:20,474 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-31 06:52:04,965 - energy_eval - INFO - Completed q for deepseek-r1:32b in 1h 29m 44s
2025-07-31 06:52:05,108 - energy_eval - INFO - Completed deepseek-r1:32b in 1h 29m 44s
2025-07-31 06:52:05,108 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 29m 44s
============================================================
2025-07-31 06:52:05,108 - __main__ - INFO - Starting run: deepseek_32_dev_testing - 2
2025-07-31 06:52:05,108 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-31 06:52:05,109 - energy_eval - INFO - Using device: cuda
2025-07-31 06:52:05,109 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-31 06:52:05,122 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-31 06:52:05,122 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-31 06:52:05,122 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-31 06:52:05,122 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-32b_q_128_dev_think_2.csv
2025-07-31 06:52:05,135 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-31 08:22:31,100 - energy_eval - INFO - Completed q for deepseek-r1:32b in 1h 30m 25s
2025-07-31 08:22:31,248 - energy_eval - INFO - Completed deepseek-r1:32b in 1h 30m 26s
2025-07-31 08:22:31,248 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 30m 26s
============================================================
2025-07-31 08:22:31,248 - __main__ - INFO - Starting run: deepseek_32_dev_testing - 3
2025-07-31 08:22:31,249 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-31 08:22:31,249 - energy_eval - INFO - Using device: cuda
2025-07-31 08:22:31,249 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-31 08:22:31,263 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-31 08:22:31,263 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-31 08:22:31,263 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-31 08:22:31,264 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-32b_q_128_dev_think_3.csv
2025-07-31 08:22:31,278 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-31 09:37:43,700 - energy_eval - INFO - Completed q for deepseek-r1:32b in 1h 15m 12s
2025-07-31 09:37:43,839 - energy_eval - INFO - Completed deepseek-r1:32b in 1h 15m 12s
2025-07-31 09:37:43,839 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 15m 12s
============================================================
2025-07-31 09:37:43,839 - __main__ - INFO - Starting run: deepseek_32_dev_testing - 4
2025-07-31 09:37:43,839 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_dev_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:32b': {'q': False}}, think=True, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/testing'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', smtp_password='qjzk qmri wtrl fqyw', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Using the provided context, answer the question with as few words as possible. Be thorough in your analysis of the context but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer with as few words as possible. Be thorough in your analysis but answer in as few words as possible. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word. Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-31 09:37:43,839 - energy_eval - INFO - Using device: cuda
2025-07-31 09:37:43,840 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_dev_128.jsonl
2025-07-31 09:37:43,852 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-31 09:37:43,852 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-31 09:37:43,852 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-31 09:37:43,852 - energy_eval - INFO - Saving results to: C:\Users\Ethan\Documents\PhD\LMPowerConsuption\results\testing\hotpot_deepseek-r1-32b_q_128_dev_think_4.csv
2025-07-31 09:37:43,865 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-31 10:45:28,115 - energy_eval - INFO - Completed q for deepseek-r1:32b in 1h 7m 44s
2025-07-31 10:45:28,228 - energy_eval - INFO - Completed deepseek-r1:32b in 1h 7m 44s
2025-07-31 10:45:28,228 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 7m 44s
============================================================
