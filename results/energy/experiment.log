2025-06-27 12:46:21,029 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'smollm:135m': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=8, device='cpu', modes={'deepseek-r1:1.5b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results/energy'), result_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results'), data_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-27 12:46:21,030 - energy_eval - INFO - Using device: cpu
2025-06-27 12:46:21,030 - energy_eval - INFO - Loading dataset from /home/s72kw/Documents/LMPowerConsuption/data/boolq_mini_128.jsonl
2025-06-27 12:46:21,052 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-27 12:46:21,052 - energy_eval - INFO - 
============================================================
Running model: smollm:135m
============================================================
2025-06-27 12:46:21,052 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {}
++++++++++++++++++++++++++++++
2025-06-27 12:46:21,171 - energy_eval - INFO - Completed smollm:135m in 0h 0m 0s
2025-06-27 12:46:21,171 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 0m 0s
============================================================
2025-06-27 12:48:30,037 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'smollm:135m': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=8, device='cpu', modes={'smollm:135m': {False, 'q'}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results/energy'), result_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results'), data_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-27 12:48:30,037 - energy_eval - INFO - Using device: cpu
2025-06-27 12:48:30,038 - energy_eval - INFO - Loading dataset from /home/s72kw/Documents/LMPowerConsuption/data/boolq_mini_128.jsonl
2025-06-27 12:48:30,043 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-27 12:48:30,043 - energy_eval - INFO - 
============================================================
Running model: smollm:135m
============================================================
2025-06-27 12:48:30,043 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {False, 'q'}
++++++++++++++++++++++++++++++
2025-06-27 12:48:30,043 - energy_eval - ERROR - Critical error: 'set' object has no attribute 'items'
Traceback (most recent call last):
  File "/home/s72kw/Documents/LMPowerConsuption/src/experiment.py", line 382, in <module>
    run()
  File "/home/s72kw/Documents/LMPowerConsuption/src/experiment.py", line 58, in run
    for mode_tag, include_passage in model_modes.items():
                                     ^^^^^^^^^^^^^^^^^
AttributeError: 'set' object has no attribute 'items'
2025-06-27 12:49:22,504 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'smollm:135m': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=8, device='cpu', modes={'smollm:135m': {False, 'q'}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results/energy'), result_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results'), data_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-27 12:49:22,504 - energy_eval - INFO - Using device: cpu
2025-06-27 12:49:22,504 - energy_eval - INFO - Loading dataset from /home/s72kw/Documents/LMPowerConsuption/data/boolq_mini_128.jsonl
2025-06-27 12:49:22,510 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-27 12:49:22,510 - energy_eval - INFO - 
============================================================
Running model: smollm:135m
============================================================
2025-06-27 12:49:22,510 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {False, 'q'}
++++++++++++++++++++++++++++++
2025-06-27 12:49:22,510 - energy_eval - ERROR - Critical error: 'set' object has no attribute 'items'
Traceback (most recent call last):
  File "/home/s72kw/Documents/LMPowerConsuption/src/experiment.py", line 382, in <module>
    run()
  File "/home/s72kw/Documents/LMPowerConsuption/src/experiment.py", line 58, in run
    for mode_tag, include_passage in model_modes.items():
                                     ^^^^^^^^^^^^^^^^^
AttributeError: 'set' object has no attribute 'items'
2025-06-27 12:51:31,622 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'smollm:135m': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=8, device='cpu', modes={'smollm:135m': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results/energy'), result_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results'), data_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-27 12:51:31,622 - energy_eval - INFO - Using device: cpu
2025-06-27 12:51:31,622 - energy_eval - INFO - Loading dataset from /home/s72kw/Documents/LMPowerConsuption/data/boolq_mini_128.jsonl
2025-06-27 12:51:31,628 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-27 12:51:31,628 - energy_eval - INFO - 
============================================================
Running model: smollm:135m
============================================================
2025-06-27 12:51:31,628 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-06-27 12:53:09,159 - energy_eval - WARNING - Experiment interrupted by user
2025-06-27 12:53:49,271 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'smollm:135m': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=8, device='cpu', modes={'smollm:135m': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results/energy'), result_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results'), data_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-27 12:53:49,272 - energy_eval - INFO - Using device: cpu
2025-06-27 12:53:49,272 - energy_eval - INFO - Loading dataset from /home/s72kw/Documents/LMPowerConsuption/data/boolq_mini_128.jsonl
2025-06-27 12:53:49,277 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-27 12:53:49,277 - energy_eval - INFO - 
============================================================
Running model: smollm:135m
============================================================
2025-06-27 12:53:49,277 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-27 12:54:33,492 - energy_eval - WARNING - Experiment interrupted by user
2025-06-27 12:55:11,275 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'smollm:135m': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=8, device='cpu', modes={'smollm:135m': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results/energy'), result_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results'), data_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-27 12:55:11,275 - energy_eval - INFO - Using device: cpu
2025-06-27 12:55:11,275 - energy_eval - INFO - Loading dataset from /home/s72kw/Documents/LMPowerConsuption/data/boolq_mini_128.jsonl
2025-06-27 12:55:11,280 - energy_eval - ERROR - Dataset loading failed: Expecting value: line 2 column 1 (char 1)
2025-06-27 12:58:42,403 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'smollm:135m': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=8, device='cpu', modes={'smollm:135m': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results/energy'), result_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results'), data_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-27 12:58:42,404 - energy_eval - INFO - Using device: cpu
2025-06-27 12:58:42,404 - energy_eval - INFO - Loading dataset from /home/s72kw/Documents/LMPowerConsuption/data/boolq_mini_128.jsonl
2025-06-27 12:58:42,547 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-27 12:58:42,547 - energy_eval - INFO - 
============================================================
Running model: smollm:135m
============================================================
2025-06-27 12:58:42,547 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-27 12:58:53,576 - energy_eval - ERROR - Error processing sample 0: name 'CONGFIG' is not defined
2025-06-27 12:58:59,299 - energy_eval - WARNING - Experiment interrupted by user
2025-06-27 12:59:17,900 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'smollm:135m': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=8, device='cpu', modes={'smollm:135m': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results/energy'), result_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results'), data_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-27 12:59:17,900 - energy_eval - INFO - Using device: cpu
2025-06-27 12:59:17,900 - energy_eval - INFO - Loading dataset from /home/s72kw/Documents/LMPowerConsuption/data/boolq_mini_128.jsonl
2025-06-27 12:59:17,905 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-27 12:59:17,905 - energy_eval - INFO - 
============================================================
Running model: smollm:135m
============================================================
2025-06-27 12:59:17,905 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-27 13:01:21,759 - energy_eval - INFO - Completed q mode for smollm:135m
2025-06-27 13:04:20,952 - energy_eval - WARNING - Experiment interrupted by user
2025-06-27 13:10:45,364 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'smollm:135m': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=8, device='cpu', modes={'smollm:135m': {'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results/energy'), result_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results'), data_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-27 13:10:45,364 - energy_eval - INFO - Using device: cpu
2025-06-27 13:10:45,364 - energy_eval - INFO - Loading dataset from /home/s72kw/Documents/LMPowerConsuption/data/boolq_mini_128.jsonl
2025-06-27 13:10:45,369 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-27 13:10:45,369 - energy_eval - INFO - 
============================================================
Running model: smollm:135m
============================================================
2025-06-27 13:10:45,369 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-06-27 13:16:47,863 - energy_eval - INFO - Loaded Wikipedia corpus and indexes
2025-06-27 13:24:00,698 - energy_eval - WARNING - Experiment interrupted by user
2025-06-27 13:25:51,871 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results/energy'), result_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/results'), data_dir=PosixPath('/home/s72kw/Documents/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-27 13:25:51,872 - energy_eval - INFO - Using device: cpu
2025-06-27 13:25:51,872 - energy_eval - INFO - Loading dataset from /home/s72kw/Documents/LMPowerConsuption/data/boolq_mini_128.jsonl
2025-06-27 13:25:51,916 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-27 13:25:51,916 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-27 13:25:51,916 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-27 15:49:05,510 - energy_eval - INFO - Completed q mode for deepseek-r1:8b
2025-06-27 15:55:02,228 - energy_eval - INFO - Loaded Wikipedia corpus and indexes
2025-06-28 05:20:14,777 - energy_eval - INFO - Completed q+r mode for deepseek-r1:8b
2025-06-28 05:26:23,032 - energy_eval - INFO - Completed deepseek-r1:8b in 16h 0m 31s
2025-06-28 05:26:23,077 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-06-28 05:26:23,077 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 09:59:30,872 - energy_eval - INFO - Completed q mode for deepseek-r1:14b
2025-06-28 10:05:48,801 - energy_eval - INFO - Loaded Wikipedia corpus and indexes
2025-06-28 10:06:03,808 - energy_eval - ERROR - Error processing sample 0: model requires more system memory (10.5 GiB) than is available (9.7 GiB) (status code: 500)
2025-06-28 10:06:14,453 - energy_eval - ERROR - Error processing sample 1: model requires more system memory (10.5 GiB) than is available (9.7 GiB) (status code: 500)
2025-06-28 10:06:25,040 - energy_eval - ERROR - Error processing sample 2: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:06:35,597 - energy_eval - ERROR - Error processing sample 3: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:06:46,246 - energy_eval - ERROR - Error processing sample 4: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:06:56,821 - energy_eval - ERROR - Error processing sample 5: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:07:07,397 - energy_eval - ERROR - Error processing sample 6: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:07:17,976 - energy_eval - ERROR - Error processing sample 7: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:07:28,597 - energy_eval - ERROR - Error processing sample 8: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:07:39,198 - energy_eval - ERROR - Error processing sample 9: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:07:49,790 - energy_eval - ERROR - Error processing sample 10: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:08:00,453 - energy_eval - ERROR - Error processing sample 11: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:08:11,027 - energy_eval - ERROR - Error processing sample 12: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:08:21,625 - energy_eval - ERROR - Error processing sample 13: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:08:32,242 - energy_eval - ERROR - Error processing sample 14: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:08:42,825 - energy_eval - ERROR - Error processing sample 15: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:08:53,453 - energy_eval - ERROR - Error processing sample 16: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:09:04,034 - energy_eval - ERROR - Error processing sample 17: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:09:14,617 - energy_eval - ERROR - Error processing sample 18: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:09:25,246 - energy_eval - ERROR - Error processing sample 19: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:09:35,812 - energy_eval - ERROR - Error processing sample 20: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:09:46,403 - energy_eval - ERROR - Error processing sample 21: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:09:56,987 - energy_eval - ERROR - Error processing sample 22: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:10:07,591 - energy_eval - ERROR - Error processing sample 23: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:10:18,167 - energy_eval - ERROR - Error processing sample 24: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:10:28,791 - energy_eval - ERROR - Error processing sample 25: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:10:39,369 - energy_eval - ERROR - Error processing sample 26: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:10:49,985 - energy_eval - ERROR - Error processing sample 27: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:11:00,617 - energy_eval - ERROR - Error processing sample 28: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:11:11,233 - energy_eval - ERROR - Error processing sample 29: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:11:21,811 - energy_eval - ERROR - Error processing sample 30: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:11:32,428 - energy_eval - ERROR - Error processing sample 31: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:11:43,042 - energy_eval - ERROR - Error processing sample 32: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:11:53,656 - energy_eval - ERROR - Error processing sample 33: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:12:04,235 - energy_eval - ERROR - Error processing sample 34: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:12:15,038 - energy_eval - ERROR - Error processing sample 35: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:12:25,605 - energy_eval - ERROR - Error processing sample 36: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:12:36,194 - energy_eval - ERROR - Error processing sample 37: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:12:46,820 - energy_eval - ERROR - Error processing sample 38: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:12:57,406 - energy_eval - ERROR - Error processing sample 39: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:13:07,998 - energy_eval - ERROR - Error processing sample 40: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:13:18,558 - energy_eval - ERROR - Error processing sample 41: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:13:29,203 - energy_eval - ERROR - Error processing sample 42: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:13:39,812 - energy_eval - ERROR - Error processing sample 43: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:13:50,393 - energy_eval - ERROR - Error processing sample 44: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:14:00,973 - energy_eval - ERROR - Error processing sample 45: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:14:11,577 - energy_eval - ERROR - Error processing sample 46: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:14:22,160 - energy_eval - ERROR - Error processing sample 47: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:14:32,753 - energy_eval - ERROR - Error processing sample 48: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:14:43,335 - energy_eval - ERROR - Error processing sample 49: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:14:53,926 - energy_eval - ERROR - Error processing sample 50: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:15:04,519 - energy_eval - ERROR - Error processing sample 51: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:15:15,125 - energy_eval - ERROR - Error processing sample 52: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:15:25,706 - energy_eval - ERROR - Error processing sample 53: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:15:36,285 - energy_eval - ERROR - Error processing sample 54: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:15:46,878 - energy_eval - ERROR - Error processing sample 55: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:15:57,464 - energy_eval - ERROR - Error processing sample 56: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:16:08,063 - energy_eval - ERROR - Error processing sample 57: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:16:18,660 - energy_eval - ERROR - Error processing sample 58: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:16:29,432 - energy_eval - ERROR - Error processing sample 59: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:16:40,030 - energy_eval - ERROR - Error processing sample 60: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:16:50,649 - energy_eval - ERROR - Error processing sample 61: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:17:01,260 - energy_eval - ERROR - Error processing sample 62: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:17:11,840 - energy_eval - ERROR - Error processing sample 63: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:17:22,428 - energy_eval - ERROR - Error processing sample 64: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:17:33,018 - energy_eval - ERROR - Error processing sample 65: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:17:43,606 - energy_eval - ERROR - Error processing sample 66: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:17:54,188 - energy_eval - ERROR - Error processing sample 67: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:18:04,815 - energy_eval - ERROR - Error processing sample 68: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:18:15,410 - energy_eval - ERROR - Error processing sample 69: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:18:25,995 - energy_eval - ERROR - Error processing sample 70: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:18:36,605 - energy_eval - ERROR - Error processing sample 71: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:18:47,398 - energy_eval - ERROR - Error processing sample 72: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:18:57,978 - energy_eval - ERROR - Error processing sample 73: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:19:08,553 - energy_eval - ERROR - Error processing sample 74: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:19:19,174 - energy_eval - ERROR - Error processing sample 75: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:19:29,752 - energy_eval - ERROR - Error processing sample 76: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:19:40,327 - energy_eval - ERROR - Error processing sample 77: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:19:50,907 - energy_eval - ERROR - Error processing sample 78: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:20:01,488 - energy_eval - ERROR - Error processing sample 79: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:20:12,062 - energy_eval - ERROR - Error processing sample 80: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:20:22,657 - energy_eval - ERROR - Error processing sample 81: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:20:33,272 - energy_eval - ERROR - Error processing sample 82: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:20:44,020 - energy_eval - ERROR - Error processing sample 83: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:20:54,594 - energy_eval - ERROR - Error processing sample 84: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:21:05,215 - energy_eval - ERROR - Error processing sample 85: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:21:15,790 - energy_eval - ERROR - Error processing sample 86: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:21:26,392 - energy_eval - ERROR - Error processing sample 87: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:21:36,970 - energy_eval - ERROR - Error processing sample 88: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:21:47,598 - energy_eval - ERROR - Error processing sample 89: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:21:58,178 - energy_eval - ERROR - Error processing sample 90: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:22:08,763 - energy_eval - ERROR - Error processing sample 91: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:22:19,324 - energy_eval - ERROR - Error processing sample 92: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:22:29,965 - energy_eval - ERROR - Error processing sample 93: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:22:40,546 - energy_eval - ERROR - Error processing sample 94: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:22:51,308 - energy_eval - ERROR - Error processing sample 95: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:23:01,900 - energy_eval - ERROR - Error processing sample 96: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:23:12,516 - energy_eval - ERROR - Error processing sample 97: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:23:23,104 - energy_eval - ERROR - Error processing sample 98: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:23:33,703 - energy_eval - ERROR - Error processing sample 99: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:23:44,298 - energy_eval - ERROR - Error processing sample 100: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:23:54,863 - energy_eval - ERROR - Error processing sample 101: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:24:05,458 - energy_eval - ERROR - Error processing sample 102: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:24:16,087 - energy_eval - ERROR - Error processing sample 103: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:24:26,680 - energy_eval - ERROR - Error processing sample 104: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:24:37,275 - energy_eval - ERROR - Error processing sample 105: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:24:47,853 - energy_eval - ERROR - Error processing sample 106: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:24:58,497 - energy_eval - ERROR - Error processing sample 107: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:25:09,315 - energy_eval - ERROR - Error processing sample 108: model requires more system memory (10.5 GiB) than is available (9.8 GiB) (status code: 500)
2025-06-28 10:25:19,870 - energy_eval - ERROR - Error processing sample 109: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:25:30,449 - energy_eval - ERROR - Error processing sample 110: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:25:41,017 - energy_eval - ERROR - Error processing sample 111: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:25:51,592 - energy_eval - ERROR - Error processing sample 112: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:26:02,191 - energy_eval - ERROR - Error processing sample 113: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:26:13,097 - energy_eval - ERROR - Error processing sample 114: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:26:23,802 - energy_eval - ERROR - Error processing sample 115: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:26:34,395 - energy_eval - ERROR - Error processing sample 116: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:26:45,037 - energy_eval - ERROR - Error processing sample 117: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:26:55,610 - energy_eval - ERROR - Error processing sample 118: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:27:06,177 - energy_eval - ERROR - Error processing sample 119: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:27:16,760 - energy_eval - ERROR - Error processing sample 120: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:27:27,340 - energy_eval - ERROR - Error processing sample 121: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:27:37,948 - energy_eval - ERROR - Error processing sample 122: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:27:48,539 - energy_eval - ERROR - Error processing sample 123: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:27:59,122 - energy_eval - ERROR - Error processing sample 124: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:28:09,704 - energy_eval - ERROR - Error processing sample 125: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:28:20,331 - energy_eval - ERROR - Error processing sample 126: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:28:30,905 - energy_eval - ERROR - Error processing sample 127: model requires more system memory (10.5 GiB) than is available (9.9 GiB) (status code: 500)
2025-06-28 10:28:30,905 - energy_eval - INFO - Completed q+r mode for deepseek-r1:14b
2025-06-28 10:28:36,626 - energy_eval - INFO - Completed deepseek-r1:14b in 5h 2m 13s
2025-06-28 10:28:36,626 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-06-28 10:28:36,626 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-06-28 14:30:44,857 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-28 14:30:44,857 - energy_eval - INFO - Using device: cpu
2025-06-28 14:30:44,857 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 14:30:44,873 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 14:30:44,873 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:7b
============================================================
2025-06-28 14:30:44,873 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 14:31:55,119 - energy_eval - WARNING - Experiment interrupted by user
2025-06-28 14:32:22,093 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-28 14:32:22,093 - energy_eval - INFO - Using device: cpu
2025-06-28 14:32:22,093 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 14:32:22,093 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 14:32:22,093 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:7b
============================================================
2025-06-28 14:32:22,093 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {}
++++++++++++++++++++++++++++++
2025-06-28 14:32:22,157 - energy_eval - INFO - Completed deepseek-r1:7b in 0h 0m 0s
2025-06-28 14:32:22,157 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-28 14:32:22,157 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 14:32:22,157 - energy_eval - INFO - Completed q mode for deepseek-r1:8b
2025-06-28 14:34:16,264 - energy_eval - WARNING - Experiment interrupted by user
2025-06-28 14:34:40,003 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-28 14:34:40,003 - energy_eval - INFO - Using device: cpu
2025-06-28 14:34:40,003 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 14:34:40,003 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 14:34:40,003 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:7b
============================================================
2025-06-28 14:34:40,003 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {}
++++++++++++++++++++++++++++++
2025-06-28 14:34:40,084 - energy_eval - INFO - Completed deepseek-r1:7b in 0h 0m 0s
2025-06-28 14:34:40,084 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-28 14:34:40,084 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 15:10:35,453 - energy_eval - WARNING - Experiment interrupted by user
2025-06-28 15:12:11,351 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:8b': {'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-28 15:12:11,351 - energy_eval - INFO - Using device: cpu
2025-06-28 15:12:11,351 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 15:12:11,351 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 15:12:11,351 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-28 15:12:11,351 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 15:12:48,170 - energy_eval - WARNING - Experiment interrupted by user
2025-06-28 15:12:58,511 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:8b': {'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-28 15:12:58,511 - energy_eval - INFO - Using device: cpu
2025-06-28 15:12:58,511 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 15:12:58,526 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 15:12:58,526 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-28 15:12:58,526 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 15:16:38,062 - energy_eval - INFO - Loaded Wikipedia corpus and indexes
2025-06-28 15:18:01,652 - energy_eval - WARNING - Experiment interrupted by user
2025-06-28 15:30:09,956 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:8b': {'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-28 15:30:09,957 - energy_eval - INFO - Using device: cpu
2025-06-28 15:30:09,957 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 15:30:09,964 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 15:30:09,964 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-28 15:30:09,965 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 15:34:11,075 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 4, 1)
2025-06-28 16:35:22,735 - energy_eval - WARNING - Experiment interrupted by user
2025-06-28 16:36:30,234 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:8b': {'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-28 16:36:30,235 - energy_eval - INFO - Using device: cpu
2025-06-28 16:36:30,235 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 16:36:30,238 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 16:36:30,238 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-28 16:36:30,239 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 16:40:19,226 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 3, 48)
2025-06-28 17:09:00,142 - energy_eval - WARNING - Experiment interrupted by user
2025-06-28 17:39:19,549 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:8b': {'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read the following passage carefully and answer the question with only one word. It must be 'True' or 'False'.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer the following question with only one word. It must be 'True' or 'False'.\nQuestion: {question}\nAnswer:"}})
2025-06-28 17:39:19,549 - energy_eval - INFO - Using device: cpu
2025-06-28 17:39:19,549 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 17:39:19,556 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 17:39:19,556 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-28 17:39:19,557 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 17:43:26,918 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 4, 7)
2025-06-28 20:26:50,772 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:8b': {'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-06-28 20:26:50,773 - energy_eval - INFO - Using device: cpu
2025-06-28 20:26:50,773 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 20:26:50,777 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 20:26:50,777 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-28 20:26:50,777 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 20:30:48,193 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 3, 57)
2025-06-28 20:31:25,887 - energy_eval - WARNING - Experiment interrupted by user
2025-06-28 20:42:14,444 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:8b': {'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-06-28 20:42:14,444 - energy_eval - INFO - Using device: cpu
2025-06-28 20:42:14,444 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 20:42:14,448 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 20:42:14,448 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-28 20:42:14,448 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 20:46:10,485 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 3, 56)
2025-06-28 21:04:11,179 - energy_eval - INFO - Completed q+r mode for deepseek-r1:8b
2025-06-28 21:04:17,828 - energy_eval - INFO - Completed deepseek-r1:8b in 0h 22m 3s
2025-06-28 21:04:17,828 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 22m 3s
============================================================
2025-06-28 21:23:36,983 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-06-28 21:23:36,983 - energy_eval - INFO - Using device: cpu
2025-06-28 21:23:36,983 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 21:23:36,987 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 21:23:36,988 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:7b
============================================================
2025-06-28 21:23:36,988 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 21:32:18,709 - energy_eval - INFO - Completed q mode for deepseek-r1:7b
2025-06-28 21:36:33,118 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 4, 14)
2025-06-28 21:55:34,659 - energy_eval - INFO - Completed q+r mode for deepseek-r1:7b
2025-06-28 21:55:41,389 - energy_eval - INFO - Completed deepseek-r1:7b in 0h 32m 4s
2025-06-28 21:55:41,390 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-28 21:55:41,390 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 21:58:28,799 - energy_eval - WARNING - Experiment interrupted by user
2025-06-28 23:47:01,290 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-06-28 23:47:01,290 - energy_eval - INFO - Using device: cpu
2025-06-28 23:47:01,290 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-06-28 23:47:01,294 - energy_eval - INFO - Loaded dataset with 128 samples
2025-06-28 23:47:01,294 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:7b
============================================================
2025-06-28 23:47:01,294 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 23:47:01,296 - energy_eval - INFO - Completed q mode for deepseek-r1:7b
2025-06-28 23:50:41,009 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 3, 39)
2025-06-28 23:50:41,011 - energy_eval - INFO - Completed q+r mode for deepseek-r1:7b
2025-06-28 23:50:45,040 - energy_eval - INFO - Completed deepseek-r1:7b in 0h 3m 43s
2025-06-28 23:50:45,040 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-28 23:50:45,040 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-28 23:57:07,414 - energy_eval - INFO - Completed q mode for deepseek-r1:8b
2025-06-29 00:00:47,559 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 3, 40)
2025-06-29 00:17:18,729 - energy_eval - INFO - Completed q+r mode for deepseek-r1:8b
2025-06-29 00:17:22,936 - energy_eval - INFO - Completed deepseek-r1:8b in 0h 26m 37s
2025-06-29 00:17:22,936 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-06-29 00:17:22,936 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-29 00:17:22,947 - energy_eval - INFO - Completed q mode for deepseek-r1:14b
2025-06-29 00:21:06,078 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 3, 43)
2025-06-29 00:43:03,629 - energy_eval - INFO - Completed q+r mode for deepseek-r1:14b
2025-06-29 00:43:15,936 - energy_eval - INFO - Completed deepseek-r1:14b in 0h 25m 52s
2025-06-29 00:43:15,936 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-06-29 00:43:15,936 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-06-29 00:50:31,536 - energy_eval - INFO - Completed q mode for deepseek-r1:32b
2025-06-29 00:50:31,623 - energy_eval - INFO - Completed deepseek-r1:32b in 0h 7m 15s
2025-06-29 00:50:31,623 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 3m 30s
============================================================
2025-06-29 09:26:33,327 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.com', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-06-29 09:26:33,327 - energy_eval - INFO - Using device: cpu
2025-06-29 09:26:35,059 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-06-29 09:26:35,059 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:7b
============================================================
2025-06-29 09:26:35,059 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-29 12:42:46,197 - energy_eval - INFO - Completed q mode for deepseek-r1:7b
2025-06-29 12:46:37,931 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 3, 51)
2025-06-29 19:42:26,162 - energy_eval - INFO - Completed q+r mode for deepseek-r1:7b
2025-06-29 19:42:38,631 - energy_eval - INFO - Completed deepseek-r1:7b in 10h 16m 3s
2025-06-29 19:42:38,631 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-06-29 19:42:38,631 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-29 23:52:42,674 - energy_eval - INFO - Completed q mode for deepseek-r1:8b
2025-06-29 23:56:25,633 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 3, 42)
2025-06-30 07:28:13,044 - energy_eval - INFO - Completed q+r mode for deepseek-r1:8b
2025-06-30 07:28:25,725 - energy_eval - INFO - Completed deepseek-r1:8b in 11h 45m 47s
2025-06-30 07:28:25,726 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-06-30 07:28:25,726 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-06-30 12:04:34,348 - energy_eval - INFO - Completed q mode for deepseek-r1:14b
2025-06-30 12:08:17,705 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 3, 43)
2025-06-30 22:32:38,073 - energy_eval - INFO - Completed q+r mode for deepseek-r1:14b
2025-06-30 22:32:51,072 - energy_eval - INFO - Completed deepseek-r1:14b in 15h 4m 25s
2025-06-30 22:32:51,072 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-06-30 22:32:51,072 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-01 04:33:46,985 - energy_eval - INFO - Completed q mode for deepseek-r1:32b
2025-07-01 04:33:47,436 - energy_eval - INFO - Completed deepseek-r1:32b in 6h 0m 56s
2025-07-01 04:33:47,436 - energy_eval - INFO - 
============================================================
Experiment completed in 43h 7m 14s
============================================================
2025-07-01 10:37:57,215 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-01 10:37:57,215 - energy_eval - INFO - Using device: cpu
2025-07-01 10:37:58,001 - energy_eval - INFO - Loaded dataset with 7405 samples
2025-07-01 10:37:58,001 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:7b
============================================================
2025-07-01 10:37:58,001 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-01 18:20:05,555 - energy_eval - INFO - Completed q mode for deepseek-r1:7b
2025-07-01 18:24:05,392 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in (0, 3, 59)
2025-07-02 08:32:50,387 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-02 08:32:50,388 - energy_eval - INFO - Using device: cpu
2025-07-02 08:32:51,112 - energy_eval - INFO - Loaded dataset with 7405 samples
2025-07-02 08:32:51,112 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:7b
============================================================
2025-07-02 08:32:51,112 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-02 08:32:51,125 - energy_eval - INFO - Completed q mode for deepseek-r1:7b in 0h 0m 0s
2025-07-02 08:36:32,445 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0h 3m 41s
2025-07-02 08:48:51,055 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:1b': 'ollama', 'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:272b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'gemma3:1b': {'q': False, 'q+r': True}, 'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-02 08:48:51,056 - energy_eval - INFO - Using device: cpu
2025-07-02 08:48:52,588 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-02 08:48:52,589 - energy_eval - INFO - 
============================================================
Running model: gemma3:1b
============================================================
2025-07-02 08:48:52,589 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-02 13:22:34,900 - energy_eval - INFO - Completed q mode for gemma3:1b in 4h 33m 42s
2025-07-02 13:26:37,786 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0h 4m 2s
2025-07-02 19:53:03,111 - energy_eval - INFO - Completed q+r mode for gemma3:1b in 6h 26m 25s
2025-07-02 19:53:33,124 - energy_eval - INFO - Completed gemma3:1b in 11h 4m 40s
2025-07-02 19:53:33,124 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-02 19:53:33,124 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-03 01:09:48,209 - energy_eval - INFO - Completed q mode for gemma3:4b in 5h 16m 15s
2025-07-03 01:13:32,556 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0h 3m 44s
2025-07-03 09:08:38,572 - energy_eval - INFO - Completed q+r mode for gemma3:4b in 7h 55m 6s
2025-07-03 09:08:53,813 - energy_eval - INFO - Completed gemma3:4b in 13h 15m 20s
2025-07-03 09:08:53,813 - energy_eval - INFO - 
============================================================
Running model: gemma3:12b
============================================================
2025-07-03 09:08:53,813 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-03 15:27:30,365 - energy_eval - INFO - Completed q mode for gemma3:12b in 6h 18m 36s
2025-07-03 15:31:29,506 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0h 3m 59s
2025-07-03 15:32:06,925 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:1b': 'ollama', 'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'gemma3:1b': {'q': False, 'q+r': True}, 'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-03 15:32:06,926 - energy_eval - INFO - Using device: cpu
2025-07-03 15:32:08,475 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-03 15:32:08,475 - energy_eval - INFO - 
============================================================
Running model: gemma3:1b
============================================================
2025-07-03 15:32:08,475 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-03 15:32:08,485 - energy_eval - INFO - Completed q mode for gemma3:1b in 0:00:00
2025-07-03 15:35:50,089 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:41
2025-07-03 15:35:50,094 - energy_eval - INFO - Completed q+r mode for gemma3:1b in 0:00:00
2025-07-03 15:35:54,648 - energy_eval - INFO - Completed gemma3:1b in 0h 3m 46s
2025-07-03 15:35:54,648 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-03 15:35:54,648 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-03 15:35:54,653 - energy_eval - INFO - Completed q mode for gemma3:4b in 0:00:00
2025-07-03 15:39:41,734 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:47
2025-07-03 15:39:41,743 - energy_eval - INFO - Completed q+r mode for gemma3:4b in 0:00:00
2025-07-03 15:39:45,992 - energy_eval - INFO - Completed gemma3:4b in 0h 3m 51s
2025-07-03 15:39:45,992 - energy_eval - INFO - 
============================================================
Running model: gemma3:12b
============================================================
2025-07-03 15:39:45,992 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-03 15:39:45,997 - energy_eval - INFO - Completed q mode for gemma3:12b in 0:00:00
2025-07-03 15:43:30,584 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:44
2025-07-04 02:46:46,881 - energy_eval - INFO - Completed q+r mode for gemma3:12b in 11:03:16
2025-07-04 02:46:59,640 - energy_eval - INFO - Completed gemma3:12b in 11h 7m 13s
2025-07-04 02:46:59,640 - energy_eval - INFO - 
============================================================
Running model: gemma3:27b
============================================================
2025-07-04 02:46:59,640 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-04 10:09:00,267 - energy_eval - INFO - Completed q mode for gemma3:27b in 7:22:00
2025-07-04 10:09:00,492 - energy_eval - INFO - Completed gemma3:27b in 7h 22m 0s
2025-07-04 10:09:00,492 - energy_eval - INFO - 
============================================================
Experiment completed in 18h 36m 53s
============================================================
2025-07-04 10:59:32,207 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:1b': 'ollama', 'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'gemma3:1b': {'q': False, 'q+r': True}, 'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-04 10:59:32,207 - energy_eval - INFO - Using device: cpu
2025-07-04 10:59:32,207 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-07-04 10:59:32,219 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-04 10:59:32,219 - energy_eval - INFO - 
============================================================
Running model: gemma3:1b
============================================================
2025-07-04 10:59:32,219 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-04 11:14:05,749 - energy_eval - INFO - Completed q mode for gemma3:1b in 0:14:33
2025-07-04 11:18:09,031 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:04:03
2025-07-04 11:36:24,763 - energy_eval - INFO - Completed q+r mode for gemma3:1b in 0:18:15
2025-07-04 11:36:39,417 - energy_eval - INFO - Completed gemma3:1b in 0h 37m 7s
2025-07-04 11:36:39,417 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-04 11:36:39,417 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-04 11:51:39,179 - energy_eval - INFO - Completed q mode for gemma3:4b in 0:14:59
2025-07-04 11:55:33,871 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:54
2025-07-04 12:16:43,937 - energy_eval - INFO - Completed q+r mode for gemma3:4b in 0:21:10
2025-07-04 12:16:50,343 - energy_eval - INFO - Completed gemma3:4b in 0h 40m 10s
2025-07-04 12:16:50,343 - energy_eval - INFO - 
============================================================
Running model: gemma3:12b
============================================================
2025-07-04 12:16:50,343 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-04 12:33:03,459 - energy_eval - INFO - Completed q mode for gemma3:12b in 0:16:13
2025-07-04 12:37:01,346 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:57
2025-07-04 13:05:57,146 - energy_eval - INFO - Completed q+r mode for gemma3:12b in 0:28:55
2025-07-04 13:06:03,893 - energy_eval - INFO - Completed gemma3:12b in 0h 49m 13s
2025-07-04 13:06:03,893 - energy_eval - INFO - 
============================================================
Running model: gemma3:27b
============================================================
2025-07-04 13:06:03,893 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-04 13:24:43,221 - energy_eval - INFO - Completed q mode for gemma3:27b in 0:18:39
2025-07-04 13:24:43,370 - energy_eval - INFO - Completed gemma3:27b in 0h 18m 39s
2025-07-04 13:24:43,370 - energy_eval - INFO - 
============================================================
Experiment completed in 2h 25m 11s
============================================================
2025-07-04 15:32:01,872 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:1b': 'ollama', 'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'gemma3:1b': {'q': False, 'q+r': True}, 'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-04 15:32:01,872 - energy_eval - INFO - Using device: cpu
2025-07-04 15:32:02,800 - energy_eval - INFO - Loaded dataset with 7405 samples
2025-07-04 15:32:02,800 - energy_eval - INFO - 
============================================================
Running model: gemma3:1b
============================================================
2025-07-04 15:32:02,800 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-04 15:33:04,870 - energy_eval - WARNING - Experiment interrupted by user
2025-07-04 15:33:54,493 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:1b': 'ollama', 'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'gemma3:1b': {'q': False, 'q+r': True}, 'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-04 15:33:54,493 - energy_eval - INFO - Using device: cpu
2025-07-04 15:33:55,191 - energy_eval - INFO - Loaded dataset with 7405 samples
2025-07-04 15:33:55,191 - energy_eval - INFO - 
============================================================
Running model: gemma3:1b
============================================================
2025-07-04 15:33:55,191 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-04 22:52:34,076 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:1b': 'ollama', 'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cpu', modes={'gemma3:1b': {'q': False, 'q+r': True}, 'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-04 22:52:34,076 - energy_eval - INFO - Using device: cpu
2025-07-04 22:52:34,076 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_128.jsonl
2025-07-04 22:52:34,096 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-04 22:52:34,096 - energy_eval - INFO - 
============================================================
Running model: gemma3:1b
============================================================
2025-07-04 22:52:34,096 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-04 23:05:53,033 - energy_eval - INFO - Completed q mode for gemma3:1b in 0:13:18
2025-07-04 23:09:31,754 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:38
2025-07-04 23:32:33,142 - energy_eval - INFO - Completed q+r mode for gemma3:1b in 0:23:01
2025-07-04 23:32:36,937 - energy_eval - INFO - Completed gemma3:1b in 0h 40m 2s
2025-07-04 23:32:36,938 - energy_eval - INFO - 
============================================================
Running model: gemma3:4b
============================================================
2025-07-04 23:32:36,938 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-04 23:45:54,759 - energy_eval - INFO - Completed q mode for gemma3:4b in 0:13:17
2025-07-04 23:49:31,692 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:36
2025-07-05 00:12:47,186 - energy_eval - INFO - Completed q+r mode for gemma3:4b in 0:23:15
2025-07-05 00:12:51,301 - energy_eval - INFO - Completed gemma3:4b in 0h 40m 14s
2025-07-05 00:12:51,301 - energy_eval - INFO - 
============================================================
Running model: gemma3:12b
============================================================
2025-07-05 00:12:51,301 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-05 00:26:31,528 - energy_eval - INFO - Completed q mode for gemma3:12b in 0:13:40
2025-07-05 00:30:09,141 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:37
2025-07-05 00:53:45,805 - energy_eval - INFO - Completed q+r mode for gemma3:12b in 0:23:36
2025-07-05 00:53:50,086 - energy_eval - INFO - Completed gemma3:12b in 0h 40m 58s
2025-07-05 00:53:50,086 - energy_eval - INFO - 
============================================================
Running model: gemma3:27b
============================================================
2025-07-05 00:53:50,086 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-05 01:07:40,756 - energy_eval - INFO - Completed q mode for gemma3:27b in 0:13:50
2025-07-05 01:07:40,851 - energy_eval - INFO - Completed gemma3:27b in 0h 13m 50s
2025-07-05 01:07:40,851 - energy_eval - INFO - 
============================================================
Experiment completed in 2h 15m 6s
============================================================
2025-07-05 08:01:51,116 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama', 'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False, 'q+r': True}, 'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-05 08:01:51,116 - energy_eval - INFO - Using device: cuda
2025-07-05 08:01:51,116 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_128.jsonl
2025-07-05 08:01:51,135 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-05 08:01:51,135 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-05 08:01:51,135 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-05 08:10:44,561 - energy_eval - WARNING - Experiment interrupted by user
2025-07-05 08:10:50,333 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama', 'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False, 'q+r': True}, 'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-05 08:10:50,333 - energy_eval - INFO - Using device: cuda
2025-07-05 08:10:50,333 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_128.jsonl
2025-07-05 08:10:50,346 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-05 08:10:50,346 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-05 08:10:50,346 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-05 08:15:18,624 - energy_eval - INFO - Completed q mode for deepseek-r1:1.5b in 0:04:28
2025-07-05 08:20:17,640 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:04:59
2025-07-05 08:22:09,384 - energy_eval - ERROR - Error processing sample 10: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:22:19,017 - energy_eval - ERROR - Error processing sample 11: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:22:28,825 - energy_eval - ERROR - Error processing sample 12: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:22:38,595 - energy_eval - ERROR - Error processing sample 13: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:22:48,439 - energy_eval - ERROR - Error processing sample 14: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:22:58,176 - energy_eval - ERROR - Error processing sample 15: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:23:07,974 - energy_eval - ERROR - Error processing sample 16: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:23:17,719 - energy_eval - ERROR - Error processing sample 17: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:23:27,586 - energy_eval - ERROR - Error processing sample 18: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:23:37,375 - energy_eval - ERROR - Error processing sample 19: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:23:47,235 - energy_eval - ERROR - Error processing sample 20: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:23:56,934 - energy_eval - ERROR - Error processing sample 21: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:24:06,760 - energy_eval - ERROR - Error processing sample 22: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:24:16,453 - energy_eval - ERROR - Error processing sample 23: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:24:26,112 - energy_eval - ERROR - Error processing sample 24: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-05 08:24:41,510 - energy_eval - WARNING - Experiment interrupted by user
2025-07-05 08:25:00,317 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama', 'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False, 'q+r': True}, 'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-05 08:25:00,317 - energy_eval - INFO - Using device: cuda
2025-07-05 08:25:00,317 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_128.jsonl
2025-07-05 08:25:00,331 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-05 08:25:00,331 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-05 08:25:00,331 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-05 08:25:00,333 - energy_eval - INFO - Completed q mode for deepseek-r1:1.5b in 0:00:00
2025-07-05 08:28:42,812 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:42
2025-07-05 08:49:06,891 - energy_eval - INFO - Completed q+r mode for deepseek-r1:1.5b in 0:20:24
2025-07-05 08:49:12,136 - energy_eval - INFO - Completed deepseek-r1:1.5b in 0h 24m 11s
2025-07-05 08:49:12,137 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:7b
============================================================
2025-07-05 08:49:12,137 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-05 09:01:14,694 - energy_eval - INFO - Completed q mode for deepseek-r1:7b in 0:12:02
2025-07-05 09:05:01,448 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:46
2025-07-05 09:27:03,564 - energy_eval - INFO - Completed q+r mode for deepseek-r1:7b in 0:22:02
2025-07-05 09:27:07,712 - energy_eval - INFO - Completed deepseek-r1:7b in 0h 37m 55s
2025-07-05 09:27:07,712 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-05 09:27:07,712 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-05 09:39:07,055 - energy_eval - INFO - Completed q mode for deepseek-r1:8b in 0:11:59
2025-07-05 09:41:10,111 - energy_eval - WARNING - Experiment interrupted by user
2025-07-05 09:41:30,152 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama', 'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False, 'q+r': True}, 'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-05 09:41:30,153 - energy_eval - INFO - Using device: cuda
2025-07-05 09:41:30,153 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_128.jsonl
2025-07-05 09:41:30,171 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-05 09:41:30,171 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-05 09:41:30,171 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-05 09:41:30,179 - energy_eval - INFO - Completed q mode for deepseek-r1:1.5b in 0:00:00
2025-07-05 09:45:10,718 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:40
2025-07-05 09:45:10,720 - energy_eval - INFO - Completed q+r mode for deepseek-r1:1.5b in 0:00:00
2025-07-05 09:45:14,775 - energy_eval - INFO - Completed deepseek-r1:1.5b in 0h 3m 44s
2025-07-05 09:45:14,775 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:7b
============================================================
2025-07-05 09:45:14,775 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-05 09:45:14,777 - energy_eval - INFO - Completed q mode for deepseek-r1:7b in 0:00:00
2025-07-05 09:48:54,057 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:39
2025-07-05 09:48:54,059 - energy_eval - INFO - Completed q+r mode for deepseek-r1:7b in 0:00:00
2025-07-05 09:48:58,158 - energy_eval - INFO - Completed deepseek-r1:7b in 0h 3m 43s
2025-07-05 09:48:58,158 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-05 09:48:58,158 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-05 09:48:58,160 - energy_eval - INFO - Completed q mode for deepseek-r1:8b in 0:00:00
2025-07-05 09:52:37,221 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:39
2025-07-05 10:14:55,809 - energy_eval - INFO - Completed q+r mode for deepseek-r1:8b in 0:22:18
2025-07-05 10:15:00,085 - energy_eval - INFO - Completed deepseek-r1:8b in 0h 26m 1s
2025-07-05 10:15:00,085 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-07-05 10:15:00,085 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-05 10:27:26,294 - energy_eval - INFO - Completed q mode for deepseek-r1:14b in 0:12:26
2025-07-05 10:31:07,955 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:41
2025-07-05 10:53:46,269 - energy_eval - INFO - Completed q+r mode for deepseek-r1:14b in 0:22:38
2025-07-05 10:53:50,388 - energy_eval - INFO - Completed deepseek-r1:14b in 0h 38m 50s
2025-07-05 10:53:50,388 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-05 10:53:50,388 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-05 11:06:24,188 - energy_eval - INFO - Completed q mode for deepseek-r1:32b in 0:12:33
2025-07-05 11:06:24,264 - energy_eval - INFO - Completed deepseek-r1:32b in 0h 12m 33s
2025-07-05 11:06:24,264 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 24m 54s
============================================================
2025-07-06 19:28:02,309 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama', 'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False, 'q+r': True}, 'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-06 19:28:02,309 - energy_eval - INFO - Using device: cuda
2025-07-06 19:28:03,233 - energy_eval - INFO - Loaded dataset with 7405 samples
2025-07-06 19:28:03,233 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-06 19:28:03,233 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-06 19:29:51,528 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:7b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:7b': {'q': False, 'q+r': True}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-06 19:29:51,528 - energy_eval - INFO - Using device: cuda
2025-07-06 19:29:52,419 - energy_eval - INFO - Loaded dataset with 7405 samples
2025-07-06 19:29:52,419 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:7b
============================================================
2025-07-06 19:29:52,420 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-06 19:29:52,441 - energy_eval - INFO - Completed q mode for deepseek-r1:7b in 0:00:00
2025-07-06 19:31:13,207 - energy_eval - WARNING - Experiment interrupted by user
2025-07-06 19:31:32,772 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:1b': 'ollama', 'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='hotpot_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:1b': {'q': False, 'q+r': True}, 'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-06 19:31:32,772 - energy_eval - INFO - Using device: cuda
2025-07-06 19:31:32,772 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\hotpot_mini_128.jsonl
2025-07-06 19:31:32,798 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-06 19:31:32,798 - energy_eval - INFO - 
============================================================
Running model: gemma3:1b
============================================================
2025-07-06 19:31:32,798 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-06 19:31:32,807 - energy_eval - INFO - Completed q mode for gemma3:1b in 0:00:00
2025-07-06 19:32:59,247 - energy_eval - WARNING - Experiment interrupted by user
2025-07-06 19:33:05,579 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'gemma3:1b': 'ollama', 'gemma3:4b': 'ollama', 'gemma3:12b': 'ollama', 'gemma3:27b': 'ollama'}, dataset_name='hotpotqa/hotpot_qa', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'gemma3:1b': {'q': False, 'q+r': True}, 'gemma3:4b': {'q': False, 'q+r': True}, 'gemma3:12b': {'q': False, 'q+r': True}, 'gemma3:27b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-06 19:33:05,579 - energy_eval - INFO - Using device: cuda
2025-07-06 19:33:06,273 - energy_eval - INFO - Loaded dataset with 7405 samples
2025-07-06 19:33:06,273 - energy_eval - INFO - 
============================================================
Running model: gemma3:1b
============================================================
2025-07-06 19:33:06,273 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-07 07:28:32,448 - energy_eval - INFO - Completed q mode for gemma3:1b in 11:55:26
2025-07-07 07:32:21,634 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:49
2025-07-07 08:33:36,503 - energy_eval - WARNING - Experiment interrupted by user
2025-07-07 14:14:58,129 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:32b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:32b': {'q': False}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-07 14:14:58,129 - energy_eval - INFO - Using device: cuda
2025-07-07 14:14:59,723 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-07 14:14:59,723 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-07 14:14:59,723 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-07 14:15:56,911 - energy_eval - ERROR - Error processing sample 4: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-07 14:17:16,639 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:32b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:32b': {'q': False}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-07 14:17:16,639 - energy_eval - INFO - Using device: cuda
2025-07-07 14:17:18,731 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-07 14:17:18,731 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-07 14:17:18,731 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-07 14:22:15,688 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:32b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:32b': {'q': False}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-07 14:22:15,688 - energy_eval - INFO - Using device: cuda
2025-07-07 14:22:17,074 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-07 14:22:17,074 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-07 14:22:17,074 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-08 00:59:26,238 - energy_eval - INFO - Completed q mode for deepseek-r1:32b in 10:37:09
2025-07-08 00:59:26,363 - energy_eval - INFO - Completed deepseek-r1:32b in 10h 37m 9s
2025-07-08 00:59:26,363 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-08 00:59:26,363 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-08 11:43:10,638 - energy_eval - INFO - Completed q mode for deepseek-r1:8b in 10:43:44
2025-07-08 11:47:16,390 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:04:05
2025-07-08 20:40:02,792 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:32b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:32b': {'q': False}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-08 20:40:02,792 - energy_eval - INFO - Using device: cuda
2025-07-08 20:40:02,807 - datasets.load - WARNING - Using the latest cached version of the dataset since google/boolq couldn't be found on the Hugging Face Hub
2025-07-08 20:40:02,824 - datasets.packaged_modules.cache.cache - WARNING - Found the latest cached dataset configuration 'default' at C:\Users\Ethan\.cache\huggingface\datasets\google___boolq\default\0.0.0\35b264d03638db9f4ce671b711558bf7ff0f80d5 (last modified on Tue Jun  3 13:36:06 2025).
2025-07-08 20:40:02,925 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-08 20:40:02,925 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-08 20:40:02,925 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-08 20:40:02,934 - energy_eval - INFO - Completed q mode for deepseek-r1:32b in 0:00:00
2025-07-08 20:40:03,013 - energy_eval - INFO - Completed deepseek-r1:32b in 0h 0m 0s
2025-07-08 20:40:03,013 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-08 20:40:03,013 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-08 20:40:03,028 - energy_eval - INFO - Completed q mode for deepseek-r1:8b in 0:00:00
2025-07-08 20:40:11,325 - energy_eval - WARNING - Experiment interrupted by user
2025-07-08 20:41:17,381 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:32b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:32b': {'q': False}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-08 20:41:17,381 - energy_eval - INFO - Using device: cuda
2025-07-08 20:41:17,396 - datasets.load - WARNING - Using the latest cached version of the dataset since google/boolq couldn't be found on the Hugging Face Hub
2025-07-08 20:41:17,396 - datasets.packaged_modules.cache.cache - WARNING - Found the latest cached dataset configuration 'default' at C:\Users\Ethan\.cache\huggingface\datasets\google___boolq\default\0.0.0\35b264d03638db9f4ce671b711558bf7ff0f80d5 (last modified on Tue Jun  3 13:36:06 2025).
2025-07-08 20:41:17,411 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-08 20:41:17,411 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-08 20:41:17,411 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-08 20:41:17,411 - energy_eval - INFO - Completed q mode for deepseek-r1:32b in 0:00:00
2025-07-08 20:41:17,506 - energy_eval - INFO - Completed deepseek-r1:32b in 0h 0m 0s
2025-07-08 20:41:17,506 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-08 20:41:17,506 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-08 20:41:17,506 - energy_eval - INFO - Completed q mode for deepseek-r1:8b in 0:00:00
2025-07-08 20:41:46,400 - energy_eval - WARNING - Experiment interrupted by user
2025-07-08 20:49:25,059 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:32b': 'ollama', 'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:32b': {'q': False}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-08 20:49:25,059 - energy_eval - INFO - Using device: cuda
2025-07-08 20:49:27,268 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-08 20:49:27,268 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-08 20:49:27,268 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-08 20:49:27,268 - energy_eval - INFO - Completed q mode for deepseek-r1:32b in 0:00:00
2025-07-08 20:49:27,347 - energy_eval - INFO - Completed deepseek-r1:32b in 0h 0m 0s
2025-07-08 20:49:27,347 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-08 20:49:27,347 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-08 20:49:27,362 - energy_eval - INFO - Completed q mode for deepseek-r1:8b in 0:00:00
2025-07-08 20:53:05,686 - energy_eval - INFO - Loaded Wikipedia corpus and indexes in 0:03:38
2025-07-08 20:57:44,940 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 08:43:54,385 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 08:43:54,386 - energy_eval - INFO - Using device: cuda
2025-07-10 08:43:54,386 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 08:43:54,390 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 08:43:54,390 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 08:43:54,390 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-10 08:44:09,156 - energy_eval - INFO - [deepseek-r1:8b | q | QID 0] Prediction: <think>
Okay, user is asking if the World | Gold: True
2025-07-10 08:44:18,600 - energy_eval - INFO - [deepseek-r1:8b | q | QID 1] Prediction: <think>
Okay, user wants a yes/no answer | Gold: True
2025-07-10 08:44:27,498 - energy_eval - INFO - [deepseek-r1:8b | q | QID 2] Prediction: <think>
Okay, the question is "does Air | Gold: False
2025-07-10 08:44:36,349 - energy_eval - INFO - [deepseek-r1:8b | q | QID 3] Prediction: <think>
Okay, the user is asking if there | Gold: True
2025-07-10 08:44:45,394 - energy_eval - INFO - [deepseek-r1:8b | q | QID 4] Prediction: <think>
Hmm, the user wants me to answer | Gold: True
2025-07-10 08:44:54,465 - energy_eval - INFO - [deepseek-r1:8b | q | QID 5] Prediction: <think>
Okay, the user is asking whether Hope | Gold: True
2025-07-10 08:44:58,677 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 08:45:03,427 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=None, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 08:45:03,427 - energy_eval - INFO - Using device: cuda
2025-07-10 08:45:03,427 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 08:45:03,431 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 08:45:03,431 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 08:45:03,431 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-10 08:45:07,555 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 08:46:37,179 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=None, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 08:46:37,179 - energy_eval - INFO - Using device: cuda
2025-07-10 08:46:37,179 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 08:46:37,183 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 08:46:37,183 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 08:46:37,183 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-10 08:46:46,814 - energy_eval - INFO - [deepseek-r1:8b | q | QID 0] Prediction: <think>
Okay, user is asking if the World | Gold: True
2025-07-10 08:46:54,553 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 08:48:09,781 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=100, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 08:48:09,781 - energy_eval - INFO - Using device: cuda
2025-07-10 08:48:09,781 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 08:48:09,789 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 08:48:09,789 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 08:48:09,789 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-10 08:48:18,313 - energy_eval - INFO - Full output: <think>
Okay, user is asking if the World
2025-07-10 08:48:18,314 - energy_eval - INFO - [deepseek-r1:8b | q | QID 0] Prediction: <think>
Okay, user is asking if the World | Gold: True
2025-07-10 08:48:19,651 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 08:50:01,069 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=100, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 08:50:01,069 - energy_eval - INFO - Using device: cuda
2025-07-10 08:50:01,069 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 08:50:01,072 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 08:50:01,072 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 08:50:01,072 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-10 08:50:49,616 - energy_eval - INFO - Full output: <think>
Okay, user is asking if the World Cup has ever been won on penalties and wants a strict yes/no answer without explanation or punctuation. 

Hmm, this seems like a straightforward fact-check question about football history. The phrasing "has the world cup been won on penalties" suggests they're looking for confirmation whether penalty shootouts have determined any World Cup final winners.

I recall that before 1966, games were decided by extra time without penalties - just sudden death in the second half of extra time. But since then... Brazil vs Italy 1970 was a classic shootout win but wasn't official penalties yet. Then England beating Germany in 1966 via goal difference after extra time. 

Wait no, that's not what they're asking. They want to know if the final has ever been won purely on penalty kicks without any other method. The first true penalty shootout winner was West Germany beating Brazil in 1974 - but wait, that wasn't a full-time penalty shootout either. 

Actually looking deeper... Argentina vs England 1986 had extra time then penalties after goalless draw. That's the first official final decided by penalties according to FIFA rules at the time. And yes, it counts as being won on penalties.

But user just wants one word answer - no explanations needed. So "True" is correct here because there was indeed a World Cup final (1986) that went all the way to penalties after extra time.
</think>
True
2025-07-10 08:50:49,616 - energy_eval - INFO - [deepseek-r1:8b | q | QID 0] Prediction: <think>
Okay, user is asking if the World Cup has ever been won on penalties and wants a strict yes/no answer without explanation or punctuation. 

Hmm, this seems like a straightforward fact-check question about football history. The phrasing "has the world cup been won on penalties" suggests they're looking for confirmation whether penalty shootouts have determined any World Cup final winners.

I recall that before 1966, games were decided by extra time without penalties - just sudden death in the second half of extra time. But since then... Brazil vs Italy 1970 was a classic shootout win but wasn't official penalties yet. Then England beating Germany in 1966 via goal difference after extra time. 

Wait no, that's not what they're asking. They want to know if the final has ever been won purely on penalty kicks without any other method. The first true penalty shootout winner was West Germany beating Brazil in 1974 - but wait, that wasn't a full-time penalty shootout either. 

Actually looking deeper... Argentina vs England 1986 had extra time then penalties after goalless draw. That's the first official final decided by penalties according to FIFA rules at the time. And yes, it counts as being won on penalties.

But user just wants one word answer - no explanations needed. So "True" is correct here because there was indeed a World Cup final (1986) that went all the way to penalties after extra time.
</think>
True | Gold: True
2025-07-10 08:51:34,124 - energy_eval - INFO - Full output: <think>
Okay, user wants a yes/no answer about whether anyone has climbed Everest's North Face without explanation or extra words. 

Hmm, this seems like a straightforward factual question from someone who either needs quick confirmation for research or just testing basic knowledge. The phrasing is very rigid - "Answer ONLY with 'True' or 'False'" suggests they might be scripting something or want pure data extraction.

Wait no, actually the user didn't ask if it's possible but directly asked if anyone has done it. That's an important distinction. 

I know for sure that yes, people have climbed the North Face of Everest - Reinhold Messner and Ang Rita were among the first pioneers who did it in 1986-87 season. But since user insists on one-word answer only, I shouldn't mention details.

User seems to value precision over context here. Maybe they're building a database or creating multiple-choice tests where brevity matters. The no-punctuation rule makes me think this could be feeding into some automated system.

But the question itself is interesting - it's not asking about feasibility but actual achievement. That shows user understands mountaineering terminology well enough to specify "North Face" versus just general climbing questions.

Alright, keeping it simple then. Just need to confirm that yes, people have climbed it.
</think>
True
2025-07-10 08:51:34,124 - energy_eval - INFO - [deepseek-r1:8b | q | QID 1] Prediction: <think>
Okay, user wants a yes/no answer about whether anyone has climbed Everest's North Face without explanation or extra words. 

Hmm, this seems like a straightforward factual question from someone who either needs quick confirmation for research or just testing basic knowledge. The phrasing is very rigid - "Answer ONLY with 'True' or 'False'" suggests they might be scripting something or want pure data extraction.

Wait no, actually the user didn't ask if it's possible but directly asked if anyone has done it. That's an important distinction. 

I know for sure that yes, people have climbed the North Face of Everest - Reinhold Messner and Ang Rita were among the first pioneers who did it in 1986-87 season. But since user insists on one-word answer only, I shouldn't mention details.

User seems to value precision over context here. Maybe they're building a database or creating multiple-choice tests where brevity matters. The no-punctuation rule makes me think this could be feeding into some automated system.

But the question itself is interesting - it's not asking about feasibility but actual achievement. That shows user understands mountaineering terminology well enough to specify "North Face" versus just general climbing questions.

Alright, keeping it simple then. Just need to confirm that yes, people have climbed it.
</think>
True | Gold: True
2025-07-10 08:51:52,829 - energy_eval - INFO - Full output: <think>
Okay, the question is "does Air Force One fly with fighter escort?" I need to recall what I know about Air Force One and its security measures.

Air Force One is a heavily protected aircraft, so it's common for there to be an escort. The US Navy provides aerial refueling and protection during flights. So yes, fighter escorts are part of that security detail.
</think>
True
2025-07-10 08:51:52,830 - energy_eval - INFO - [deepseek-r1:8b | q | QID 2] Prediction: <think>
Okay, the question is "does Air Force One fly with fighter escort?" I need to recall what I know about Air Force One and its security measures.

Air Force One is a heavily protected aircraft, so it's common for there to be an escort. The US Navy provides aerial refueling and protection during flights. So yes, fighter escorts are part of that security detail.
</think>
True | Gold: False
2025-07-10 08:52:33,799 - energy_eval - INFO - Full output: <think>
Okay, the user is asking if there's been a Duke of Sussex before and wants me to answer only with "True" or "False". 

Hmm, I need to recall what historical titles exist in British peerage. The Duke of Sussex is an English title created by royal decree. It was first granted to Henry Fitzalan-Howard in 1867 as part of his earldom.

Wait, let me double-check that. Yes, the Duke of Sussex title dates back to Charles I creating it for his son Prince William in 1629. But after that initial creation, did anyone else hold this title? 

I remember Henry Fitzalan-Howard was indeed given the dukedom when he married Lady Florence Trevelyan. That makes sense because royal titles can be recreated or granted anew to descendants of previous holders.

The user seems curious about historical peerage but wants a quick confirmation without explanations. Maybe they're researching for something specific, like genealogy or history trivia. 

I should confirm that yes, there have been Dukes of Sussex before Henry Fitzalan-Howard in 1867. The title was established earlier and passed down through the Howard family lineage.

The answer is True because previous holders existed, even if not contemporaneously with modern understanding.
</think>
True
2025-07-10 08:52:33,799 - energy_eval - INFO - [deepseek-r1:8b | q | QID 3] Prediction: <think>
Okay, the user is asking if there's been a Duke of Sussex before and wants me to answer only with "True" or "False". 

Hmm, I need to recall what historical titles exist in British peerage. The Duke of Sussex is an English title created by royal decree. It was first granted to Henry Fitzalan-Howard in 1867 as part of his earldom.

Wait, let me double-check that. Yes, the Duke of Sussex title dates back to Charles I creating it for his son Prince William in 1629. But after that initial creation, did anyone else hold this title? 

I remember Henry Fitzalan-Howard was indeed given the dukedom when he married Lady Florence Trevelyan. That makes sense because royal titles can be recreated or granted anew to descendants of previous holders.

The user seems curious about historical peerage but wants a quick confirmation without explanations. Maybe they're researching for something specific, like genealogy or history trivia. 

I should confirm that yes, there have been Dukes of Sussex before Henry Fitzalan-Howard in 1867. The title was established earlier and passed down through the Howard family lineage.

The answer is True because previous holders existed, even if not contemporaneously with modern understanding.
</think>
True | Gold: True
2025-07-10 08:57:14,415 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=100, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 08:57:14,415 - energy_eval - INFO - Using device: cuda
2025-07-10 08:57:14,415 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 08:57:14,418 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 08:57:14,418 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 08:57:14,418 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-10 08:58:00,002 - energy_eval - INFO - Full output: <think>
Okay, user is asking if the World Cup has ever been won on penalties and wants a strict yes/no answer without explanation or punctuation. 

Hmm, this seems like a straightforward fact-check question about football history. The phrasing "has the world cup been won on penalties" suggests they're looking for confirmation whether penalty shootouts have determined any World Cup final winners.

I recall that before 1966, games were decided by extra time without penalties - just sudden death in the second half of extra time. But since then... Brazil vs Italy 1970 was a classic shootout win but wasn't official penalties yet. Then England beating Germany in 1966 via goal difference after extra time. 

Wait no, that's not what they're asking. They want to know if the final has ever been won purely on penalty kicks without any other method. The first true penalty shootout winner was West Germany beating Brazil in 1974 - but wait, that wasn't a full-time penalty shootout either. 

Actually looking deeper... Argentina vs England 1986 had extra time then penalties after goalless draw. That's the first official final decided by penalties according to FIFA rules at the time. And yes, it counts as being won on penalties.

But user just wants one word answer - no explanations needed. So "True" is correct here because there was indeed a World Cup final (1986) that went all the way to penalties after extra time.
</think>
True
2025-07-10 08:58:00,002 - energy_eval - INFO - Extracted output: <think>
Okay, user is asking if the World Cup has ever been won on penalties and wants a strict yes/no answer without explanation or punctuation. 

Hmm, this seems like a straightforward fact-check question about football history. The phrasing "has the world cup been won on penalties" suggests they're looking for confirmation whether penalty shootouts have determined any World Cup final winners.

I recall that before 1966, games were decided by extra time without penalties - just sudden death in the second half of extra time. But since then... Brazil vs Italy 1970 was a classic shootout win but wasn't official penalties yet. Then England beating Germany in 1966 via goal difference after extra time. 

Wait no, that's not what they're asking. They want to know if the final has ever been won purely on penalty kicks without any other method. The first true penalty shootout winner was West Germany beating Brazil in 1974 - but wait, that wasn't a full-time penalty shootout either. 

Actually looking deeper... Argentina vs England 1986 had extra time then penalties after goalless draw. That's the first official final decided by penalties according to FIFA rules at the time. And yes, it counts as being won on penalties.

But user just wants one word answer - no explanations needed. So "True" is correct here because there was indeed a World Cup final (1986) that went all the way to penalties after extra time.
</think>
True
2025-07-10 08:58:06,608 - energy_eval - INFO - [deepseek-r1:8b | q | QID 0] Prediction: True | Gold: True
2025-07-10 10:24:21,562 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:32b': {'q': False}, 'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 10:24:21,562 - energy_eval - INFO - Using device: cuda
2025-07-10 10:24:21,563 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 10:24:21,566 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 10:28:06,819 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 10:28:06,819 - energy_eval - INFO - Using device: cuda
2025-07-10 10:28:06,819 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 10:28:06,824 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 10:28:06,824 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 10:28:06,824 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 10:28:06,827 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 10:29:06,162 - energy_eval - ERROR - Inference failed for idx=1: 'bool' object has no attribute 'lower'
2025-07-10 10:29:43,155 - energy_eval - ERROR - Inference failed for idx=6: 'bool' object has no attribute 'lower'
2025-07-10 10:29:52,702 - energy_eval - ERROR - Inference failed for idx=0: 'bool' object has no attribute 'lower'
2025-07-10 10:30:14,618 - energy_eval - ERROR - Inference failed for idx=7: 'bool' object has no attribute 'lower'
2025-07-10 10:30:27,542 - energy_eval - ERROR - Inference failed for idx=5: 'bool' object has no attribute 'lower'
2025-07-10 10:30:52,148 - energy_eval - ERROR - Inference failed for idx=3: 'bool' object has no attribute 'lower'
2025-07-10 10:30:57,404 - energy_eval - ERROR - Inference failed for idx=2: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:31:14,693 - energy_eval - ERROR - Inference failed for idx=4: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:31:30,506 - energy_eval - ERROR - Inference failed for idx=8: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:31:41,817 - energy_eval - ERROR - Inference failed for idx=9: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:32:00,646 - energy_eval - ERROR - Inference failed for idx=10: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:32:18,271 - energy_eval - ERROR - Inference failed for idx=11: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:32:41,864 - energy_eval - ERROR - Inference failed for idx=12: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:33:02,977 - energy_eval - ERROR - Inference failed for idx=13: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:33:12,083 - energy_eval - ERROR - Inference failed for idx=14: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:33:25,055 - energy_eval - ERROR - Inference failed for idx=16: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:33:34,143 - energy_eval - ERROR - Inference failed for idx=15: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:33:38,685 - energy_eval - ERROR - Inference failed for idx=17: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:34:11,972 - energy_eval - ERROR - Inference failed for idx=18: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:34:21,681 - energy_eval - ERROR - Inference failed for idx=19: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:34:48,112 - energy_eval - ERROR - Inference failed for idx=20: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:34:53,789 - energy_eval - ERROR - Inference failed for idx=21: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:35:12,329 - energy_eval - ERROR - Inference failed for idx=22: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:35:31,191 - energy_eval - ERROR - Inference failed for idx=23: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:35:34,272 - energy_eval - ERROR - Inference failed for idx=24: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:35:52,396 - energy_eval - ERROR - Inference failed for idx=25: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:38:03,622 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 10:38:03,622 - energy_eval - INFO - Using device: cuda
2025-07-10 10:38:03,622 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 10:38:03,625 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 10:38:03,625 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 10:38:03,625 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 10:38:03,628 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 10:38:40,675 - energy_eval - ERROR - Inference failed for idx=7: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:39:33,725 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 10:39:33,725 - energy_eval - INFO - Using device: cuda
2025-07-10 10:39:33,725 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 10:39:33,727 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 10:39:33,728 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 10:39:33,728 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 10:39:33,730 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 10:40:09,571 - energy_eval - ERROR - Inference failed for idx=2: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:40:31,317 - energy_eval - ERROR - Inference failed for idx=0: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:43:50,353 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 10:43:50,353 - energy_eval - INFO - Using device: cuda
2025-07-10 10:43:50,353 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 10:43:50,356 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 10:43:50,356 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 10:43:50,356 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 10:43:50,359 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 10:44:21,952 - energy_eval - ERROR - Inference failed for idx=2: 'EmissionsTracker' object has no attribute 'duration'
2025-07-10 10:44:38,367 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 10:44:38,367 - energy_eval - INFO - Using device: cuda
2025-07-10 10:44:38,367 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 10:44:38,370 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 10:44:38,370 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 10:44:38,370 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 10:44:38,373 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 10:45:12,067 - energy_eval - ERROR - Inference failed for idx=3: 'EmissionsTracker' object has no attribute 'id'
2025-07-10 10:45:17,063 - energy_eval - ERROR - Inference failed for idx=4: 'EmissionsTracker' object has no attribute 'id'
2025-07-10 10:45:32,850 - energy_eval - ERROR - Inference failed for idx=2: 'EmissionsTracker' object has no attribute 'id'
2025-07-10 10:45:47,341 - energy_eval - ERROR - Inference failed for idx=7: 'EmissionsTracker' object has no attribute 'id'
2025-07-10 10:46:42,604 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 10:46:42,604 - energy_eval - INFO - Using device: cuda
2025-07-10 10:46:42,604 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 10:46:42,607 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 10:46:42,607 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 10:46:42,607 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 10:46:42,610 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 10:47:29,435 - energy_eval - ERROR - Inference failed for idx=0: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:47:30,714 - energy_eval - ERROR - Inference failed for idx=6: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:49:53,303 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 10:49:53,303 - energy_eval - INFO - Using device: cuda
2025-07-10 10:49:53,303 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 10:49:53,306 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 10:49:53,306 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 10:49:53,306 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 10:49:53,309 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 10:50:19,955 - energy_eval - ERROR - Inference failed for idx=1: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 10:53:37,690 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 10:53:37,690 - energy_eval - INFO - Using device: cuda
2025-07-10 10:53:37,690 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 10:53:37,692 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 10:53:37,692 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 10:53:37,692 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 10:53:37,695 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 10:54:22,375 - energy_eval - ERROR - Inference failed for idx=0: 'EmissionsTracker' object has no attribute 'duration'
2025-07-10 10:54:35,312 - energy_eval - ERROR - Inference failed for idx=6: 'EmissionsTracker' object has no attribute 'duration'
2025-07-10 10:55:45,397 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 10:55:45,397 - energy_eval - INFO - Using device: cuda
2025-07-10 10:55:45,397 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 10:55:45,407 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 10:55:45,407 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 10:55:45,407 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 10:55:45,417 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 11:00:17,407 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:00:17,407 - energy_eval - INFO - Using device: cuda
2025-07-10 11:00:17,408 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:00:17,417 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:00:17,417 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 11:00:17,417 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 11:00:17,426 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 11:10:57,356 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:10:57,357 - energy_eval - INFO - Using device: cuda
2025-07-10 11:10:57,357 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:10:57,360 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:10:57,360 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 11:10:57,360 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 11:10:57,363 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 11:13:11,872 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:13:11,873 - energy_eval - INFO - Using device: cuda
2025-07-10 11:13:11,873 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:13:11,876 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:13:11,876 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 11:13:11,876 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 11:13:11,879 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 11:17:40,584 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:17:40,584 - energy_eval - INFO - Using device: cuda
2025-07-10 11:17:40,584 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:17:40,587 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:19:50,895 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:19:50,895 - energy_eval - INFO - Using device: cuda
2025-07-10 11:19:50,895 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:19:50,898 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:26:41,813 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:26:41,813 - energy_eval - INFO - Using device: cuda
2025-07-10 11:26:41,814 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:26:41,818 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:29:35,407 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:29:35,407 - energy_eval - INFO - Using device: cuda
2025-07-10 11:29:35,407 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:29:35,411 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:32:29,415 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:32:29,416 - energy_eval - INFO - Using device: cuda
2025-07-10 11:32:29,416 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:32:29,420 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:35:50,015 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:35:50,015 - energy_eval - INFO - Using device: cuda
2025-07-10 11:35:50,015 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:35:50,019 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:46:29,425 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False, 'q+r': True}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:46:29,426 - energy_eval - INFO - Using device: cuda
2025-07-10 11:46:29,426 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:46:29,430 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:47:11,427 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 11:47:36,749 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:47:36,749 - energy_eval - INFO - Using device: cuda
2025-07-10 11:47:36,749 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:47:36,752 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:47:36,752 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 11:47:36,752 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 11:47:36,769 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 11:51:14,493 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 11:55:48,261 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:55:48,261 - energy_eval - INFO - Using device: cuda
2025-07-10 11:55:48,261 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:55:48,264 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:55:48,264 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 11:55:48,264 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 11:55:48,267 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 11:56:17,549 - energy_eval - ERROR - Critical error: Server disconnected
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 565, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 78, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 211, in run_model_mode
    responses = asyncio.run(run_async_inference(jobs, model_name))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 560, in run_async_inference
    return await asyncio.gather(*tasks)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 548, in generate_async
    async with session.post(
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 1470, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 761, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 739, in _connect_and_send_request
    await resp.start(conn)
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client_reqrep.py", line 512, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/streams.py", line 672, in read
    await self._waiter
aiohttp.client_exceptions.ServerDisconnectedError: Server disconnected
2025-07-10 11:56:48,564 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:56:48,564 - energy_eval - INFO - Using device: cuda
2025-07-10 11:56:48,564 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:56:48,567 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:56:48,567 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 11:56:48,567 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 11:56:48,570 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 11:57:07,836 - energy_eval - ERROR - Critical error: Server disconnected
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 565, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 78, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 211, in run_model_mode
    responses = asyncio.run(run_async_inference(jobs, model_name))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 560, in run_async_inference
    return await asyncio.gather(*tasks)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 548, in generate_async
    async with session.post(
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 1470, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 761, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 739, in _connect_and_send_request
    await resp.start(conn)
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client_reqrep.py", line 512, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/streams.py", line 672, in read
    await self._waiter
aiohttp.client_exceptions.ServerDisconnectedError: Server disconnected
2025-07-10 11:57:34,439 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 11:57:34,439 - energy_eval - INFO - Using device: cuda
2025-07-10 11:57:34,440 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 11:57:34,442 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 11:57:34,442 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 11:57:34,443 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 11:57:34,445 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 12:01:26,692 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 12:02:17,587 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 12:02:17,587 - energy_eval - INFO - Using device: cuda
2025-07-10 12:02:17,587 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 12:02:17,590 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 12:02:17,590 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 12:02:17,590 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 12:02:17,593 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 12:07:18,321 - energy_eval - ERROR - Critical error: 
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client_reqrep.py", line 512, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/streams.py", line 672, in read
    await self._waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 567, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 212, in run_model_mode
    responses = asyncio.run(run_async_inference(jobs, model_name))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 560, in run_async_inference
    return await tqdm_asyncio.gather(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/tqdm/asyncio.py", line 79, in gather
    res = [await f for f in cls.as_completed(ifs, loop=loop, timeout=timeout,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/tqdm/asyncio.py", line 79, in <listcomp>
    res = [await f for f in cls.as_completed(ifs, loop=loop, timeout=timeout,
           ^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/tasks.py", line 615, in _wait_for_one
    return f.result()  # May raise f.exception().
           ^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/tqdm/asyncio.py", line 76, in wrap_awaitable
    return i, await f
              ^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 548, in generate_async
    async with session.post(
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 1470, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 761, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 739, in _connect_and_send_request
    await resp.start(conn)
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client_reqrep.py", line 507, in start
    with self._timer:
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError
2025-07-10 12:07:18,557 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-128' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at /home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/tqdm/asyncio.py:75> exception=TimeoutError()>
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client_reqrep.py", line 512, in start
    message, payload = await protocol.read()  # type: ignore[union-attr]
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/streams.py", line 672, in read
    await self._waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/tqdm/asyncio.py", line 76, in wrap_awaitable
    return i, await f
              ^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 548, in generate_async
    async with session.post(
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 1470, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 761, in _request
    resp = await handler(req)
           ^^^^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client.py", line 739, in _connect_and_send_request
    await resp.start(conn)
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/client_reqrep.py", line 507, in start
    with self._timer:
  File "/home/penguins/Documents/LMPowerConsumption/.venv/lib/python3.11/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError
2025-07-10 12:35:03,828 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 12:35:03,828 - energy_eval - INFO - Using device: cuda
2025-07-10 12:35:03,828 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 12:35:03,832 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 12:35:03,832 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 12:35:03,832 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 12:35:03,836 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 12:35:03,838 - energy_eval - ERROR - Critical error: <coroutine object as_completed.<locals>._wait_for_one at 0x7bb27d76da40>
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 598, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 285, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 233, in async_process
    idx, sample, ret_metrics = task_to_job[completed_task]
                               ~~~~~~~~~~~^^^^^^^^^^^^^^^^
KeyError: <coroutine object as_completed.<locals>._wait_for_one at 0x7bb27d76da40>
2025-07-10 12:35:36,786 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 12:35:36,786 - energy_eval - INFO - Using device: cuda
2025-07-10 12:35:36,786 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 12:35:36,789 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 12:35:36,789 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 12:35:36,789 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 12:35:36,792 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 12:35:36,793 - energy_eval - ERROR - Critical error: 'async for' requires an object with __aiter__ method, got generator
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 609, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 296, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 229, in async_process
    async for completed_coro in tqdm_asyncio.as_completed(tasks, total=len(tasks),
TypeError: 'async for' requires an object with __aiter__ method, got generator
2025-07-10 12:39:34,476 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 12:39:34,476 - energy_eval - INFO - Using device: cuda
2025-07-10 12:39:34,476 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 12:39:34,479 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 12:39:34,479 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 12:39:34,479 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 12:39:34,482 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 12:39:34,483 - energy_eval - ERROR - Critical error: Could not match completed coroutine to task
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 613, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 300, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 244, in async_process
    raise RuntimeError(
RuntimeError: Could not match completed coroutine to task
2025-07-10 12:41:33,140 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 12:41:33,140 - energy_eval - INFO - Using device: cuda
2025-07-10 12:41:33,140 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 12:41:33,143 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 12:41:33,143 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 12:41:33,143 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 12:41:33,146 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 12:41:33,148 - energy_eval - ERROR - Critical error: 'coroutine' object has no attribute 'job_info'
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 590, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 277, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 228, in async_process
    idx, sample, ret_metrics = completed_task.job_info
                               ^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'coroutine' object has no attribute 'job_info'
2025-07-10 12:42:43,876 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 12:42:43,876 - energy_eval - INFO - Using device: cuda
2025-07-10 12:42:43,876 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 12:42:43,881 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 12:42:43,881 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 12:42:43,881 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 12:42:43,886 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 12:42:43,888 - energy_eval - ERROR - Critical error: 'coroutine' object has no attribute 'job_info'
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 592, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 279, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 228, in async_process
    idx, sample, ret_metrics = completed_task.job_info
                               ^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'coroutine' object has no attribute 'job_info'
2025-07-10 12:57:16,156 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 12:57:16,156 - energy_eval - INFO - Using device: cuda
2025-07-10 12:57:16,156 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 12:57:16,160 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 12:57:16,160 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 12:57:16,160 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 12:57:16,164 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 12:57:16,166 - energy_eval - ERROR - Critical error: <coroutine object as_completed.<locals>._wait_for_one at 0x70d923141b10>
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 596, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 283, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 232, in async_process
    idx, sample, ret_metrics = task_to_job[future]
                               ~~~~~~~~~~~^^^^^^^^
KeyError: <coroutine object as_completed.<locals>._wait_for_one at 0x70d923141b10>
2025-07-10 13:01:34,241 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 13:01:34,241 - energy_eval - INFO - Using device: cuda
2025-07-10 13:01:34,241 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 13:01:34,244 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 13:01:34,244 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 13:01:34,244 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 13:01:34,249 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 13:01:34,251 - energy_eval - ERROR - Critical error: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 608, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 295, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 246, in async_process
    full_output = resp["response"]
                  ~~~~^^^^^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2025-07-10 13:03:25,395 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 13:03:25,395 - energy_eval - INFO - Using device: cuda
2025-07-10 13:03:25,395 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 13:03:25,398 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 13:03:25,398 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 13:03:25,398 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 13:03:25,401 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 13:03:25,403 - energy_eval - ERROR - Critical error: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 608, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 295, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 246, in async_process
    full_output = resp["response"]
                  ~~~~^^^^^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2025-07-10 13:03:50,347 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 13:03:50,347 - energy_eval - INFO - Using device: cuda
2025-07-10 13:03:50,347 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 13:03:50,350 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 13:03:50,350 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 13:03:50,350 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 13:03:50,353 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 13:03:50,355 - energy_eval - ERROR - Critical error: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 609, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 296, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 247, in async_process
    full_output = resp["response"]
                  ~~~~^^^^^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2025-07-10 13:04:27,186 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 13:04:27,186 - energy_eval - INFO - Using device: cuda
2025-07-10 13:04:27,186 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 13:04:27,189 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 13:04:27,189 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 13:04:27,189 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 13:04:27,192 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 13:04:27,194 - energy_eval - ERROR - Critical error: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 609, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 296, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 247, in async_process
    full_output = resp["response"]
                  ~~~~^^^^^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2025-07-10 13:04:46,692 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 13:04:46,692 - energy_eval - INFO - Using device: cuda
2025-07-10 13:04:46,692 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 13:04:46,695 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 13:04:46,695 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 13:04:46,695 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 13:04:46,698 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 13:04:46,699 - energy_eval - ERROR - Critical error: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 610, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 297, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 248, in async_process
    full_output = resp["response"]
                  ~~~~^^^^^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2025-07-10 13:04:55,935 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 13:04:55,936 - energy_eval - INFO - Using device: cuda
2025-07-10 13:04:55,936 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 13:04:55,939 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 13:04:55,939 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 13:04:55,939 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 13:04:55,942 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 13:04:55,944 - energy_eval - ERROR - Critical error: 'NoneType' object is not subscriptable
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 611, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 298, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 249, in async_process
    full_output = resp["response"]
                  ~~~~^^^^^^^^^^^^
TypeError: 'NoneType' object is not subscriptable
2025-07-10 13:14:46,081 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 13:14:46,081 - energy_eval - INFO - Using device: cuda
2025-07-10 13:14:46,081 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 13:14:46,085 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 13:14:46,085 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 13:14:46,085 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 13:14:46,088 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 13:15:57,334 - energy_eval - ERROR - Critical error: tuple indices must be integers or slices, not str
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 602, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 296, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 247, in async_process
    full_output = resp["response"]
                  ~~~~^^^^^^^^^^^^
TypeError: tuple indices must be integers or slices, not str
2025-07-10 13:16:27,880 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 13:16:27,880 - energy_eval - INFO - Using device: cuda
2025-07-10 13:16:27,880 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 13:16:27,883 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 13:16:27,883 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 13:16:27,883 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 13:16:27,886 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 13:17:41,540 - energy_eval - ERROR - Critical error: tuple indices must be integers or slices, not str
Traceback (most recent call last):
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 603, in <module>
    run()
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 79, in run
    run_model_mode(
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 297, in run_model_mode
    asyncio.run(async_process())
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/penguins/Documents/LMPowerConsumption/src/experiment.py", line 248, in async_process
    full_output = resp["response"]
                  ~~~~^^^^^^^^^^^^
TypeError: tuple indices must be integers or slices, not str
2025-07-10 13:18:40,925 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:1.5b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 13:18:40,925 - energy_eval - INFO - Using device: cuda
2025-07-10 13:18:40,925 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 13:18:40,928 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 13:18:40,928 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 13:18:40,928 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 13:18:40,931 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 13:29:56,211 - energy_eval - INFO - Completed q for deepseek-r1:1.5b in 0h 11m 15s
2025-07-10 13:29:56,310 - energy_eval - INFO - Completed deepseek-r1:1.5b in 0h 11m 15s
2025-07-10 13:29:56,310 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 11m 15s
============================================================
2025-07-10 13:32:01,316 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:1.5b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 13:32:01,316 - energy_eval - INFO - Using device: cuda
2025-07-10 13:32:01,316 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 13:32:01,319 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 13:32:01,319 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:1.5b
============================================================
2025-07-10 13:32:01,319 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {}
++++++++++++++++++++++++++++++
2025-07-10 13:32:01,411 - energy_eval - INFO - Completed deepseek-r1:1.5b in 0h 0m 0s
2025-07-10 13:32:01,411 - energy_eval - INFO - 
============================================================
Experiment completed in 0h 0m 0s
============================================================
2025-07-10 13:33:11,696 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False}}, wiki_dir=PosixPath('data/hotpot_wiki-processed'), corpus_cache=PosixPath('cache/wiki.pkl'), tfidf_cache=PosixPath('cache/tfidf.pkl'), index_cache=PosixPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results/energy'), result_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/results'), data_dir=PosixPath('/home/penguins/Documents/LMPowerConsumption/data'), emissions_dir=PosixPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 13:33:11,696 - energy_eval - INFO - Using device: cuda
2025-07-10 13:33:11,696 - energy_eval - INFO - Loading dataset from /home/penguins/Documents/LMPowerConsumption/data/boolq_mini_128.jsonl
2025-07-10 13:33:11,699 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 13:33:11,699 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 13:33:11,699 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 13:33:11,702 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 16:54:37,105 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 16:54:37,105 - energy_eval - INFO - Using device: cuda
2025-07-10 16:54:40,014 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-10 16:58:14,680 - energy_eval - INFO - Loaded wiki in 0h3m34s
2025-07-10 16:58:14,681 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 16:58:14,681 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-10 16:58:14,742 - energy_eval - INFO - Built 3270 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 17:47:35,619 - energy_eval - ERROR - Critical error: 'EmissionsTracker' object has no attribute 'final_emissions_data'
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 598, in <module>
    run()
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 79, in run
    run_model_mode(
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 292, in run_model_mode
    asyncio.run(async_process())
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\asyncio\runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\asyncio\runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 253, in async_process
    pred, inf_metrics = process_boolq_prediction(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 557, in process_boolq_prediction
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,518 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-441' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,532 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-501' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,532 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-504' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,533 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-508' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,535 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-586' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,535 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-622' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,535 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-633' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,535 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-636' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,536 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-654' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,536 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-670' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,536 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-706' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,536 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-724' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,537 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-732' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,537 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-735' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,537 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-745' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,537 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-746' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,539 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-747' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,539 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-748' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,539 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-749' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,539 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-750' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,540 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-751' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,540 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-752' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,540 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-753' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,541 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-754' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,541 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-755' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,541 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-756' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,541 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-757' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 17:47:45,542 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-758' coro=<generate_async() done, defined at C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py:578> exception=AttributeError("'EmissionsTracker' object has no attribute 'final_emissions_data'")>
Traceback (most recent call last):
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\experiment.py", line 580, in generate_async
    return await loop.run_in_executor(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ethan\Documents\PhD\LMPowerConsuption\src\inference.py", line 81, in inference
    "duration": float(tracker.final_emissions_data.duration),
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'
2025-07-10 21:42:46,867 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 21:42:46,867 - energy_eval - INFO - Using device: cuda
2025-07-10 21:42:48,941 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-10 21:45:05,963 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 21:46:05,358 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 21:46:05,358 - energy_eval - INFO - Using device: cuda
2025-07-10 21:46:07,911 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-10 21:47:26,818 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 21:48:24,565 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 21:48:24,565 - energy_eval - INFO - Using device: cuda
2025-07-10 21:48:27,055 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-10 21:49:51,419 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 21:50:03,983 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 21:50:03,983 - energy_eval - INFO - Using device: cuda
2025-07-10 21:50:03,983 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-07-10 21:50:04,003 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 21:53:38,295 - energy_eval - INFO - Loaded wiki in 0h3m34s
2025-07-10 21:53:38,295 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 21:53:38,295 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-10 21:53:38,298 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 22:03:32,282 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 9m 53s
2025-07-10 22:14:11,609 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 22:14:11,609 - energy_eval - INFO - Using device: cuda
2025-07-10 22:14:11,609 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-07-10 22:14:11,612 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 22:15:07,264 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 22:16:33,303 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_mini_128.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 22:16:33,304 - energy_eval - INFO - Using device: cuda
2025-07-10 22:16:33,304 - energy_eval - INFO - Loading dataset from C:\Users\Ethan\Documents\PhD\LMPowerConsuption\data\boolq_mini_128.jsonl
2025-07-10 22:16:33,307 - energy_eval - INFO - Loaded dataset with 128 samples
2025-07-10 22:16:33,307 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-10 22:20:07,819 - energy_eval - INFO - Loaded wiki in 0h3m34s
2025-07-10 22:20:07,820 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-10 22:20:07,820 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-10 22:20:07,824 - energy_eval - INFO - Built 0 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 22:20:07,825 - energy_eval - INFO - Completed q for deepseek-r1:8b in 0h 0m 0s
2025-07-10 22:29:48,406 - energy_eval - INFO - Built 128 prompts in 0h 9m 40s (+ retrieval: 0h 0m 3s, 0.0009 kWh, 0.0003 kg)
2025-07-10 22:39:44,331 - energy_eval - INFO - Completed q+r for deepseek-r1:8b in 0h 19m 36s
2025-07-10 22:39:50,476 - energy_eval - INFO - Completed deepseek-r1:8b in 0h 19m 42s
2025-07-10 22:39:50,476 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:14b
============================================================
2025-07-10 22:39:50,476 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-10 22:39:50,483 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 22:49:53,498 - energy_eval - INFO - Completed q for deepseek-r1:14b in 0h 10m 3s
2025-07-10 22:59:33,377 - energy_eval - INFO - Built 128 prompts in 0h 9m 39s (+ retrieval: 0h 0m 3s, 0.0009 kWh, 0.0003 kg)
2025-07-10 23:09:29,436 - energy_eval - INFO - Completed q+r for deepseek-r1:14b in 0h 19m 35s
2025-07-10 23:09:34,486 - energy_eval - INFO - Completed deepseek-r1:14b in 0h 29m 44s
2025-07-10 23:09:34,486 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:32b
============================================================
2025-07-10 23:09:34,486 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False}
++++++++++++++++++++++++++++++
2025-07-10 23:09:34,492 - energy_eval - INFO - Built 128 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
2025-07-10 23:23:26,737 - energy_eval - INFO - Completed q for deepseek-r1:32b in 0h 13m 52s
2025-07-10 23:23:31,482 - energy_eval - INFO - Completed deepseek-r1:32b in 0h 13m 56s
2025-07-10 23:23:31,482 - energy_eval - INFO - 
============================================================
Experiment completed in 1h 6m 58s
============================================================
2025-07-10 23:52:02,767 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Read this passage, then answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\n\nPassage: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. No explanations. No punctuation. Just one word.\nQuestion: {question}\nAnswer:"}})
2025-07-10 23:52:02,767 - energy_eval - INFO - Using device: cuda
2025-07-10 23:52:05,090 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-10 23:52:05,090 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-10 23:55:05,066 - energy_eval - WARNING - Experiment interrupted by user
2025-07-10 23:57:56,321 - energy_eval - INFO - Starting experiment with config:
ExperimentConfig(model_types={'deepseek-r1:8b': 'ollama', 'deepseek-r1:14b': 'ollama', 'deepseek-r1:32b': 'ollama'}, dataset_name='google/boolq', dataset_file='boolq_1.jsonl', config='fullwiki', split='validation', n_samples=None, max_new_tokens=64, batch_size=4, device='cuda', modes={'deepseek-r1:8b': {'q': False, 'q+r': True}, 'deepseek-r1:14b': {'q': False, 'q+r': True}, 'deepseek-r1:32b': {'q': False}}, wiki_dir=WindowsPath('data/hotpot_wiki-processed'), corpus_cache=WindowsPath('cache/wiki.pkl'), tfidf_cache=WindowsPath('cache/tfidf.pkl'), index_cache=WindowsPath('cache/index.pkl'), intro_min_chars=51, hash_bits=20, token_pattern='(?u)\\b\\w+\\b', energy_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results/energy'), result_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/results'), data_dir=WindowsPath('C:/Users/Ethan/Documents/PhD/LMPowerConsuption/data'), emissions_dir=WindowsPath('emissions'), retrieval_only=False, email_results=True, from_email='eheavey626@gmail.com', to_email='s72kw@unb.ca', log_level='INFO', prompt_templates={'hotpot': {'with_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know and the context provided. Context: {context}\nQuestion: {question}\nAnswer:', 'without_context': 'Answer the following to the best of your ability. You must provide an answer. If you are unsure, make an educated guess based on what you know. Question: {question}\nAnswer:'}, 'boolq': {'with_context': "Using the provided context, answer the question ONLY with 'True' or 'False'. Be thorough in your analysis of the context but answer with just one word.Do not overcomplicate your thinking. Do not go in circles.\nContext: {context}\nQuestion: {question}\nAnswer:", 'without_context': "Answer ONLY with 'True' or 'False'. Be thorough in your analysis but answer with just one word.Do not overcomplicate your thinking. Do not go in circles.\nQuestion: {question}\nAnswer:"}})
2025-07-10 23:57:56,321 - energy_eval - INFO - Using device: cuda
2025-07-10 23:57:58,456 - energy_eval - INFO - Loaded dataset with 3270 samples
2025-07-10 23:57:58,456 - energy_eval - INFO - Retrieval mode requested - loading wiki
2025-07-11 00:01:36,381 - energy_eval - INFO - Loaded wiki in 0h3m37s
2025-07-11 00:01:36,381 - energy_eval - INFO - 
============================================================
Running model: deepseek-r1:8b
============================================================
2025-07-11 00:01:36,381 - energy_eval - INFO - 
++++++++++++++++++++++++++++++
Running modes: {'q': False, 'q+r': True}
++++++++++++++++++++++++++++++
2025-07-11 00:01:36,597 - energy_eval - INFO - Built 3270 prompts in 0h 0m 0s (+ retrieval: 0h 0m 0s, 0.0000 kWh, 0.0000 kg)
