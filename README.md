
# LMPowerConsuption

**LMPowerConsuption** is a lightweight framework for **measuring both the
accuracy *and* energyâ€‘usage of small language models (SLMs) on popular QA
benchmarks**.  
It grew out of my PhD work on environmentally aware evaluation of retrievalâ€‘
augmented generation systems.

---

## âœ¨Â Key ideas
*Â **Oneâ€‘liner experiments** â€“ run any HF model on any HF dataset with a single
  config flag.  
*Â **Energy firstâ€‘class** â€“ every prompt is wrapped in a
  [`codecarbon`](https://github.com/mlco2/codecarbon) tracker; perâ€‘question
  Joules and aggregate kWh are logged automatically.  
*Â **Reproducible CSV outputs** â€“ predictions, gold answers, EMÂ /Â F1 and kWh are
  saved in tidy files ready for pandas/R analysis.

---

## ğŸ”–Â Repository layout
LMPowerConsuption/
â”œâ”€ scripts/
â”‚  â”œâ”€ hotpot\_smol\_eval\_scored.py   # HotpotQA evaluation & energy
â”‚  â””â”€ boolq\_smol\_eval\_scored.py    # BoolQ evaluation & energy
â”œâ”€ Energy/                         # perâ€‘question CodeCarbon logs
â”œâ”€ avg\_results.txt                 # running table of overall scores
â””â”€ requirements.txt                # pinned deps

---

## âš¡ï¸Â Quick start

```bash
# 1Â â€”Â clone & create minimal env
git clone https://github.com/VeiledTee/LMPowerConsuption.git
cd LMPowerConsuption
python -m venv .venv && .venv/Scripts/activate   # Windows
python -m ensurepip --upgrade
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt

# 2Â â€”Â run a demo (BoolQ, questionâ€‘only prompt)
python scripts/boolq_smol_eval_scored.py

# 3Â â€”Â check results
cat boolq_smol_q_only_results.csv      # perâ€‘question data
cat avg_results.txt                    # datasetâ€‘level summary
````

### Commandâ€‘line options

Each script can be tweaked via the constants at the top:

| Flag              | Meaning                                                      |
| ----------------- | ------------------------------------------------------------ |
| `INCLUDE_PASSAGE` | Â Include dataset context/passage in the prompt               |
| `N_SAMPLES`       | Evaluate only the first *n* rows (speedy smokeâ€‘run)          |
| `MODEL_NAME`      | Any HF identifier (quantised GGUF works via `ctransformers`) |
| `MAX_NEW_TOK`     | Decoding budget per question                                 |

---

## ğŸ“ŠÂ Output files

| File                            | What it contains                                |       |         |                            |
| ------------------------------- | ----------------------------------------------- | ----- | ------- | -------------------------- |
| `boolq_smol_q_only_results.csv` | `qid, predicted, gold, em, energy_kWh` per item |       |         |                            |
| `avg_results.txt`               | DATASET                                       | VERSION |MODEL | avg\_EM | avg\_energy\_kWh\` per run |
| `Energy/energy_<id>.csv`        | Raw `codecarbon` trace for the *idâ€‘th* prompt   |       |         |                            |

Merge multiple runs with pandas or Excel to rank models by **energy per correct
answer**.

---

## ğŸ”ŒÂ Adding a new benchmark

1. Copy one of the scripts in `scripts/`.
2. Change `DATASET_NAME`, `build_prompt()` and scoring function.
3. Log EM/F1 exactly as in Hotpot or BoolQ; `energy_sum` & `avg_results.txt`
   require no changes.

---

## ğŸ› Â Requirements

\*Â PythonÂ â‰¥â€¯3.9
\*Â `transformers`, `datasets`, `codecarbon`, `accelerate`
\*Â NVIDIA GPUÂ (optional) â€“ intâ€‘4 models fit in 6â€‘8â€¯GB; CPU also works (slower).

Exact versions are pinned in `requirements.txt`.

---

## ğŸ¤Â Contributing

Pull requests that add new datasets, models, or alternative energy loggers
(`zeus`, `pyRAPL`, IPMI) are very welcome!  Please open an issue first to
discuss the scope.
